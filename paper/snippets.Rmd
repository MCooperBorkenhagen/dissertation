# wordwise measures
In order to establish a common measure of accuracy across computational architectures examined here, a word-level metric was constructed in order to avoid issues having to do with the idiosyncrasies of unitwise measures of accuracy and how they differ in LSTM versus feedforward model architectures. Each model was tested for every word in the training set after some amount of training and the output for each word was recorded. The output was then compared against every possible target output. The actual output and the target output are then compared using an L2 norm. In order to make such a calculation for the time-varying model, the outputs are flattened (across timesteps) into a 1d array. Outputs were padded with zeros in order to make them all the same length, thus facilitating the calculation of euclidean distance. This procedure isn't necessary for the outputs of the feedforward network because that model uses centered (and padded) inputs an outputs during training. Thus all its outputs (and the targets to which they are compared) are the proper (identical) dimensions.

For a word to be correct using a word-wise measure, the produced output needs to be closer to the proper target than to all others. When measuring this difference rank of the true target relative to all others is recorded, where if the true target is the closest target, the rank is one. This word-level measure avoids the possibility that error differences across the LSTM and feedforward architectures might be related to the differences in the dimensions of the output vectors, differences in the ways that output phonemes are specified (i.e., the feedforward architecture doesn't use start- and end-of-word patterns/ units), or other differences not directly related to the cognitive model of the computational learner. This accuracy measure (shown as "error" in figures) is standardized in order to compare measures directly across model and behavioral data.



One issue about such a comparison (and comparisons of model performance as it relates to processing difficulty in general) has to do with equating measures arising from computational models and those derived from human performance. The LSTM-based system investigated here always has information about a given output segment available for error correction. There is no additional penalty for a word being longer (phonologically) other than the difficulties that arise from co-occurrence statistics across phonological segments (e.g., that certain transitional probabilities at word endings might be less frequent at the ends of words than at beginnings). Furthermore, mean squared error, while it has been a standard of evaluation in other similar computational approaches, may be less precise than other alternatives for time-varying learning systems like the architecture put forward here.
