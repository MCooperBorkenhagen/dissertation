# wordwise measures
In order to establish a common measure of accuracy across computational architectures examined here, a word-level metric was constructed in order to avoid issues having to do with the idiosyncrasies of unitwise measures of accuracy and how they differ in LSTM versus feedforward model architectures. Each model was tested for every word in the training set after some amount of training and the output for each word was recorded. The output was then compared against every possible target output. The actual output and the target output are then compared using an L2 norm. In order to make such a calculation for the time-varying model, the outputs are flattened (across timesteps) into a 1d array. Outputs were padded with zeros in order to make them all the same length, thus facilitating the calculation of euclidean distance. This procedure isn't necessary for the outputs of the feedforward network because that model uses centered (and padded) inputs an outputs during training. Thus all its outputs (and the targets to which they are compared) are the proper (identical) dimensions.

For a word to be correct using a word-wise measure, the produced output needs to be closer to the proper target than to all others. When measuring this difference rank of the true target relative to all others is recorded, where if the true target is the closest target, the rank is one. This word-level measure avoids the possibility that error differences across the LSTM and feedforward architectures might be related to the differences in the dimensions of the output vectors, differences in the ways that output phonemes are specified (i.e., the feedforward architecture doesn't use start- and end-of-word patterns/ units), or other differences not directly related to the cognitive model of the computational learner. This accuracy measure (shown as "error" in figures) is standardized in order to compare measures directly across model and behavioral data.



One issue about such a comparison (and comparisons of model performance as it relates to processing difficulty in general) has to do with equating measures arising from computational models and those derived from human performance. The LSTM-based system investigated here always has information about a given output segment available for error correction. There is no additional penalty for a word being longer (phonologically) other than the difficulties that arise from co-occurrence statistics across phonological segments (e.g., that certain transitional probabilities at word endings might be less frequent at the ends of words than at beginnings). Furthermore, mean squared error, while it has been a standard of evaluation in other similar computational approaches, may be less precise than other alternatives for time-varying learning systems like the architecture put forward here.






### tabling a statistical model
```{r}

require(papaja)
require(glue)



descriptives = mono %>% 
  filter(stage %in% c('Middle', 'Late') & model == 'LSTM') %>%
  filter(train_test == 'train') %>% 
  group_by(stage) %>% 
  summarise(frequency_m = mean(freq_scaled),
            consistency_m = mean(consistency)) %>% 
  mutate(stage_f = case_when(stage == 'Middle' ~ -.5,
                             stage == 'Late' ~ .5))

mono %>% 
  filter(stage %in% c('Middle', 'Late') & model == 'LSTM') %>%
  filter(train_test == 'train') %>% 
  left_join(descriptives) %>% 
  mutate(frequency_c = freq_scaled-frequency_m,
         consistency_c = consistency-consistency_m) %>% 
  stats::lm(accuracy ~ stage_f*consistency_c*frequency_c, data = .) -> model

modelCI = confint(model)
modeldata = summary(model)

tabledata = data.frame(modeldata$coefficients) %>% 
  mutate(Estimate = sig_level(Estimate, Pr...t..),
         Pr...t.. = presentp(Pr...t..))

tabledata$CI = squarebracket(modelCI[,1], modelCI[,2], digits = 3)

tabledata$Predictor = c('Intercept', 'Learning stage', 'Consistency', 'Frequency', 'Learn stg x consist', 'Learn stg x freq', 'Consistency x frequency', 'Learn stg x consist x freq')

rownames(tabledata) = NULL



HEADER = c('*b*', '*SE*', '*t*', '*p*', '95% CI', 'Predictor')
names(tabledata) = HEADER

tabledata %>% 
  select(Predictor, everything()) %>% 
  apa_table(digits = 3, format = 'pandoc', align = 'l', landscape = T,
            note = 'Model estimated using lm() in R with confidence intervals estimated using confint(), both methods from the native R stats package [@R2021]. Bold parameter estimates indicate significant *p*-values below the alpha threshold of .05.',
            caption = 'Regression Model of the Interaction Between Learning Stage, Word (Body-Rime) Consistency, and Word Frequency on Mean Squared Error')


```


This is the test of conditions in the taraban factors for the model data.

```{r, echo=FALSE}

taraban_crossval %>% 
  filter(!is.na(taraban_group)) %>% 
  filter(epoch == 27) %>% 
  mutate(frequency = case_when(freq_taraban == 'high' ~ .5,
                               freq_taraban == 'low' ~ -.5),
         condition = case_when(taraban_group == 1 ~ -.5,
                               taraban_group == 2 ~ -.5,
                               taraban_group == 3 ~ .5,
                               taraban_group == 4 ~ .5),
         test_control = case_when(taraban_test == 'test' ~ -.5,
                                  taraban_test == 'control' ~ .5)) %>% 
  lmer(mse ~ frequency*condition*test_control + (1|run_id) + (1|word), data = .) %>% 
  summary()

```




## Batches
One potential limitation of the learning approach developed here concerns the type of batch learning used. Batches constitute the learning trials over which weight updates occur and therefore likely play a role in the dynamics of how well aspects of structure are learned and the timecourse for such learning. These dynamics may be quite complex, with larger implications for the nature of knowledge development in the long term developmental process of the model and a reader that it is designed to simulate. This is no doubt an area of potentially valuable future research, though a few things can be noted here.

The assembly of minibatches in this approach is designed for reasons more related to the engineering of the computational system provided rather than a direct hypothesis about the nature of human learning in this learning domain. This specification of the architecture allows for the use of words without alignment of any kind - a feature thought to be a virtue of the approach given the potential learning challenges imposed by including empty slots in input and output representations. However, this may not be an unrelated issue to human learning. For example, the minibatch approach taken here can be implemented over batches of only one learning trial, which is termed "stochastic gradient descent" in the literature on artificial neural networks. One possibility is that human learning occurs over such minimal number of learning experiences. If so, the architecture implemented here represents a viable one for experimenting with such a possibility, which is afforded by the ability to learn without slots and alignment.