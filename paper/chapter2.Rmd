
# Chapter 2: Architecture and implementational details
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Words for training
Words were aggregated for modeling from a corpus of 250 commonly read children's books for children five years-old and younger in the United States [@Lewis2021]. This corpus was used given that a word's presence in such a database indicates that a child is likely to be exposed to it early in literacy development, at least in the US. Once collected (full criteria described below), all words were transcribed using the CMU pronunciation dictionary implemented in Python with the Natural Language Toolkit (nltk) library [@Bird2009]. 

Words of eight letters or less were compiled for the purposes of training and testing the models developed here. This number was chosen because it represents a conservative estimate of the word identification span for typical readers [@Rayner1998]. It is important to note, however, that the model is capable to processing any word regardless of its length. This is important given that while a reader - a child, for example - may only have learned words of shorter lengths, he or she nonetheless has the capacity to attempt to read words of longer lengths. This is one differences between the time distributed computational process proposed here and other connectionist models based on fixed-dimension representations in feedforward networks. Feedforward systems require pre-specification about word length that prohibit generalization to other forms that fall outside the (perceptual) window of what has been specified for the purposes of modeling the process.

In addition to the constraint placed on the number of letters, a word was included for training if it was eight phonemes or less, and three syllables or less. These constraints were included for the training set in order to avoid length effects that were outside the focus of the analyses presented here. However, the capacity of the model to accommodate longer words than those included for training is demonstrated. Also, training can happen over words (or orthographic/ phonological sequences) of unlimited length, this length was just limited in order to focus the learning process on reasonably-lengthed words from this realistic database of child-oriented language.

A number of outlier words were removed from the set due to their idiosyncratic characteristics. Words removed if they were abbreviations ("Nov") or initialisms ("bbq"). Additionally, children's books contain language that is quite different from words commonly read in other genres. These words include printed expressions of motherese ("aaaaa"), sing-song ("yipiyuk"), or forms of onomatopoeia ("zoooooom", "wooshee") that aren't well-suited for modeling the type of word reading attempted here. Single-letter words were removed as well, given that there are relatively few of these words (even though the architecture is equipped to handle such words).

Morphological variants were added to the corpus for words that possessed common variants. Sometimes, this applied to words that only appeared in their morphologically complex form in the children's book corpus, like `r scaps('adults')` (where the bare form `r scaps('adult')` was added), on other times this involved adding a morphologically complex form to the set where the simplex form was present in the children's book corpus (e.g., `r scaps('buyers')` for the word `r scaps('buyer')`).

Word frequencies for all words included in the corpus were collected using the Hyperspace Analogue to Language norms found in the English Lexicon Project database [@Balota2007]. These frequencies were used for the gradient scaling operation used in backpropagation for model training (explained further in this chapter). The purpose of using the values provided in the ELP database was to allow for more direct comparisons to behavioral data from the English Lexicon Project in the results provided.

## General description of the architecture
In this section, an initial general description of the computational architecture is provided. This is followed by a more technical description of how the architecture operates functionally and mathematically.

The architecture takes in a time distributed sequence of letters (one timestep per letter) and passes the representation of that sequence to the phonological portion of the network^[For the purposes here, the term "phonological" refers to speech sounds, their representations, and the ways that they relate to one another during reading - both in the context of the computational model and in reading more generally.]. This orthographic representation is composed of two separate vectors: the last _hidden state_ of the orthographic pattern learned along with the _cell state_, described in greater detail later. These vectors are important because they serve as the initial memory state for learning about the time-distributed phonological sequence corresponding to the visual pattern learned. This pair of representations can be thought of as the orthographic context within which a phonological sequence is learned. Importantly, while the nature of the temporal dynamics around how this representation is passed has the potential to be manipulated, in the experiments reported here the representation is built up over an entire orthographic input pattern before being passed to phonology. The information flow in this architecture approximates what we believe to be taking place at least in shorter words, and is similar to the assumptions of other learning models discussed previously.

Once the phonological network receives its initial state from orthography, it begins processing the word's corresponding phonological pattern (i.e., a sequence of phonemes representing the spoken form of a word^[Here the term "phoneme" is used to refer to a segment of a spoken word. The nature of phonemes and their emergence in development is an important topic. The simplifying assumption is made here that spoken words are made up of phonemes, though the architecture described could be easily extended to perform computations over alternative representations in the phonological portion of the network. Such an extension would be valuable given that learning about print has noteworthy effects on spoken language knowledge [@Muter2004; @Seidenberg2017].]. The time-varying way in which phonology is processed is very similar to that of orthography. The phonological input is represented with one phoneme per timestep. However, because the goal of the naming process is to produce a spoken form of the printed word, the phonological portion of the network is trained to produce the sequence of phonemes as its output. This takes place by building up a representation by the orthographic representation passed as the initial state of the phonological LSTM layer along with the phonological sequence it is drawing in as its input (once it has received the orthographic vectors for the word). 

### Phonological terminal segments
In order to understand how phonemes are specified and how they are processed, it is useful to understand that a given word's phonological input and output pattern differ in one important way. The phonological input pattern contains a special representation at the beginning of the word (i.e., as the first timestep) and the corresponding output pattern contains an end of word representation as the final timestep (but not the beginning-of-word representation). For example, the word `r scaps('breath')`, would correspond to the sequence of phonemes `r scaps('b-r-eh-th')`. CMU pronunciation dictionary representations are used to identify phonemes in a way very similar to those used in @Sibley2010. We use the CMU dictionary characters to represent the segments in this description, but keep in mind that like other connectionist models the representations are machine-readable binary vectors (these representations are described in full below).

The input pattern for this spoken word would be `r scaps('#-b-r-eh-th')`, with an output pattern of `r scaps('b-r-eh-th-%')`. The word initial representation (`r scaps('#')` in the example) allows the phonological portion of the network to be trained with knowledge of when a word should be initiated during the production of the spoken wordform corresponding the orthographic input provided, and when the production of the phonological form should be terminated (i.e., `r scaps('%')`, when the production should stop). In order to represent this segment in the binary representations used to train and test the network, a unit was included for each of the two terminal segments. When that segment was present (i.e., it was the start or the end of the word), the unit encoding the presence of that segment was set to `r scaps('1')`.

The reason for specifying phonemes in this way is related to how the phonological portion of the network learns via the time-varying phonological input pattern and and its corresponding output pattern. The input and output pattern are offset by one segment. Training in the phonological portion of the network thus takes the form of producing the next phonemic segment, one phoneme at a time. By the end of a training trial, the sequence of phonemes for the word has been produced, with error calculated with respect to each phoneme^[This method of calculating error with respect to each output segment is known as _teacher forcing_ [@Williams1989] and is used to expedite training here. Other implementations are possible and will lead to slower model convergence, but will be important in subsequent simulation work.].

### Phoneme representations
The phonological patterns are distributed representations over articulatory features common to phonetic descriptions found in the linguistic literature on the topic and similar to those used in previous connectionist models of word recognition [e.g., @Harm1999; @Harm2004; @Sibley2010], but using binary indicators for articulatory features. In order to account for phonological information relevant to words with more than one syllable, features for primary stress and for secondary stress were included in the the set of representations. Only vowels ever received a stress marking, where a given vowel could be designated as primary stress (with the primary stress unit set to `r scaps('1')`), secondary stress (with the secondary stress unit set to `r scaps('1')`), or no stress (with both stress units set to `r scaps('0')`)^[Though note that it is possible for consonants to receive stress in English [@Fudge2015], it is simply rare. Note that consonant syllabification occasionally occurs in the model reported here, which is noted in the chapter on experimental results.]. The distinction only applies in situations where primary and secondary stress are both present in the word, where in most cases where stress is present the distinction between primary stress and no stress is enough to mark the prosodic contour of the word (as in many 2-syllable words). Therefore, and in sum, the representations are those from @Harm1999 and @Harm2004 with four extra units added, two for the stress marking of the segment (primary and secondary) and two for the features representing the beginning and end of word segments, respectively. The representations can be found in Appendix XX.

Stress encodings were adopted from the CMU pronouncing dictionary. To illustrate, take the word _squirmy_. The phonological wordform is defined as `r scaps('s-k-w-er1-m-iy0')`, with the vowel marked with `r scaps('1')` receiving primary stress, and the vowel marked with `r scaps('0')` receiving no stress. For comparison, take the 3-syllable word _understand_. The phonological word in this case was defined as `r scaps('ah2-n-d-er0-s-t-ae1-n-d')`, with a distinction between primary stress (`r scaps('ah2')`), secondary stress (`r scaps('ae1')`), and no stress (`r scaps('er0')`). 

Two additional units were present on each phonological representation in order to represent the start-of-word and end-of-word segments. In each case, when that segment was used, the representation consisted of a vector in which every unit was off except for that critical unit (i.e., the one representing the "start producing" or "stop producing" feature), which was on (set to `1`). Training phonological sequences with the start-of-word segment allows for an offline production process, separate from training, where the phonological portion of the network can freely unroll a phonological sequence until it reaches the end of the word, which is marked by the corresponding end-of-word representation. Learning about these terminal points accumulates through training, with the segment occupying a timestep (either the first or last in the phonological wordform) just like any other phoneme. These elements of the training patterns are necessary given that the time-varying type of learning being modeled here benefits from explicit training on the boundaries of the phonological wordform, and is essential for being able to produce a spoken form of the printed word in "production" mode, where a spoken form unfolds over timesteps when provided only an orthographic pattern (the "context" represented by the cell state and hidden state generated by the orthographic LSTM which is passed to phonology). The production mode of the process is describe more fully belowXX.

### Orthographic representations
Each letter is defined as a binary vector with the "hot node" corresponding to the sequential index of that letter in the alphabet, where the vector is 26 units long. So, the vector for the letter `r scaps('a')` is 26 units long, with the first unit set to `r scaps('1')` and the 25 units after it set to `r scaps('0')`. This is very similar to how letters are represented in other connectionist learning models [@Cox2019]^[However, note that these representations are quite different than the related architecture reported in @Sibley2010, where letters were represented as conjoined patterns with adjacent letters in order to expedite training.], except that the letter representations are processed one at a time and aren't assembled into padded ensembles for a given word.

Relatedly, an important detail to keep in mind in terms of how these representations are processed is that no centering or justification is used because the LSTM functions as a loop, with the loop running over _n_ iterations, where _n_ is defined by the number of letters in the word^[Another relevant detail here concerns the batched nature of training, which is discussed in the section on batch learning]. As a result, a training trial's orthographic input processing is always straightforward. The input pattern contains a representation for each letter of the word, with the representation for that letter being one timestep. As a result the orthographic LSTM operates over some number of timesteps, defined by the number of letters in the word. 

As a result of these features, the phonological portion of the network operates as a subnetwork within the larger architecture, though its behavior is heavily influenced by orthography. The way that the phonological network operates allows it to encode internal representations of phonology in a time-distributed way, while mixing in a straightforward and cognitively plausible way with orthographic information taken in and processed by the orthographic layer. This allows the architecture to maintain a critical phonological component, but also allowing information processing across orthography and phonology in a way that resembles previous learning architectures and human cognition. The primacy of phonology and phonological learning in reading is not controversial, though there are debates about the processes at play in how spoken language units are assembled during reading as a function of visual processes [@Rastle2010].

### Possibilities of different inputs to be present during training
The training trial process described above corresponds to a teaching signal where the learner is both hearing and and seeing the visual word while receiving feedback about the word that should be produced given the input. This is an important part of any cross modal form of learning; learning is enhanced by experiences in which the information signal for both modalities are present - a method in reading education called "reading while listening" [@Reitsma1988; @VanBon1991]. However, learning to read (as in other forms of learning) also (in fact, most often) involves the production of spoken language from the visual signal alone ^[Note that the term "production" is used throughout this work to mean the generation of a spoken form from some input]. This training scenario of considered further in the concluding chapter. The learning process used in all the simulations reported here concerns receiving orthographic and phonological inputs during training, though similar performance is obtained when training on the phonological temporal signal alone on input (i.e., information about how many timesteps/phonemes should pass in computing a phonological output).

## Production (testing) process
A different method is used for the purposes of testing the network's ability to produce a phonological form when provided an orthographic input pattern, though the different processes aren't fundamentally different in terms of the information processing that takes place. They only differ procedurally. On production (what we could also think of as "testing") trials, an orthographic input pattern is provided, and its activation state builds up over the entire time-varying pattern (for our purposes here this is an entire word). This input process is identical to the one that occurs during training, where each letter is provided as a timestep, and the activation state of the layer is provided as a cell state and hidden state (of the final timestep).

After the orthographic pattern is passed to phonology, the `#` segment is provided as a single timestep to the phonological LSTM, indicating that production should take place. From there, information propagates forward through the phonological LSTM as it would on any other trial, producing dynamics over its internal state as it usually does, though only for the single timestep. An output pattern is produced and recorded as the first segment of the word, with the intention of building on that segment until the model is done producing the word. Given that the network never perfectly produces a phoneme (in a unit-wise sense), in order to approximate the speech sound produced the nearest phoneme to the one produced is calculated and recorded for the trial using an L2 norm. This calculation is similar to many other implementations of similar phonological production processes from the connectionist literature (ref.). After the speech sound is recorded, it is passed back to the input in order to determine the next speech sound in the word. Because the state of the network is maintained from the prior timestep, which was initiated by the representation of the printed word passed from the orthographic layer, the effect of this process is essentially, unfolding the phonological wordform one segment at a time. When subsequent phonemes are produced, they are added to those generated previously, constituting a produced word that unfolds over continuous time. The production process ends for a given word when the network produces the stop segment (`%`), the boundary which has been learned during training. Alternatively, a limit to the total number of phonemes produced can be set, where the production ends and the experimenter is able to observe what is producedXX.

At this point, the reasoning behind having separate training and testing trials should be more clear. When assessing the ability of the network to produce a complete word, this process allows the phonological layers of the network to settle on a state for each segment of the production, while also being independent in its ability to complete the process of producing a spoken form of the word over continuous time. In this mode, the model is unconstrained with respect to how long it engages in production. This aspect of the process is important given the temporal nature of spoken language production and the challenges this time-varying process presents in terms of computational modeling. A temporal signal needs to drive the process if this aspect of the action is to be simulated, and the dynamical but independent nature of the production process as described here allows this to take place in a realistic manner in the sense that when we read a word we do so until the utterance terminates. This is different than other computational implementations of reading models where, typically the phonological output is of a _fixed_ dimensionality, which assumes that the named word always encodes something about the phonological length of all words it has ever learned (e.g., in spatial models that produced a vowel-centered phonological output padded by `_` on the left and right side to accomodate the longest trained word).

As an additional point of comparison, other computational models of reading have also used different training and testing processes. For example, the CPD++ model in @Perry2010 used different procedures, which they termed "training" mode and "running" mode. In their case, the distinction was quite dramatic, involving rather different mechanisms for learning and producing. For example, training involved learning over pre-specified, structured grapheme-phoneme pairs, among other structured experience to support the development of knowledge in the system. The authors engineered this process as an extension from well-establish phonics instructional methods in education (i.e., that use grapheme-phoneme correspondences), rather than by some more fundamental learning mechanism. In some cases, entirely different learning mechanisms were introduced in order to learn common tricky orthographic wordforms, like those that show a vowel-consonant-e pattern (e.g., _bite_). In other cases, additional "backup" strategies were developed in order to assign letters in the orthographic pattern to slots in the syllabic template operating in the system (see p. 122 for discussion of the nonword example _fanj_). By contrast, testing involved a separate process of aligning an orthographic input to build a hypothetical phoneme string based on the learned grapheme-phoneme associations. In the proposed architecture here, the difference between training and testing process isn't a mechanistic one, rather simply implements a different (but highly related) temporal process for the purposes of generating a response.

## Anatomy of an LSTM
Recurrent neural networks have been influential in the evolution of cognitive science in previous decades [@Elman1990]. Cognition is subject to time-sensitive processes, and thus it is important to understand how to represent time in the cognitive models we use to understand human behavior. This is related to the core assumtion underlying this dissertation, namely that models of reading should account for temporal processes relevant to reading behavior.

Recurrent neural networks received a considerable boost in their sophistication with the development of the long short term memory unit, and since their development the architecture has gained traction in cognitive science [ref.]. The LSTM was developed in order to account for a computationally important problem: how can representations of experiences from the past be maintained in systems that learn over long, temporally-distributed sequences of events. This problem most clearly manifests in understanding the gradients used for weight updates in simple recurrent networks (SRNs) [@Elman1990]. In SRNs, knowledge is typically updated after the system receives a input in the form of a single event (timestep) of a temporally distributed sequence of events. This might be a sequence of words, phonemes, visual events, etc. However, for events that maintain a distal temporal relationship (i.e., long-distance dependencies), the gradient is not maintained, at least directly. This is commonly described as the _vanishing_ gradient problem, where error signals over long temporal distances become increasingly dissociated. This problem becomes particularly clear in the application of such architectures to language processing. For example, anaphoric reference often requires an unambiguous association between a nominal referent and its distal pronoun, as in the sentence " _Sam_ looked at the monkey in the cage and as though the expectation was for thousands and thousands of years of evolution to be undone, motioned for an object sitting on the ground to be passed over to _her_." 

A related problem can also occur when the error signal across proximal timesteps is too large (rather than too weak as before). This has been labeled the _exploding_ gradient problem. Given that SRNs function as a many layer feedforward network, where the depth of the network corresponds to the number of timesteps over which inputs will vary for an example. When correlational structure is distal across timesteps, the error signal becomes to weak, and the gradients vanish. When the correlational structure across timesteps is too strong, the gradients become very strong due to their high correlation, leading to volatile behavior as weights are updated in the system. 

LSTM networks, a single instance of such a network we will call a _block_, solve this problem by governing the extent to which information received at a previous point in time is relevant at the current moment in time. This is achieved by maintaining memory about the input pattern across timesteps in a representational state called the _cell state_. Information about the current input (timestep) may to flow into it on an attenuated basis using _gates_, which are themselves neural networks with trainable parameters that operate within the larger neural network structure. There are several gates, and they will be described based on what they contribute to the process. In essence though, they allow the representational state to be altered such that information is either dropped, added, or passed off to future timesteps.

At any particular timestep, for sake of description let's imagine the first in a sequence for a example (e.g., a word), the input vector is provided to the network. For our purposes, we can think of the example to be the printed word, _breath_, so the first timestep (if letters are designated as the temporal segment) would be the letter _b_. Unless one is provided, there is no existing representational state for that example because this is the first timestep.  In the case of our architecture, the vector representing this letter consists of as many units as there are letters in the alphabet, with the $i$th unit set to `1` and all others set to `0`, where $i$ is the sequential index of where that letter occurs in the alphabet (e.g., the letter _b_ would have the $2$nd unit on). Upon input, the vector is combined (via concatenation) with the hidden representation of the timestep prior. In the case of the first segment (timestep) of the example (orthographic wordform), this hidden representation is a vector of units all with the value `0`. The length of the hidden representation is as long as there are units in the input vector, which will become more clear when we understand how hidden representations for subsequent timesteps are formed. Once these two vectors are concatenated the resulting vector is passed to each of the four gates within the LSTM block.

Each gate functions similarly internally but is combined with the maintained representational state of the LSTM block in different ways to produce different memory effects based on the particular gate's intended function. The process described here happens within any single timestep before passing the state to the next timestep (timesteps iterate within a loop defined by the total number of timesteps in the example). We can think of the separate gating processes as happening in a sequential way, which can aid in understanding how they function relative to one another, though some of the computation happens in parallel which is made feasible computationally using current libraries and hardware designed for fast, efficient, parallel computation with neural networks such as LSTMs.

The concatenated input is passed to a gate whose job it is to attenuate the cell state in a way that diminishes information that isn't relevant to the ongoing memory of the system. Of course, early in the sequence, the cell state hasn't built up information about the temporal dependencies present in the sequence, so this attenuation is minimal. The gate is a neural network with sigmoidal activation function applied upon output, thereby passing values between zero and one to be combined with the cell state before the signal is moved along to the next gating routine. The output of the forget gate is combined with the cell state using multiplication. This has the effect of maintaining activation states over units of the cell state when the highest value possible is passed through the forget gate (`1`), and minimizing the values for the lower end of the scale of values passed through (when `0`). Once combined, the cell state is passed to the next gating functions. The formula for the forget gate portion of the LSTM is shown in XX.
$$
f_t = \sigma(W_fx_t + U_fh_{t-1} + b_f)
$$
Here, $W$ and $U$ represent the learned weights of the gate $f$ at a particular timestep $t$. A given input representation at time $t$ is shown as $x_t$, and the representation passed from the previous timestep is $h_{t-1}$. In prose, the latter is referred to as a _hidden representation_ and the former as an _input representation_, though the equation makes clear the symmetry between these two vectors given that they function in very similar ways with respect to any given gate (here, the forget gate but the symmetry is clear in other gates as well). The element $b_f$ is a bias unit included in the gate. The output is passed through a sigmoidal transfer function, $\sigma$, before it combines with the cell state.

The concatenated input (plus hidden) vector is also passed to a set of gates called the _input gate_ and _cell gate_^[The name "input gate" is conventional and used consistently throughout the literature, however the name of this other gate, which has been termed the "cell gate" here is not. We will call it this for the time being, however note that the name is somewhat misleading. Its relationship to the cell state of the LSTM is no more privileged than other gates within the LSTM block. The term "gate" is used here for both (in fact all) gates in order to underscore their architectural similarity, though the terminology may differ slightly from descriptions elsewhere.], whose activity is coordinated and thus described together. This coordination occurs with respect to their outputs and how they are combined before they are passed to the cell state; their internal functioning is independent of each other.

The gates each receive the concatenated hidden and input vectors, just like the forget gate. This longer vector is passed to a neural network with trainable parameters, and outputs a signal. The output is summed and passed through either a sigmoidal transfer function in the case of the input gate or a hyperbolic tangent function ($tanh$) in the case of the cell gate, these are shown in XX and XX. For notation, we will use the symbol $\Im$ to represent the input gate and the symbol $\tilde{c}$ is used to represent the cell gate.
$$
\Im_t = \sigma(W_{\Im}x_t + U_{\Im}h_{t-1} + b_{\Im})
$$
$$
\tilde{c}_t = \tanh(W_{\tilde{c}}x_t + U_{\tilde{c}}h_{t-1} + b_{\tilde{c}})
$$
Their outputs are then combined via multiplication before being added to the cell state, and at this point the cell state has already been attenuated by the output of the forget gate, $f_t$. This whole process is represented in XX and defined in terms of the cell state $c$. This equation accounts for the final cell state at the end of the current timestep, and is passed on to the next timestep as its cell state to be further updated.
$$
c_t = f_t \cdot c_{t-1} + \Im_t \cdot \tilde{c}_t
$$
The last gate involved in the function of the block is the _output gate_, referenced with the notation $o$. Its job is to determine what activation gets passed on to the next timestep in the form of a hidden representation. The function of this gate helps to make clear why this vector is termed the _hidden representation_: it is the emergent representation of the current timestep (segment) that combines information from the input, the previous hidden representation, and the cell state of the block. It essentially combines these different, related information sources (external perceptual information and internal memory representations) in the form of a single vector that represents information about that timestep for the next timestep. This vector is different than the cell state itself, which is characterized elsewhere here.

The output gate internally functions identically to both the forget gate and the input gate^[These three gates only differ internally from the fourth, the cell gate, because of the transfer function used (i.e., $\sigma$ instead of $\tanh$).], and its formula is shown in XX.
$$
o_t = \sigma(W_ox_t + U_oh_{t-1} + b_o)
$$
This gate receives a concatenated input ($h_{t-1}$ with $x_t$), which are passed through trained weights. The output is then summed and a sigmoidal transfer function is applied. Importantly, the output signal is used differently than other gates. The vector is multiplied against a copy of the cell state (a copy which has subsequently been transformed using a $\tanh$ function). This is shown in XX, where $h_t$ in the previous description should be thought of as the hidden state that the current timestep produces for the next timestep.
$$
h_t = o_t \cdot \sigma_h(c_t)
$$

This operation allows for the newly created hidden representation to be of the appropriate length, given that the output of the operation is determined by the dimensions of the cell state (i.e., the hidden state dimensions are the same as the cell state dimensions). The output of this set of operations is then passed on as the hidden representation used in the next timestep.

This process is repeated across as many timesteps as occur in the input. As a result, when an example (e.g., an orthographic wordform) terminates, what's left is the cell state and a hidden state. The cell state is in its final state for a given timestep once it has been combined via addition with the (combined) output of the input and cell gates. The hidden representation is ready for use on the subsequent timestep once it has been combined via multiplication with the $\tanh$ transformed cell state as described in the above section on the output gate and shown in XX. As a result, any LSTM layer has the capacity to pass the cell state as well as all hidden states from each timestep of the example if necessary or useful (not just the hidden state of the final timestep). This distinction is important in terms of the representational capacity of the layer The cell state should be seen as a memory representation of the example as a whole, whereas the hidden state of the last timestep is more narrow in its representational capacity. The information contained in this vector is more relevant to the final timestep itself, though has been affected directly (via a $\tanh$ scaling operation) by the cell state, and so contains a trace of information relevant to the entire word.

At this point a few important aspects of the cognitive nature of such a system can be foregrounded in computational terms. Statistical learning systems, like the artificial neural network described here, should account for the statistical dependencies of the percepts they experience. Feedforward networks accomplish this by taking in information on an input layer (like orthography) and updating a set of trained weights with respect to error calculated on the output layer. Dependencies are encoded in the weights given the degree of covariation between the input pattern and its corresponding output pattern, taking into account this covariation across all learning trials. This is a spatial mapping given that the covariation, which is captured by the dependencies present within a given learning trial, happens without time distribution on the input and output. The LSTM performs a spatial mapping for a single timestep, but also captures dependencies across timesteps, capturing the dynamics of segments across the training example.

The dynamics specifically play out with respect to the two outputs served up by the block: the cell state and the hidden state. While these two vectors will always bear a relation to one another, most specifically in terms of the combination that occurs on the output gate (shown in XX), they are computationally and functionally distinct. The cell state serves as the memory state that captures statistical dependencies across all timesteps of a learning trial (letters or phonemes across words) -- a distal relationship -- and the hidden state captures the proximal analogue: the emergent representation of the current segment with respect to the cell state.

Without gating mechanisms that allow information to flow into the long term memory of the example, these long term dependencies can't be captured. Likewise, without the ability to combine information about the long term dependencies with information being processed at a given timestep (i.e., the combining function of the output gate and the cell state that creates the hidden state $h_t$), proximal information about the segment can't be transferred from one segment to the next. So, the two internal "state" elements of an LSTM have the capacity to capture something long-term (the cell state) and short term (the hidden state) across a training trial.

This relates directly to aspects of human behavior in important ways. For example, the debate about the viability of connectionist models of word reading are sometimes pitched in terms of a model's capacity (or lack thereof) to account directly for segmental phenomena found in experimental work with humans [@Kawamoto2015]. The temporal model developed here is capable of addressing such issues given its ability to simulate phenomena related to segments due to the fact that segments are part of the temporal phenomena related to speech and print.  Introspection about different representational grain sizes in the perceptual inputs of the learning system should be possible in a system that is concerned with learning that involves segments, and this is the case for the architecture being proposed. This capacity deviates sharply from other models of word reading given the limitations of purely spatial learning systems, and the ways in which representational schemes for letters and sounds further exacerbate the capacity to introspect about segmental properties of the learning they involve. For example, the LSTM architecture captures more closely that the learner has representations for "sublexical" units possessed by any given input or output, and that the representational capacity can be observed by observing the model respond to that sublexical unit directly. For example, individual letters and letter sequences have pronunciations, which can be directly observed by setting up a testing trial using the network. This isn't possible in feedforward networks due to the abstract ways in which segments are specified and processed in those systems. This virtue of the architecture will be returned to in the results when we look at the dynamics of a training trial and the types of targeted phenomena the network can simulate for segments of language.



## Representational virtues
Readers can process, correctly or not, letter strings of any length. Any model architecture of producing phonology from orthography should be able to account for this fact. Also, this should include the ability to read any word, regardless of the length of its spoken form. There should be a direct relationship between the length of a word's printed or spoken form and its representation used by any computational architecture that consumes it. This relationship is maintained in the current architecture in a very direct way. The representation of the printed form of a word is generated letter-for-letter, where the orthographic representation is as long as there are letters in the printed form. The phonological representation is formed in a similarly veridical way. A spoken form is defined as a sequence of phonemes, which is a well-established level of linguistic representation for the spoken form of words. The output representation for a word is build phoneme-by-phoneme from the word's phonemic form, where each phoneme is defined as a distributed pattern over phonemic features, as described elsewhere here. Therefore, the length of the spoken form of the word is defined by the number of phonemes in the word, and the complexity of the representation can be estimated by the number of features on over an individual or string of phonemes. This representational scheme is intended to represent the structure of the language in a way that is veridical, or at least more veridical than other implementations of similar models.

### Justifications for justification
A number of alternative schemes have been entertained in other models of word naming. The most common approach involves specifying the orthographic and phonological representations using vowel centered representations [@Chang2019; @Cox2019]. For models of reading that deal in monosyllabic words this approach captures something appropriate about vision, given that an entire printed word can be taken in on a single fixation if the word is short. Granting this assumption, learning that the centered pattern `__breath__` maps onto `_ - _ - B - R - EH - TH - _ - _` is a defensible simplifying assumption.

However, procedures like that require justification of some kind, typically in the center with both left and right pads, introduce a computational problem: perceptual segments come to occupy slots in their representational domains. For example, the letter _b_ from `__breath__` comes to be represented in a different way than that same letter in other positions (e.g., _cob_, or _curb_). Some computational architectures have tried had to work around this issue one way or another, usually by abstracting away from positions to higher order linguistic units. For example, @Plaut1996 represented input and output patterns based on the syllabic structure which a given segment is commonly associated. Their method was developed in the wake of other procedures designed to encode just enough of the relative structure of segments in words to be suitable for modeling monosyllabic words. For example, see the discussion in @Seidenberg1989 and their use of orthographic and phonological "triples" [@Wickelgren1969] to work around this problem, which is thought to be a plausible alternative given that such representations build in contextual information that helps the model abstract away fro position-specific representations in the input and output layers.

Issues around positional representations and dispersion All previous connectionist models of word recognition have involved spatial (and in some cases over temporally varying segments) justification of one kind or another, usually implemented as empty slots to the right of an orthographic or phonological pattern (e.g., `r scaps('breath____')`. When present, this feature of an architecture (and the patterns that are used to train it) represents a deep limitation in its psychological plausibility. While including justification in some form is understandable as an aspect of the engineering problem that computational modelers face in modeling visual (or other) word recognition processes, they assume that the patterns that are used in orienting words, left or right, on their input and/ or output layers (however the justification might be implemented) are an aspect of the learning process, though


