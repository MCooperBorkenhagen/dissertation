
# Chapter 2: Architecture and implementational details
## Training details
#### Training environment
Words were aggregated for modeling from a corpus of 250 commonly read children's books for children five years-old and younger in the United States [@Lewis2022]. This corpus was used given that a word's presence in such a database indicates that a child is likely to be exposed to it early in literacy development, at least in the US. Once collected (full criteria described below), all words were transcribed using the CMU pronunciation dictionary implemented in Python with the Natural Language Toolkit (nltk) library [@Bird2009]. 

Words of eight letters or less were compiled for the purposes of training and testing the models developed here. This number was chosen because it represents a conservative estimate of the word identification span for typical readers [@Rayner1998]. It is important to note, however, that the model is capable to processing any word regardless of its length. This is important given that while a reader - a child, for example - may only have learned words of shorter lengths, she or he nonetheless has the capacity to attempt to read words of longer lengths. This is one difference between the time-varying computational process proposed here and other connectionist models based on fixed-dimension representations in feedforward networks. Feedforward systems require pre-specification about word length that prohibit generalization to other forms that fall outside the (perceptual) window of what has been specified for the purposes of modeling the process.

In addition to the constraint placed on the number of letters, a word was included for training if it was eight phonemes or less, and three syllables or less. These constraints were included for the training set in order to avoid length effects that were outside the focus of the analyses presented here. It is worthwhile to note that training can happen in the architecture presented over words (or orthographic/ phonological sequences) of unlimited length; word lengths were limited only in order to focus the learning process on reasonably-lengthed words from this realistic database of child-oriented language.

A number of outlier words were removed from the set due to their idiosyncratic characteristics. Words removed if they were abbreviations ("Nov") or initialisms ("bbq"). Additionally, children's books contain language that is quite different from words commonly read in other genres. These words include printed expressions of motherese ("aaaaa"), sing-song ("yipiyuk"), or forms of onomatopoeia ("zoooooom", "wooshee") that aren't well-suited for modeling the type of word reading attempted here. Single-letter words were removed as well, given that there are relatively few of these words (even though the architecture is equipped to handle them).

Morphological variants were added to the corpus for words that possessed common variants. Sometimes, this applied to words that only appeared in their morphologically complex form in the children's book corpus, like `r scaps('adults')` (where the bare form `r scaps('adult')` was added), or other times this involved adding a morphologically complex form to the set where the simplex form was present in the children's book corpus (e.g., `r scaps('buyers')` for the word `r scaps('buyer')`).

Word frequencies for all words included in the corpus were collected using the Hyperspace Analogue to Language norms found in the English Lexicon Project database [@Balota2007]. These frequencies were used for the gradient scaling operation used in backpropagation for model training (explained later in this chapter). The purpose of using the values provided in the ELP database was to facilitate direct comparisons to behavioral data from the English Lexicon Project.

#### General description of the architecture
In this section, an initial general description of the computational architecture is provided. This is followed by a more technical description of how the LSTM portion of the architecture operates functionally and mathematically (in the section titled _Anatomy of the time-varying layer_).

The architecture takes in a time-varying sequence of letters (one timestep per letter) and passes the representation of that sequence to the phonological portion of the network^[For the purposes here, the term "phonological" refers to speech sounds, their representations, and the ways that they relate to one another during reading - both in the context of the computational model and in reading more generally.]. This orthographic representation is composed of two separate vectors: the last _hidden state_ of the orthographic pattern learned along with the _cell state_, described in greater detail later. These vectors are important because they serve as the initial memory state for learning about the temporal phonological sequence corresponding to the visual pattern learned. This pair of representations can be thought of as the orthographic context within which a phonological sequence is learned. Importantly, while the nature of the temporal dynamics around how this representation is passed has the potential to be manipulated, in the experiments reported here the representation is built up over an entire orthographic input pattern before being passed to phonology. The information flow in this architecture approximates what we believe to be taking place at least in shorter words, and is similar to the assumptions of other learning models discussed previously.

Once the phonological network receives its initial state from orthography, it begins processing the word's corresponding phonological pattern - that is, a sequence of phonemes representing the spoken form of a word^[Here the term "phoneme" is used to refer to a segment of a spoken word. The nature of phonemes and their emergence in development is an important topic. The simplifying assumption is made here that spoken words are made up of phonemes, though the architecture described could be easily extended to perform computations over alternative representations in the phonological portion of the network. Such an extension would be valuable given that learning about print has noteworthy effects on spoken language knowledge [@Muter2004; @Seidenberg2017].]. The time-varying way in which phonology is processed is very similar to that of orthography. The phonological input is represented with one phoneme per timestep. However, because the goal of the naming process is to produce a spoken form of the printed word, the phonological portion of the network is trained to produce the sequence of phonemes as its output. This takes place by building up a representation with the orthographic representation passed as the initial state of the phonological LSTM layer along with the phonological sequence it is drawing in as its input (only after it has received the orthographic vectors for the word). The initial state of the phonological portion of the network is updated as timesteps progress over phonemes, but it is initialized as the final state passed from orthography.

#### Phonological terminal segments
In order to understand how phonemes are specified and how they are processed, it is useful to understand that a given word's phonological input and output pattern differ in one important way. The phonological input pattern contains a special representation at the beginning of the word (i.e., as the first timestep) and the corresponding output pattern contains an end of word representation as the final timestep (but not the beginning-of-word representation). For example, the word `r scaps('breath')`, would correspond to the sequence of phonemes `r scaps('b-r-eh-th')`. CMU pronunciation dictionary representations were used to identify phonemes for printed words in the corpus in a way very similar to those used in @Sibley2010. We also use the CMU dictionary characters to represent the segments in this description (see Appendix A for a key), but keep in mind that like other connectionist models the representations are machine-readable binary vectors (these representations are described in full below).

The input pattern for this spoken word would be `r scaps('#-b-r-eh-th')`, with an output pattern of `r scaps('b-r-eh-th-%')`^[Note that these are string/symbolic versions of what is implemented computationally as binary vectors.]. The word-initial representation (`r scaps('#')` in the example) allows the phonological portion of the network to be trained with knowledge of when a word should be initiated during the production of the spoken wordform corresponding the orthographic input provided, and when the production of the phonological form should be terminated (i.e., `r scaps('%')`, when the production should stop). In order to represent this segment in the binary representations used to train and test the network, a unit was included for each of the two terminal segments. When that segment was present (i.e., it was the start or the end of the word), the unit encoding the presence of that segment was set to `r scaps('1')`.

The reason for specifying phonemes in this way is related to how the phonological portion of the network learns via the time-varying phonological input pattern and and its corresponding output pattern. The input and output pattern are offset by one segment. Training in the phonological portion of the network thus takes the form of producing the next phonemic segment, one phoneme at a time. By the end of a training trial, the sequence of phonemes for the word has been produced, with error calculated with respect to each phoneme^[This method of calculating error with respect to each output segment is known as _teacher forcing_ [@Williams1989] and is used to expedite training here. Other implementations are possible and will lead to slower model convergence, but will be important in subsequent simulation work.].

#### Phoneme representations
The phonological patterns are distributed representations over articulatory features common to phonetic descriptions found in literature in theoretical linguistics, and similar to those used in previous connectionist models of word recognition [e.g., @Harm1999; @Harm2004; @Sibley2010], but using binary indicators for articulatory features. In order to account for phonological information relevant to words with more than one syllable, features for primary stress and for secondary stress were included in the the set of representations. Only vowels ever received a stress marking, where a given vowel could be designated as primary stress (with the primary stress unit set to `r scaps('1')`), secondary stress (with the secondary stress unit set to `r scaps('1')`), or no stress (with both stress units set to `r scaps('0')`)^[Though it is possible for consonants to receive stress in absence of an articulated vowel in English [@Fudge2015], it is considered rare. Note that consonant syllabification occasionally occurs in the model reported here, which is noted in the section in Chapter 3 on experimental results.]. The distinction only applies in situations where primary and secondary stress are both present in the word, where in most cases where stress is present the distinction between primary stress and no stress is enough to mark the prosodic contour of the word (as in many 2-syllable words). Therefore, and in sum, the representations are those from @Harm1999 and @Harm2004 with four extra units added, two for the stress marking of the segment (primary and secondary) and two for the features representing the beginning and end of word segments, respectively. The representations can be found in Appendix A.

Stress encodings were adopted from the CMU pronouncing dictionary. To illustrate, take the word `r scaps('squirmy')`. The phonological wordform is defined as `r scaps('s-k-w-er1-m-iy0')`, with the vowel marked with `r scaps('1')` receiving primary stress, and the vowel marked with `r scaps('0')` receiving no stress. For comparison, take the 3-syllable word `r scaps('understand')`. The phonological word in this case was defined as `r scaps('ah2-n-d-er0-s-t-ae1-n-d')`, with a distinction between primary stress (`r scaps('ah2')`), secondary stress (`r scaps('ae1')`), and no stress (`r scaps('er0')`). 

Two additional units were present on each phonological representation in order to represent the start-of-word and end-of-word segments. In each case, when that segment was used, the representation consisted of a vector in which every unit was off except for that critical unit (i.e., the one representing the "start producing" or "stop producing" feature), which was on (set to `r scaps('1')`). Training phonological sequences with the start-of-word segment allows for an offline production process, separate from training, where the phonological portion of the network can freely unroll a phonological sequence until it reaches the end of the word, which is marked by the corresponding end-of-word representation. Learning about these terminal points accumulates through training, with the segment occupying a timestep (either the first or last in the phonological wordform) just like any other phoneme. These elements of the training patterns are necessary given that the time-varying type of learning being modeled here benefits from explicit training on the boundaries of the phonological wordform, and is essential for being able to produce a spoken form of the printed word in "production" mode, where a spoken form unfolds over timesteps when provided only an orthographic pattern (the "context" represented by the cell state and hidden state generated by the orthographic LSTM which is passed to phonology). The production mode of the process is described more fully below.

#### Orthographic representations
Each letter is defined as a binary vector with the "hot node" corresponding to the sequential index of that letter in the alphabet, where the vector is 26 units long. So, the vector for the letter `r scaps('a')` is 26 units long, with the first unit set to `r scaps('1')` and the 25 units after it set to `r scaps('0')`. This is very similar to how letters are represented in other connectionist learning models [@Cox2019]^[However, these representations are quite different than the related architecture reported in @Sibley2010, where letters were represented as conjoined patterns with adjacent letters in order to expedite training.], except that the letter representations are processed one at a time and aren't assembled into padded ensembles for a given word.

Relatedly, an important detail to keep in mind in terms of how these representations are processed is that no centering or justification is used because the LSTM functions as a loop, with the loop running over _n_ iterations, where _n_ is defined by the number of letters in the word^[Another relevant detail here concerns the batched nature of training, which is discussed in the section on batches used during learning.]. As a result, a training trial's orthographic input processing is always straightforward. The input pattern contains a representation for each letter of the word, with the representation for that letter being one timestep. As a result the orthographic LSTM operates over some number of timesteps, defined by the number of letters in the word. After the orthographic state is passed to phonology, a corresponding loop happens over the quantity of phonemes possessed by that word (plus the start of word symbol).

As a result of these features, the phonological portion of the network operates as a subnetwork within the larger architecture, though its behavior is heavily influenced by orthography. The way that the phonological network operates allows it to encode internal representations of phonology in a time-varying way, while mixing in a straightforward and cognitively plausible way with orthographic information taken in and processed by the orthographic layer. This allows the architecture to maintain a critical phonological component, but also allowing information processing across orthography and phonology in a way that resembles previous learning architectures and human cognition. The primacy of phonology and phonological learning in reading is not controversial, though there are debates about the processes at play in how spoken language units are assembled during reading as a function of visual processes [@Rastle2010].

## Conceptual depiction of processing in the model
Figure 1 shows a conceptual depiction of the architecture, which we will call a "T" network. The name is intended to remind us about the temporal orientation of the model (hence "T") and the shape of the network, which is related to the architectural features that allow the temporal information from orthography and phonology to combine to read aloud a word. The point of combination is depicted as the point of intersection between the two legs of the structure shown.


```{r TNetworkFigure, echo=FALSE, out.width='300px', fig.align='center', fig.cap='A conceptual depiction of the model reported is shown. Each arm of the T corresponds to a domain of perception and in the case of the horizontal arm corresponding to spoken language processing, of perception and action. Arrows indicate the direction of processing.'}
include_graphics('img/T_network.png')
```

This depiction is intended be a simplified one of the architecture, and to emphasize a higher level generalization of the information processing at play, namely the way that time-varying orthographic and phonological signals mix in order to produce an output. See the section below (*Depiction of full architecture*) for a more detailed schematic of the entire architecture. 

#### Possibilities of different inputs to be present during training
The training trial process described above corresponds to a teaching signal where the learner is both hearing and and seeing the visual word while receiving feedback about the word that should be produced given the input. This is an important part of any cross modal form of learning; learning is enhanced by experiences in which the information signal for both modalities are present - a method in reading education called "reading while listening" [@Reitsma1988; @VanBon1991]. However, learning to read (as in other forms of learning) also (in fact, most often) involves the production of spoken language from the visual signal alone ^[The term "production" is used throughout this work to mean, simply, the generation of a sequence of phonemes at the output layer for some input.]. This training scenario is considered further in the concluding chapter. The learning process used in all the simulations reported here concerns receiving orthographic and phonological inputs during training, though similar performance is obtained when training on the phonological temporal signal alone on input (i.e., information about how many timesteps/phonemes should pass in computing a phonological output).

#### Two different ways to produce phonological output
There are two different methods possible for testing the model on what it produces for a given word. They aren't fundamentally different in terms of the information processing that takes place. They only differ procedurally. First, a word can be tested in the same way that it trains: by providing a paired phonological and orthographic input. This will have the result of producing a phonological output and following the processing steps described already (where the orthographic layer passes its final orthographic representation for that word to then mix with phonology). For clarity, I will call this method of producing output from input _testing mode_. Most of the analyses reported focus on error data generated from this method.

The other way that the network can be tested in its ability to produce phonology is what we will call _production mode_^[Note that the results don't differ across the two methods of producing output for the network, rather they are used for different purposes when reporting results. Also, testing mode is much more efficient (takes much less time) than the production mode because it more straightforwardly leverages the graphical computing hardware also used for training the network. As a result, testing mode allows results to be generated much more easily (ten times more quickly). Later work could be focused on the different modes, how they differ, and ways of making both equally efficient for computing purposes. For now, I will use the two methods for different basic purposes.]. In the analyses reported in Chapter 3, this method is used to examine the production of the network at the end of training because it allows for easy generation of the string (i.e., human-readable) forms of the model's productions having gone through its primary training period (though quantitative analyses for these productions are also reported). Using production mode, an orthographic input pattern is provided, and its activation state builds up over the entire time-varying pattern in exactly the same way during training and testing. Each letter is provided as a timestep, and the activation state of the layer is provided as a cell state and hidden state (of the final timestep) after the last letter. 

After the orthographic pattern is passed to phonology, the `r scaps('#')` segment is provided as a single timestep to the phonological LSTM. Conceptually, we can think of this as indicating to the network that production should take place. From there, information propagates forward through the phonological LSTM as it would on any other trial, producing an output over its internal state as it usually does, though only for the single timestep. An output pattern is produced and recorded as the first segment of the word, with the intention of building on that segment until the model is done producing the word.

Given that the network only ever produces a binary vector representing its approximation of a target phoneme, a nearest phoneme needs to be calculated from the representation it produces. In order to do this, the nearest phoneme is calculated using an L2 norm. The nearest phoneme is recorded for that timestep (both its string/symbolic form and its vector form). After the speech sound is recorded, the vector produced is passed back to the input in order to continue on in the production and determine the next speech sound in the word^[This process is very similar to what happens in training and testing. Instead of the target phoneme representation being presented at input, it is instead the network's prediction about what that phoneme is. Therefore, these processes differ only to the extent that the network has faulty predictions about what the phoneme is at a given point in the temporal process. This is one reason why it is important for the network to have obtained a high level of knowledge before using productions of this kind (i.e., this is why production mode is used to inspect words produced by the network at the end of training).].

Because the state of the network is maintained from the prior timestep the effect of this process is essentially, unfolding the phonological form of the word one segment at a time. When subsequent phonemes are produced, they are added to those generated previously, constituting a produced word that unfolds over continuous time. At each timestep, once a phoneme is produced as output, it then becomes the input for the subsequent timestep's prediction, just as described above. The production process ends for a given word when the network produces the stop segment (`r scaps('%')`), the word boundary which has been learned during training. Alternatively, a limit to the total number of phonemes produced can be set, where the production ends and the experimenter is able to observe what is produced (this is the same as the process for recording outputs described in Sibley et al., 2010).

At this point, the difference between production and testing trials should be more clear, as should their similarity. When assessing the ability of the network to produce a complete word, the production mode process allows the phonological layers of the network to settle on a state for each segment of the production by dictating its own input at a given timestep in phonological production. This also represents a process for the network wherein is is independent in its ability to produce a spoken form of the printed word over continuous time. In this mode, the model is unconstrained with respect to how long it engages in production.

This method of testing the knowledge of a trained network is noteworthy given the temporal nature of producing a phonological code when reading, and the challenges that time-varying processes present in terms of computational modeling. A temporal signal needs to drive the process if this aspect of the action is to be simulated, and the dynamical but independent nature of the production process described here allows this to take place in a realistic manner (in the sense that when we read a word we do so until the utterance terminates). This is different than other computational implementations of reading models where the phonological output (and input) is of a fixed dimensionality (e.g., the implementation in Cox et al., 2019), which assumes that the named word always encodes something about the phonological length of all words it has ever learned (e.g., in spatial models that produced a vowel-centered phonological output padded by a null representation, representing something like `r scaps('_')`, on the left and right side to accommodate the longest trained word).

As an additional point of comparison, other computational models of reading have also used different training and testing processes. For example, the CDP++ model in @Perry2010 used different procedures they termed "training" mode and "running" mode. In their case, the distinction was quite dramatic, involving rather different mechanisms for learning orthography-phonology associations and producing them. In some cases, entirely different mechanisms were introduced in order to learn common tricky orthographic forms, like those that show possess vowel-consonant-`r scaps('e')` pattern (e.g., `r scaps('bite')`). In other cases, additional "backup" strategies were developed in order to assign letters in the orthographic pattern to slots in the syllabic template operating in the system (see p. 122 of that paper for discussion of the nonword example `r scaps('fanj')`). By contrast, testing involved a separate process of aligning an orthographic input to build a hypothetical phoneme string based on the learned graphemes and their possible positions within their structured orthographic syllabic template.

By comparison, in the proposed architecture here, the differences between the training, testing and production processes aren't fundamental in terms of implementing different ideas about learning and producing output, rather they implement different (but highly related) temporal processes afforded by the architecture for the purpose of generating a sequential phonological output.

## Anatomy of the time-varying layer
#### Background of LSTM networks
Recurrent neural networks have been influential in the evolution of cognitive science in previous decades [@Elman1990]. Cognition is subject to time-sensitive processes, and thus it is important to understand how to represent time in the cognitive models we use to understand human behavior. This is related to the core assumption underlying this dissertation: cognitive models of reading should account for temporal processes relevant to reading behavior.

Recurrent neural networks received a considerable boost in their sophistication with the development of the long short term memory unit [@Hochreiter1997a]. Since its development the artificial neural network architecture has gained considerable momentum in its application in engineering of various types [@Greff2017] including related to the development of architectures that perform language-oriented tasks [@Graves2005; @Sundermeyer2015]. However, there has yet to be significant traction in cognitive science proper, despite the popularity of sequential models that predated it, like the SRN.

The LSTM architecture was developed in order to account for a computationally important problem: how can representations of experiences from the past be maintained in systems that learn over long, temporally-distributed sequences of events. This problem most clearly manifests in understanding the gradients used for weight updates in simple recurrent networks (SRNs) [@Elman1990], including those already applied to reading-related models. For example, @Sibley2008 and @Sibley2010 (both of which used versions of an SRN) had to take steps to minimize problems associated with long sequences. In SRNs, knowledge is typically updated after the system receives a input in the form of a single event (timestep) of a temporally distributed sequence of events. This might be a sequence of words, phonemes, visual events, etc. However, for events that maintain a distal temporal relationship (i.e., long-distance dependencies), the gradient is not maintained, at least directly. This is commonly described as the "vanishing gradient" problem [@Hochreiter1998], where error signals over long temporal distances (or across many layers of a network) become increasingly dissociated. This problem becomes particularly clear in the application of such architectures to language processing. For example, anaphoric reference often requires an association between a nominal referent and its distal pronoun, as in the sentence: _Sam_ looked at the monkey in the cage and as though the expectation was for thousands and thousands of years of evolution to be undone, motioned for an object sitting on the ground to be passed over to _her_. In artificial neural networks devised to capture these dependencies, maintaining the signal across the temporal distance between the two words is difficult, and increases in difficulty as that distance increases.

#### An LSTM block: states and gates
Because the model architecture used in this dissertation differs from those elsewhere in the literature on word reading, the following section is included to provide a more detailed and mechanistic account of how information is passed through an LSTM. The goal of this description is to provide both a mathematical description, but one that can be followed intuitively due to the corresponding conceptual description provided for each mathematical element of the information processing system.

LSTM networks, a single instance of such a network we will call a _block_ for convenience^[The term block isn't a technical one; it is just to help the reader understand the mechanisms being described here and isn't a conventional term. In other literatures they are often called "cells", which can be confusing and misleading when talking about cognition. For clarity on the concept, consider the following: you can imagine specifying a certain number of hidden units to an LSTM layer. By doing so, you generate that many blocks in the layer. For example, an LSTM layer with 100 hidden units would have 100 blocks. Each block will always have certain components (described throughout this section). The term "block" here just helps us think about the simplest possible LSTM layer (i.e., a layer with one hidden unit).], solve this problem by governing the extent to which information received at a previous point in time is relevant at the current moment in time (or current moment to future moment, etc.). This is achieved by maintaining memory about the input pattern across timesteps in a representational state called the _cell state_. Information about the current input (timestep) may to flow into it on an attenuated basis using _gates_, which are themselves neural networks with trainable parameters that operate within the larger neural network structure. There are several gates, and they will be described based on what they contribute to the process. In essence though, they allow the representational state to be altered such that information is either dropped, added, or passed off to future timesteps.

At any particular timestep the input vector is provided to the network. For our purposes, we can think of the example to be the printed word, `r scaps('breath')`, so the first timestep (if letters are designated as the temporal segment) would be the letter `r scaps('b')`. We can think of this same description for the phonological form of a word, where in the architecture reported here a timestep is a phoneme. Unless one is provided, there is no existing representational state for that example because this is the first timestep. In the case of our architecture, the vector representing this letter consists of as many units as there are letters in the alphabet, with the $i$th unit set to `r scaps('1')` and all others set to `r scaps('0')`, where $i$ is the sequential index of where that letter occurs in the alphabet (e.g., the letter `r scaps('b')` would have the $2$nd unit on). Upon input, the vector is combined (via concatenation) with the hidden representation of the timestep prior. In the case of the first segment (timestep) of the example (orthographic wordform), this hidden representation is a vector of units all with the value `r scaps('0')`. The length of the hidden representation is as long as there are units in the input vector (i.e., 26 for orthographic segments, one for every letter in the alphabet), which will become more clear when we understand how hidden representations for subsequent timesteps are formed. Once these two vectors are concatenated the resulting vector is passed to each of the four gates within the LSTM block.

Each gate functions similarly internally but is combined with the maintained representational state of the LSTM block in different ways to produce different memory effects based on the particular gate's intended function. The process described here happens within any single timestep before passing the state to the next timestep (timesteps iterate within a loop defined by the total number of timesteps in the example). We can think of the separate gating processes as happening in a sequential way, which can aid in understanding how they function relative to one another, though some of the computation happens in parallel which is made feasible computationally using current libraries and hardware designed for fast, efficient, parallel computation with neural networks such as LSTMs.

The concatenated input is passed to a gate whose job it is to attenuate the cell state in a way that diminishes information that isn't relevant to the ongoing memory of the system. Of course, early in the sequence, the cell state hasn't built up information about the temporal dependencies present in the sequence, so this attenuation is minimal. The gate is a neural network with a sigmoid activation function applied upon output, thereby passing values between zero and one to be combined with the cell state before the signal is moved along to the next gating routine. The output of the forget gate is combined with the cell state using multiplication. This has the effect of maintaining activation states over units of the cell state when the highest value possible is passed through the forget gate (`r scaps('1')`), and minimizing the values for the lower end of the scale of values passed through (when `r scaps('0')`). Once combined, the cell state is passed on to the next gating functions. The formula for the forget gate portion of the LSTM is shown in Equation 1.

\begin{equation}
f_t = \sigma(W_fx_t + U_fh_{t-1} + b_f)
\end{equation}

Here, $W$ and $U$ represent the learned weights of the gate $f$ at a particular timestep $t$. A given input representation at time $t$ is shown as $x_t$, and the representation passed from the previous timestep is $h_{t-1}$. The latter is referred to as a _hidden representation_ and the former as an _input representation_, though the equation makes clear the symmetry between these two vectors given that they function in very similar ways with respect to any given gate (here, the forget gate but the symmetry is clear in other gates as well). The element $b_f$ is a bias unit included in the gate. The output is passed through a sigmoid transfer function, $\sigma$, before it combines with the cell state.

The concatenated input (plus hidden) vector is also passed to a set of gates called the _input gate_ and _cell gate_, whose activity is coordinated and thus described together^[The name "input gate" is conventional and used consistently throughout the literature, however the name of this other gate, which has been termed the "cell gate" here, is not. We will call it this for the time being, however note that the name is somewhat misleading. Its relationship to the cell state of the LSTM is no more privileged than other gates within the LSTM block. The term "gate" is used here for both (in fact all) gates in order to underscore their architectural similarity, though the terminology may differ slightly from descriptions elsewhere.]. This coordination occurs with respect to their outputs and how they are combined before they are passed to the cell state; their internal functioning is independent of each other.

The gates each receive the concatenated hidden and input vectors, just like the forget gate. This longer vector is passed to a neural network with trainable parameters, and outputs a signal. The output is summed and passed through either a sigmoid transfer function in the case of the input gate or a hyperbolic tangent function ($tanh$) in the case of the cell gate, these are shown in Equations 2 and 3. For notation, we will use the symbol $\Im$ to represent the input gate and the symbol $\tilde{c}$ is used to represent the cell gate.

\begin{equation}
\Im_t = \sigma(W_{\Im}x_t + U_{\Im}h_{t-1} + b_{\Im})
\end{equation}
\begin{equation}
\tilde{c}_t = \tanh(W_{\tilde{c}}x_t + U_{\tilde{c}}h_{t-1} + b_{\tilde{c}})
\end{equation}

Their outputs are then combined via multiplication before being added to the cell state, and at this point the cell state has already been attenuated by the output of the forget gate, $f_t$. This whole process is represented in Equation 4 and defined in terms of the cell state $c$. This equation accounts for the final cell state at the end of the current timestep, and is passed on to the next timestep as its cell state to be further updated.

\begin{equation}
c_t = f_t \cdot c_{t-1} + \Im_t \cdot \tilde{c}_t
\end{equation}

The last gate involved in the function of the block is the _output gate_, referenced with the notation $o$. Its job is to determine what activation gets passed on to the next timestep in the form of a hidden representation. The function of this gate helps to make clear why this vector is termed the _hidden representation_: it is the emergent representation of the current timestep (segment) that combines information from the input, the previous hidden representation, and the cell state of the block. It essentially combines these different, related information sources (external perceptual information and internal memory representations) in the form of a single vector that represents information about that timestep for the next timestep. This vector is different than the cell state itself, which is characterized elsewhere here.

The output gate internally functions identically to both the forget gate and the input gate^[These three gates only differ internally from the fourth, the cell gate, because of the transfer function used (i.e., $\sigma$ instead of $\tanh$).], and its formula is shown in Equation 5.

\begin{equation}
o_t = \sigma(W_ox_t + U_oh_{t-1} + b_o)
\end{equation}

This gate receives a concatenated input ($h_{t-1}$ with $x_t$), which is passed through trained weights. The output is then summed and a sigmoid transfer function is applied. Importantly, the output signal is used differently than other gates. The vector is multiplied against a copy of the cell state (a copy which has subsequently been transformed using a $\tanh$ function). This is shown in Equation 6, where $h_t$ in the previous description should be thought of as the hidden state that the current timestep produces for the next timestep.

\begin{equation}
h_t = o_t \cdot \sigma_h(c_t)
\end{equation}

This operation allows for the newly created hidden representation to be of the appropriate length, given that the output of the operation is determined by the dimensions of the cell state (i.e., the hidden state dimensions are the same as the cell state dimensions). The output of this set of operations is then passed on as the hidden representation used in the next timestep.

This process is repeated across as many timesteps as occur in the input. As a result, when an example (e.g., an orthographic input) terminates, what's left is the cell state and a hidden state. The cell state is in its final state for a given timestep once it has been combined via addition with the (combined) output of the input and cell gates. The hidden state is ready for use on the subsequent timestep once it has been combined via multiplication with the $\tanh$ transformed cell state as described in the above section on the output gate and shown in Equation 5. As a result, any LSTM layer has the capacity to pass the cell state as well as all hidden states from each timestep of the example if necessary or useful (not just the hidden state of the final timestep). This distinction is important in terms of the representational capacity of the layer. The cell state should be seen as a memory representation of the example as a whole, whereas the hidden state of the last timestep is more narrow in its representational capacity. The information contained in this vector is more relevant to the final timestep itself, though has been affected directly (via a $\tanh$ scaling operation) by the cell state, and so contains a trace of information relevant to the entire input sequence.

#### Cognitive aspects of the LSTM
At this point a few important aspects of the cognitive nature of such a system can be foregrounded in computational terms. Statistical learning systems, like the artificial neural network described here, should account for the statistical dependencies of the percepts they experience. Feedforward networks accomplish this by taking in information on an input layer (like orthography) and updating a set of trained weights with respect to error calculated on the output layer. Dependencies are encoded in the weights given the degree of covariation between the input pattern and its corresponding output pattern, taking into account this covariation across all learning trials. This is a _spatial_ mapping, and captures the dependencies present within a given learning trial in a spatially-fixed format. That is, the mapping in a feedforward network happens without temporal distribution of elements on the input and output^[Except of course in recurrent architectures that use continuous recurrent backpropagation, like the attractor networks simulated in @Plaut1996 and elsewhere in the connectionist literature. As discussed earlier (in Chapter 1) those networks have a temporal element that works in a different way across segments as the LSTM being described here.]. The LSTM performs a spatial mapping for a single timestep, but also captures dependencies across timesteps, capturing the dynamics of segments across the training example.

The dynamics specifically play out with respect to the two outputs served up by the block: the cell state and the hidden state. While these two vectors will always bear a relation to one another, most specifically in terms of the combination that occurs on the output gate (shown in Equation 5), they are computationally and functionally distinct. The cell state serves as the memory state that captures statistical dependencies across all timesteps of a learning trial (letters or phonemes across words) -- a distal relationship -- and the hidden state captures the proximal analogue: the emergent representation of the current segment with respect to the cell state.

Without gating mechanisms that allow information to flow into the long term memory of the example, these long term dependencies can't be captured. Likewise, without the ability to combine information about the long term dependencies with information being processed at a given timestep (i.e., the combining function of the output gate and the cell state that creates the hidden state $h_t$), proximal information about the segment can't be transferred from one segment to the next. So, the two internal state elements of an LSTM have the capacity to capture something long-term (the cell state) and short term (the hidden state) across a training trial. This dynamic is what forms the characterization of the LSTM architecture as "long short-term".

#### Other training details
The remaining details surrounding the training process follow from other common connectionist models of reading aloud. During training, backpropagation through time is used in order to update weights [@Rumelhart1986], as with other connectionist simulations [@Chang2019; @Cox2019; @Plaut1996; @Seidenberg1989]. In addition to the LSTM layers used for orthography and phonology, a simpler feedforward layer was used on the phonological output layer. This was done to provide representational capacity unique to the phonemic segments at the output layer that differed to the capacity of the LSTM layer dedicated to phonology. It resembles similar sequence-to-sequence systems developed for other language tasks [@Sutskever2014], however subsequent experimentation with augmented architectures will be worthwhile despite the general strong performance of the models reported here. Hidden layers of 400 units were used for all layers of the network. This value was chosen because it performed adequately relative to other possibilities, but other specifications are expected to perform at similar levels to the models here and results do not hinge on this or other values for initial hyperparameters used. Loss is calculated as binary crossentropy (similar to connectionist applications elsewhere), and root mean square propagation (RMSProp) for momentum is used as an optimizer [@chollet2015keras]. Performance accuracy is reported as mean square error, which is standard based on other similar models. The network was implemented in keras [@chollet2015keras] using Python 3.9.

#### Representational virtues
Readers can process, correctly or not, letter strings of any length. Any model architecture of producing phonology from orthography should be able to account for this fact. Also, this should include the ability to attempt to read any word, regardless of the length of its spoken form. There should be a direct relationship between the length of a word's printed or spoken form and its representation used by any computational architecture that consumes it. This relationship is maintained in the current architecture in a very direct way. The representation of the printed form of a word is generated letter-for-letter, where the orthographic representation is as long as there are letters in the printed form^[Though alternative implementations are considered in Chapter 4.]. The phonological representation is formed in a similarly straightforward and veridical way. A spoken form is defined as a sequence of phonemes - a well-established level of linguistic representation for the spoken form of words. The output representation for a word is build phoneme-by-phoneme, where each phoneme is defined as a distributed pattern over articulatory and phonological features, as described elsewhere here. Therefore, the length of the spoken form of the word is defined by the number of phonemes in the word, and the complexity of the representation can be estimated by the number of features on over an individual or string of phonemes (here 34 phonological and related features). This representational scheme is intended to represent the structure of the language in a straightforward way that differentiates it from other models of these cognitive and perceptual processes.

#### The justification for no justification
A number of alternative architectural and representational schemes have been entertained in other models of word naming. The most common approach involves specifying the orthographic and phonological representations using vowel centered representations [@Chang2019; @Cox2019]. For models of reading that deal in monosyllabic words this approach captures something appropriate about vision, given that an entire printed word can be taken in on a single fixation if the word is short. Granting this assumption, learning that the centered pattern `r scaps('\\_\\_breath\\_\\_')` maps onto `r scaps('\\_\\_b-r-eh-th\\_\\_')` is a defensible simplifying assumption.

However, procedures like that require justification of some kind, typically in the center with both left and right pads, introduce a computational problem: perceptual segments come to occupy slots in their representational domains. For example, the letter `r scaps('b')` from `r scaps('breath')` comes to be represented in a different way than that same letter in other positions (e.g., `r scaps('cob')`, or `r scaps('curb')`). Some computational architectures have tried hard to work around this issue one way or another, usually by abstracting away from positions to higher order linguistic units. For example, PMSP96 represented input and output patterns based on the syllabic structure which a given segment is commonly associated. Their method was developed in the wake of other procedures designed to encode just enough of the relative structure of segments in words to be suitable for modeling monosyllabic words. For example, see the discussion in SM89 and their use of orthographic and phonological "triples" [@Wickelgren1969] to work around this problem. These are accepted as a plausible alternative given that such representations build in contextual information that helps the model abstract away from position-specific representations in the input and output layers.

The architecture reported here avoids these issues due to the sequential processing afforded by the recurrent LSTM layers used in orthography and phonology. Recall that timesteps are only processed if they are present as perceptual segments. That is, the orthographic input layer operates over as many letters as there are in the word, and the phonological one operates over as many phonemes as there are in the word (plus the "start of word" symbol).

#### Assembling batches for learning
One important detail for avoiding padded inputs (and justification) concerns the assembly of batches for training. Batches consisted of words of the same phonological length, where the orthographic length of words within a batch could vary in terms of their orthographic length. This was done to avoid batches of a single word, which slows down training dramatically. The variable length orthographic inputs were accommodated with masked timesteps on input (i.e., timesteps that are skipped when empty). To make this process more simple, during training batches were selected from all words of the same phonological length until all the words of that length were trained, then a new length was selected for training. This was repeated until all words were trained during that epoch. Within each epoch, a set of shorter words (e.g., words with a phonological length of five phonemes) were more likely to come earlier in the epoch than words of a longer length (e.g., those with 8 phonemes). This sampling process doesn't have significant effects on learning because ultimately within each epoch all words are presented at some point. Rather, the sampling procedure only dictates the set of words (based on phonological length) that is selected and trained before moving onto words of the next phonological length.

An alternative training batch assembly process would be the selection of one training example at a time (i.e., stochastic gradient descent). Investigating the role of batch size on learning, and the behavioral correlates in humans would no doubt be a worthwhile avenue of future research but is outside the scope of the work being done here. These steps were taken to identify a training process that was both realistic and fast. The assembly of batches in this way isn't intended to convey a specific hypothesis about how batches correspond to learning in humans. Instead it is intended to help navigate the tradeoff between computational efficiency and specifying a realistic learning environment for the model.

#### Word frequency
The role of experience in the development of reading skill is an essential one to the reading process [@Seidenberg2018]. More experience with a word facilitates knowledge about that word and the ability to use the word in production and naming. The simplest and most common way that experience effects in word reading have been investigated has been in terms of word frequency as estimated from corpora of text or speech. Word frequency can be implemented in computational models of reading in different ways. Implementations have included the selection of words for training in a probabilistic fashion with the probability of selection determined by the word's frequency of occurrence [e.g., @Seidenberg1989] and the scaling of the learning signal during training proportional to the frequency of that word. The latter was used here for computational efficiency given the type of batch training that was used, where the gradients calculated for a learning trial were scaled proportional to the frequency of that word in a large corpus of text [see @Sibley2010 for a similar implementation]. Frequencies from the Hyperspace Analogue to Language (HAL) were used [@Lund1996], accessed through the data provided by the English Lexicon Project [@Balota2007]. These frequency calculations were utilized to facilitate a comparison to the behavioral data also provided in the ELP. In order to avoid frequencies of zero, two was added to every frequency value in the set.

Because word frequencies exhibit a dramatic range, a scaling operation was used to both capture the distribution and translate the value into a proportion (where a value of one would cause the full gradient for that word to participate in the weight update, and a value of zero would mean throwing that gradient away). To accomplish this, the formula from SM89 (p. 530) was used to calculate $p$, the proportion applied to the gradient update for the word being learned.

$$
p = K \cdot log(frequency + 2)
$$
This results in a distribution of proportions for each word that is monotonically related to the raw frequencies of those words. The value of the constant $K$ was selected such that the highest value for $p$ was .93 for the most frequent word (just as in SM89) - the word `r scaps('and')` (raw frequency of `r prettyNum(freq_for_word(mono, 'and'), big.mark=',')`). For the monosyllabic corpus this resulted in $K$ being equal to `r round(monosyllabic_k, 3)`. A plot of the distribution of values for $p$ against raw values for frequency are shown in Figure 3.

```{r monopbyraw, echo=FALSE, message=FALSE, warning=FALSE, fig.height=3, fig.width=3, fig.cap='Scaled word frequency by raw frequency for words included for training in the time-varying monosyllabic model implemented. For reference, the most frequent word, "and", can be seen in the upper right hand corner.', fig.align='center'}

mono %>% 
  ggplot(aes(freq, freq_scaled)) +
  geom_point(color = 'gray30', size = .05) +
  theme_apa() +
  labs(x = 'Raw frequency', y = 'Scaled frequency') +
  annotate("segment", x = 8.2e+06, xend = 10.5e+06, y = .75, yend = .91,
         colour = "black", size = .15, arrow = arrow()) +
  annotate(geom="text", x=7.8e+06, y=.71, label='"AND"',
         color="black")
```


## Depiction of full architecture
The full architecture is depicted in Figure 3. This schematic shows at a higher level of detail the network depiction shown earlier, where the junction of the two networks takes place in the phonological LSTM by virtue of the state passing process from the orthographic LSTM, which constitutes the orthographic context for temporal phonological processing to take place. The network shown represents that network used in all of the time-varying simulations reported in Chapters 3 and 4.

```{r architectureFigure, echo=FALSE, out.width='300px', fig.align='center', fig.cap='A schematic depiction of the model architecture is shown. Portions labeled as "time-distributed" correspond to LSTM layers. Boxes that are empty correspond to hidden units employed for that layer.'}
include_graphics('img/architecture.png')
```


\newpage

