---
title: "3. Architecture"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Words used for training and testing
The perceptual span within which letters can be clearly identified on a single fixation is approximately eight letters (ref.). For this reason, words of eight letters or less were compiled from the previously discussed sources for the purposes of training and testing the models developed in this work. The timecourse of phonological activation of orthographic input is a debated topic, but little is known about how such processes operate for long words. It is possible if not likely that a dynamic process where orthographic inputs propagate information to activate phonology, initiating a response, which is modulated by subsequent visual input. Consider, for example, reading the word _architecture_, which is longer than the eight-letter span considered for experimentation here. Phonological activation could well be taking place over the initial visual portion of the word, such that the _...ture_ portion isn't processed until after the phonology of the root (`AA - R - K - AH - T - EH - K - T`) is initiated. While the dynamics of such processes are interesting and important for developing a model of word reading that accounts for how long words are read, such cases are outside of what is dealt with here. A simpler corpus has been developed for the purposes of this work, built on with assumptions based on perceptual span (some results are provided that demonstrate the ability of the model to produce output for words longer than this span, demonstrating the broader capacity of the model and architecture). A virtue of the architecture developed in this dissertation is that such dynamics are possible given the nature of the computational learner developed, though these dynamics are not accounted for in service of accounting for simpler and more fundamental processes in word reading.

Words were aggregated from a corpus of 250 commonly read children's books for children five years-old and younger in the United States [@lewis2021]. This corpus was used given that a word's presence in such a database indicates that a child is likely to be exposed to it early in literacy develpment, at least in the US. A word was included for training if it was XX letters or less, XX phonemes or less, and XX syllables or less. These constraints were included for the training set in order to avoid length effects that were outside the focus of the analyses presented here. However, the capacity of the model to accomodate longer words than those included for training is demonstrated. Also, training can happen over words (or orthographic/ phonological sequences) of unlimited length, this length was just limited in order to focus the learning process on reasonably lengthed words from this realistic database of child-oriented language.

Practically speaking, this has the effect of including XX% of the total words from the database, or XX of XX total words from the children's corpus from @lewis2021. Figure XX depicts this inclusion criteria visually.

A number of outlier words were removed from the set due to their idiosyncratic characteristics. Words removed if they were abbreviations ("Nov") or initialisms ("bbq"), which constituted XX% (XX total words) from the child language source data. Additionally, children's books contain language that is quite different from words commonly read in other genres. These words include printed expressions of motherese ("aaaaa"), sing-song ("yipiyuk"), or forms of onomatopoeia ("zoooooom", "wooshee") that aren't well-suited for modeling the type of word reading attempted here. Single-letter words were removed as well, given that there are relatively few of these words (even though the architecture is equipped to handle such words).

Once collected, all words were transcribed using the CMU pronunciation dictionary implemented in Python with the Natural Language Toolkit (nltk) library (ref.). In order to increase the training set size, an in an effort to do so in a realistic manner, morphological variants were added to the corpus for words that possessed common variants. Sometimes, this applied to words that only appeared in their morphologically complex form in the children's book corpus, like "adults" (where the bare form "adult" was added), on other times this involved adding a morphologically complex form to the set where the simplex form was present in the children's book corpus (e.g., "buyers" for the word "buyer").



# Representations


# Architecture
In this section, an initial high-level description of the computational learner is provided, and is followed by a more technical description of how the architecture operates functionally and mathematically.

The architecture takes in a time distributed sequence of letters (one timestep per letter) and passes the representation of that sequence to the phonological portion of the network. The representation (termed the _cell state_, described in greater detail later) serves as the initial memory state for learning about the time-distributed phonological sequence corresponding to the visual pattern learned. Importantly, while the nature of the temporal dynamics around how this representation is passed has the potential to be manipulated, in the experiments reported here the representation is built up over an entire orthographic input pattern before being passed to phonology. While the timecourse of how visual information activates phonological information is unclear based on behavioral research, the information flow in this architecture approximates what we believe to be taking place at least in short words, but is used during all learning in the reported experiments.

Once the phonological network receives its initial state from orthography, if begins processing the corresponding phonological pattern to the orthographic input it has just taken in. The time-distributed way in which phonology is processed is very similar to that of orthography. In training trials that take in a phonological input along with the orthographic input (a teaching signal where the visual and auditory inputs are provided on the same learning trial), the phonological input is represented with one phoneme per timestep. Phonemes are used for convenience, and future extensions of the architecture proposed here should experiment with more sophisticated representations of spoken language. The sequence of phonemes builds up a representation and is mapped to the same phonological sequence at the output, having also integrated the orthographic representation passed as the initial cell state of the phonological LSTM layer. 

However, a word's the phonological input and output patterns differ in an important way. The phonological input pattern contains a special representation at the beginning of the word (i.e., as the first timestep) and the corresponding output pattern contains an end of word representation as the final timestep (but not the beginning-of-word representation). For example, the word _breath_, would correspond to the sequence of phonemes `B - R - EH - TH` (see Appendix XX for the CMU dictionary representations used). The input pattern for this spoken word would be `# - B - R - EH - TH`, with an output pattern of `B - R - EH - TH - %`. This allows the phonological part of the network to be trained with knowledge of when a word should be initiated during the production of the spoken wordform corresponding the orthographic input provided, and when the production of the phonological form should be terminated (i.e., the production should stop). It also causes the time-varying phonological input pattern and and its corresponding output pattern to be offset by one segment.

The process described above corresponds to a teaching signal where the learner is both hearing and and seeing the visual word while recieving feedback about the word that should be produced given the input. Learning takes place in the orthographic and phonological portions of the network (see later full description of the trainable parameters of the network), with only a phonological output driving learning. An architecture of this kind can be easily extended to include both a phonological and orthographic output layer despite the more simple version of the architecture adopted for this work.

Not surprisingly, a learner trained on simultaneously provided orthographic and phonological input with an error signal injected into such layers from a phonological output layer does not become independent in its ability to produce phonology from orthography alone - it over-relies on the phonological input given its identity with the word's phonological output. Training can proceed in other ways in order to avoid this overreliance on phonological input signal. Given the dual nature of the input system in the architecture, learning can take place given by providing inputs on either input layer, or both. Because the orthographic part of the network is connected to phonology through a memory sharing operation that takes place through a passing of cell state vectors (rather than directly through, say, an input layer where inputs are shared directly), training that involves mapping an orthographic input to a phonological output _without_ a phonological input, requires an alternative specification of the inputs on phonology in order to initiate production. During such learning, the phonological input layer receives an empty phonological input, but with the "initiate" segment provided as the first timestep of the sequence (for notation purposes, the symbol "#" is used for this segment).

The timespan of the input sequence when training in this way is determined by setting some set number of empty timesteps as the null input. Because of the nature of the LSTM architecture, we use a null input sequence defined by the number of phonemes of the word plus one (i.e., plus the "initiate production"/ "#" segment) in order to give the model a chance at producing the right number of phonemes on the output layer (because the input and output dimensions on the phonological portion of the network are identical). For example, in such a training trial using the word _breath_, the phonological input pattern would be `# - _ - _ - _ - _`, with the same output pattern as before, `B - R - EH - TH - %`.  This specification relates to the fact that the network uses no justification (i.e., "padding"), and that a temporal signal is required to drive the dynamics of the learning over parameters of the LSTM layer. An alternative method could be used that provided more timesteps than those provided strictly by the length of the phonological form itself (e.g., defined by the longest phonological form in the entire training corpus), but such a process requires padding on the output layer given that the time distribution of the input would then be greater than the number of phonemes in the word (plus the word-edge representation). In fact, a training process like this is adopted in most other machine translation endeavors taken up in the engineering and NLP literatures (see ref. and ref. for examples), but is avoided here due to the fact that "justification" of inputs and outputs deviates from the sort of veridical representations this work takes as fundamental to developing a more psychologically plausible architecture of time-distributed word recognition processes.

The phonological portion of the network, while being influenced by orthography, operates as a subnetwork that functions in a similar way to an autoencoder, mapping phonological inputs to phonological outputs in the process of developing deep knowledge about spoken language.

A different method of production is used for the purposes of testing the network's ability to "name" when provided an orthographic input pattern. On production (or "testing") trials, an orthographic input pattern is provided, and its activation state builds up over the entire time-varying pattern (for our purposes here this is an entire word). 


This production method is similar to those used for the purposes of machine translation, where online training and offline performance of such systems are dissociated for engineering reasons (ref.). Here, the different training and testing processes are used for two reasons

thus producing a phonological wordform from its corresponding printed version. The printed sequence is a (computer readable) series of letters and the spoken version is a (computer readable) series of speech sounds; that is, the architecture uses veridical representations of print and speech as the learning environment.








## Phonology
The primacy of phonology and phonological learning in reading is not controversial. There are debates about the processes at play in how spoken language units are assembled during reading as a function of visual processes (rastle2010)

## Latency
Given that computational processing times on a GPU haven't been validated as an analagous method for naming latency in humans, similar to Sibley et al. (2010) a measure of model confidence for processing an orthographic word was designed to measure the latency for reading that word. 

## Learning versus reading
Similar to the CPD++ model in Perry et al. (2010) the network reported on here uses different processes for learning versus reading ("training" and "running" in their language). However, the difference between these routines in the case of our learner is subtle and doesn't represent a substantially different model of cognition across the two activities. 

## Representational virtues
Readers can process, correctly or not, letter strings of any length. Any model architecture of producing phonology from orthography should, in principle, be able to account for this fact. Past computational architectures have always had to work around this issue one way or another.

Positional information in other proposed models deviates from this assumption.

## No justification for justification
All previous connectionist models of word recognition have involved spatial (and in some cases over temporally varying segments) justification of one kind or another, usually implemented as empty slots to the right of an orthographic or phonological pattern (e.g., "breath____"). When present, this feature of an architecture (and the patterns that are used to train it) represents a deep limitation in its psychological plausibility. While including justification in some form is understandable as an aspect of the engineering problem that computational modelers face in modeling visual (or other) word recognition processes, they assume that the patterns that are used in orienting words, left or right, on their input and/ or output layers (however the justification might be implemented) are an aspect of the learning process, though


