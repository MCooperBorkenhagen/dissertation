# Chapter 3: Experimentation
The purpose of the model developed here is to produce a phonological code from a word's corresponding printed code. A number of results are presented based on comparing the model's behavior to experimentation from behavioral research with humans. Additionally, this chapter contains comparisons to similarly oriented models of word reading from elsewhere in the cognitive science literature. The purpose of this experimentation is twofold: to present results that convey important features of the architecture as it relates to cognition and development and to make comparisons to experimentation elsewhere of similar phenomena, both from work in computational models of reading and human behavior.

The organization of this chapter is as follows. A few additional specifications about testing and production trials are provided first, then results for several different experiments are reported. The first half of the results concern the model learning words with one syllable. The second half concerns words that are multisyllabic. For each section there is an account of the basic performance of the model, comparisons to human data using a megastudy of word naming [@Balota2007], and comparisons to experiments that manipulate the properties of words and examine the associated naming behavior of the model (and correspondingly, humans in the behavioral data). First, some additional implementation details of the simulations are discussed in order to properly frame the results and corresponding discussion of the model behavior in this chapter.

## Additional training and testing specifications
#### Testing model accuracy: mean squared error
Results of the model's ability to correctly produce a spoken form of a word can be based on a number of different measures. The most common measure of the model's ability is an accuracy score. Accuracy can be structured in a few different ways - similar to that of a human reader. Given that the goal of the model is to take a sequence of letters and recode them in the form of a sequence of speech sounds, individual sounds can be inspected for their accuracy along with the entire sequence of sounds. This includes the vector output produced, and the string analogue: the symbolic phonemes that the vectors represent. Production mode is used for this purpose (which is described further below). Alternatively, testing mode is used to report accuracy as mean squared error (MSE). Testing mode (which was described in Chapter 2) is used when reporting accuracy at different points throughout training (as opposed to testing the network's ability to produce a phoneme sequence at the end of training). This measure of accuracy is convenient because it expresses accuracy as a single value for any given example (word), and is also a frequently used metric in the literature on computational models of word recognition and naming. Alternative metrics of model performance are provided occasionally and described along with results reported. The formula for mean squared error is shown in 7.

\begin{equation}
MSE = \frac{1}{2}\sum_{i=1}^{n}(Y_i-\widehat{Y}_i)^2
\end{equation}

#### Production trials
In order to inspect the sequence of phonemes produced at the end of training, _production mode_ is used. This process was described earlier in a conceptual way, and is different than the procedure for producing output that is used in training and testing. It is reviewed here and described in a bit more detail for clarity. When a word is submitted for testing an orthographic pattern is introduced to the the orthographic input layer. This pattern is structured just like that of the orthographic input during training: it is a two dimensional array with as many letter representations along the first axis as there are letters in the word. This time-varying input pattern is input to the orthographic input layer, and its representation decomposed into its two critical states for use in phonology: the cell state and the final hidden state. Once these vectors are achieved for the orthographic form of the word, that portion of the model rests and is unchanged for the remainder of the trial.

The states of the orthographic layer are passed to phonology to generate the phonological code output. This process is initiated by the input of the start of word segment (which we can think of as the symbol `#`), the representation of which was described earlier. The fact that the model has been trained with an explicit start of word marker enables the phonological layer to produce a next phoneme given the current state of the layer and its input pattern. At the start of production this context is provided completely from the orthographic layer. That is, the role of the phonological portion of the model is to produce a phonological code given the orthographic context.

Once the first phonemic pattern is produced it is saved for subsequent phonemes to be appended to it in order to form a full sequence of phonemes that constitutes a phonological output. While the weights in the network are frozen at this stage prior to testing, the states of the phonological LSTM is updated at each timestep following the same processing flow as described in the architecture section. Once a phoneme is output at each timestep, it is fed back to the input of the phonological layer to support the prediction of the subsequent phoneme. This differs in process from training trials but is the same conceptually given that in training, the entire phonological sequence is processed one segment at a time without the feedback process from the output layer back to the input. Because the output for a given phoneme is very unlikely to be specified as a perfect binary pattern (because predictions almost always have until that approach zero or one, but aren't actually those values), any phoneme output is compared to all possible phoneme outputs using an L2 norm and the nearest phoneme is selected and recorded as the produced phoneme (both the symbol representing the phoneme and the activity pattern are saved). The production trial progresses until the phonological output layer produces the stop producing segment (`%`) or exhausts the total possible number of segments given the longest words included in training. At this point the word produced is recorded for analysis.

The produced pattern (in array form) is a time-distributed series of vectors representing patterns of activation over phonological features. These patterns can be thought of as a pattern that might pre-specify a motor command to be passed to an articulatory mechanism during production (see SM89 for more of a discussion on the process). Measures of phoneme accuracy are reported either using Euclidean distance or using the symbolic, string representation of the phoneme produced (using the Euclidean measure).

#### Comparisons to naming latency
Most often, accuracy measures (whether from production or testing mode) are compared to naming latency taking from behavioral data, as in other similar reporting on computational models [@Sibley2010]. This is a proxy measurement, rather than one that conveys true temporal dynamics of the computational model. The concept of time in terms of computational latency in this architecture not clear, especially given the novelty of application of LSTM architectures to understanding human cognitive processes related to reading and speech. This is a topic for further research, and is likely to be fruitful given the temporal nature of such a computational system. Nonetheless, the graded measures of accuracy described above are used here as an approximate analogue to naming latency in comparisons to behavioral data, with further technical discussion relative to a given result were applicable.


## Results for monosyllabic words
#### Monosyllabic word corpus
Given that a number of behavioral effects for reading monosyllabic words are well established, along with investigation of those effects using computational models of such processes, a set of monosyllabic words was subset from the full corpus of words and used for simulations reported here. A list used in other connectionist modeling was used for this purpose [@Cox2019]. Words from a number of experiments were included so that model behavior could be compared directly to that of human readers. These sets are described along with specific results reported throughout this section. The resulting training set totaled `r length(unique(mono$word))` words. Words were included using the same criteria reported in Chapter 2.

In order to link model behavior to that of humans, a standard approach was taken. Effects of word-level characteristics on model accuracy are provided first. Then more granular effects related to experimental manipulations of those word characteristics are reported. Many effects that are relevant to monosyllabic words are subsequently reproduced in the model that also learns multisyllabic words. The results for the monosyllabic words start with word frequency, move on to print-speech consistency, and then examine their interaction before reproducing effects found in experimental manipulations of these factors taken from prior studies. In examining initial results of these word characteristics, results from a standard feedforward network that maps orthography to phonology are also reported in order to establish a connection between the time-varying model reported here and another standard model architecture from the learning literature.

The time-varying model was trained to 72 epochs, and achieved asymptotic performance at that point. Accuracy data were generated using testing mode halfway through training (at epoch 36) in order to examine the effect of the frequency manipulation on learning. A comparison feedforward model was trained over the same set of training items, also to asymptote (300 epochs) using all the same specifications as the model reported in @Cox2019. The same gradient-based frequency manipulation was used in the feedforward model as with the time-varying model.

### Frequency and consistency effects
#### Frequency
More experience with a word leads to more robust representation of the word, which in turn enhances the ability to act on it in some form. The most common proxy for experience in the reading literature is the frequency of a word's occurrence in a set of texts. Words that are more frequently encountered tend to be easier to read, resulting in general in faster reading times and fewer errors [@Balota2004; @Forster1973; @Stanovich1978]. Models of reading aloud have simulated the effect of experience in a number of studies, where experience is sometimes implemented by proxy as a frequency-influenced procedure during learning [@Sibley2010; @Plaut1996]. Similar to other models word frequencies were used here in order to scale gradients during training for the time-varying and feedforward models. One issue is whether the time-varying architecture is sensitive to experience implemented in this way, and concerns whether or not this sensitivity resembles that of other computational architectures and human behavioral data.

In order to examine the effect of frequency on accuracy, mean squared error for the time-varying and feedforward models were generated halfway through training. These are reported alongside both latency and accuracy values taken from @Balota2007. All values were standardized in order to share a y-axis. This involved arranging the accuracy scores such that increases in the values represented increases in processing difficulty (i.e., mirroring the distribution of response times). Figure 4 shows the relationship for the two computational models alongside behavioral data from the ELP. Frequencies have been log transformed. Increases in word frequency are associated with a reduction in processing difficulty. This trend is reduced in both computational models relative to the data from @Balota2007, but in all four we see a common pattern.

```{r, freqeffectHumanComp, echo=FALSE, warning=FALSE, fig.height=3, fig.cap="The relationship between word frequency (scaled) and word-level processing difficulty is shown for human behavioral data (panel A shows naming accuracy and panel B shows naming latency), a feedforward network architecture (C), and the time-varying/ LSTM architecture (D). Points are individual words that participated in training. In the case of human accuracy (A) errors are computed as accuracy of naming the word in a megastudy of word reading [@Balota2007] where the accuracy value of a word has the sign reversed. Human RTs are standardized milliseconds. Model errors (C-D) are mean squared error, calculated unitwise on the phonological output layer of the network (over differences between the produced output for a unit and the target output). All metrics are standardized so that points (words) could share a common y-axis."}

STAGE = 'Middle'

elp_words = mono %>% 
  select(word, elp_acc) %>% 
  filter(!is.na(elp_acc)) %>% 
  pull(word) %>% 
  unique()


 
descriptives = mono %>%
  filter(train_test == 'train') %>% 
  filter(stage == STAGE) %>% 
  group_by(model, stage) %>% 
  summarise(MEAN = mean(accuracy),
            SD = sd(accuracy))


COLORS = c('LSTM' = 'firebrick', 'Feedforward' = 'goldenrod', '(D) Time-varying' = 'firebrick', 'Time-varying' = 'firebrick',  '(C) Feedforward' = 'goldenrod', 'Human - Accuracy' = 'Black', 'Human - RT' = 'Grey20', '(A) Human - Naming Accuracy' = 'Black', '(B) Human - Naming RT' = 'Grey20')

descriptives_elp_acc = mono %>% 
  filter(word %in% elp_words) %>% 
  filter(stage == STAGE & model == 'LSTM') %>% #arbitrary because values repeat
  filter(train_test == 'train') %>% 
  summarise(accMEAN = mean(elp_acc),
            accSD = sd(elp_acc),
            rtMEAN = mean(elp_rt),
            rtSD = sd(elp_rt)) %>% 
  mutate(model = '(A) Human - Naming Accuracy')

d_elp_acc = mono %>% 
  filter(word %in% elp_words) %>% 
  filter(stage == STAGE & model == 'LSTM') %>% #arbitrary because values repeat
  filter(train_test == 'train') %>% 
  mutate(model = '(A) Human - Naming Accuracy') %>% 
  left_join(descriptives_elp_acc) %>% 
  mutate(acc = -(elp_acc-accMEAN)/accSD) %>% 
  select(word, model, acc, freq, consistency)


descriptives_elp_rt = mono %>% 
  filter(word %in% elp_words) %>% 
  filter(stage == STAGE & model == 'LSTM') %>% #arbitrary because values repeat
  filter(train_test == 'train') %>% 
  summarise(accMEAN = mean(elp_acc),
            accSD = sd(elp_acc),
            rtMEAN = mean(elp_rt),
            rtSD = sd(elp_rt)) %>% 
  mutate(model = '(B) Human - Naming RT')

d_elp_rt = mono %>% 
  filter(word %in% elp_words) %>% 
  filter(stage == STAGE & model == 'LSTM') %>% #arbitrary because values repeat
  filter(train_test == 'train') %>% 
  mutate(model = '(B) Human - Naming RT') %>% 
  left_join(descriptives_elp_rt) %>% 
  mutate(acc = (elp_rt-rtMEAN)/rtSD) %>% 
  select(word, model, acc, freq, consistency)



mono %>% 
  filter(stage == STAGE & train_test == 'train') %>% 
  left_join(descriptives, by = c('model')) %>% 
  mutate(acc = (accuracy-MEAN)/SD) %>% 
  filter(word %in% elp_words) %>% 
  select(word, model, acc, freq) %>% 
  rbind(select(d_elp_acc, -consistency)) %>%
  rbind(select(d_elp_rt, -consistency)) %>% 
  mutate(model = case_when(model == 'LSTM' ~ '(D) Time-varying',
                           model == 'Feedforward' ~ '(C) Feedforward',
                           TRUE ~ model)) %>% 
  ggplot(aes(log(freq), acc, color = model)) +
  geom_point(size = .2) +
  geom_smooth(method = 'lm', color = 'grey32', span = 1/3, fill = 'grey48') +
  scale_color_manual(values = COLORS) +
  facet_grid(~model) +
  labs(x = 'Frequency (log)', y = 'Processing difficulty') +
  theme_apa() +
  theme(legend.position = 'none', strip.text = element_text(size = 5.25)) +
  ylim(c(-1, 5))
```

\newpage
Furthermore, the effect of frequency fades as learning progresses; by the end of training this effect is attenuated substantially such that accuracy is consistently high among most words. With enough experience, processing difficulty is minimized even for the most infrequent words. In table 5 we see this in the cumulative error scores at two points in training the time-varying network: once early on (at 36 epochs) and again late in training (at 72 epochs). Errors become low even for the least frequent words.

```{r, freqeffectEarlyLate, echo=FALSE, fig.height=3.5, fig.cap= 'The effect of word frequency early and late in training on the time-varying network differs such that with ample experience errors on infrequent words are minimized and the overall effect of frequency on error is reduced. Raw errors are plotted for words (points) using mean squared error.' }

mono %>% 
  filter(stage == c('Middle', 'Late')) %>% 
  filter(model == 'LSTM') %>% 
  filter(train_test == 'train') %>% 
  mutate(trainstage = case_when(stage == 'Middle' ~ 'Early in training',
                                stage == 'Late' ~ 'Late in training')) %>% 
  ggplot(aes(log(freq), accuracy)) +
  geom_point(size = .5, color = 'firebrick') +
  geom_smooth(method = 'lm', color = 'grey32', fill = 'grey48') +
  scale_color_manual(values = COLORS) +
  facet_grid(~trainstage) +
  labs(x = 'Frequency (log)', y = 'Mean Squared Error') +
  theme_apa() +
  theme(legend.position = 'none')
```


#### Consistency
Another important aspect of learning to read 
concerns how structural complexity affects reading performance. Readers are sensitive to the degree of coherent covariation among the mappings between printed and spoken words; the less covariation that exists between letters and sounds, the slower and more error prone performance is. Debates about how readers deal with this complexity and the role of experience in development and performance have been important for understanding the cognitive system that underlies reading development and performance, with different theoretical frameworks approaching the computations (and associated measurement) in different ways [see @Seidenberg2022 for a thorough review].

Parallel distributed processing theories of reading and lexical processing [@Plaut1996; @Seidenberg1989], have established that complexity of this kind leads to more burdensome processing (e.g., longer naming latencies, higher error) which can be overcome with sufficient experience. Connectionist models encode information about the mappings between perceptual modalities in neuron-like processing units, which come to encode the relationships between printed and spoken words with increasing robustness. This robustness is affected by the amount of experience the reader (or computational model) has in learning the words during training and the reliability with which the mappings across modalities occur across those words.

The complexity of the mappings between print and speech is defined in terms of _consistency_: the extent to which the pronunciation of a word is similar other similarly spelled words^[While different theoretical approaches define this structural properties in different ways, it is outside the scope of the current work to adjudicate between different approaches. This issue is discussed thoroughly elsewhere [@Seidenberg2022] and would be worthwhile for consideration in future modeling exercises using the architecture reported here.]. Defining consistency in a measurable way is important for understanding the behavior of the network and corresponding reading behavior in humans. The approach taken here is similar to @Plaut1996, where the consistency of a word is defined by the orthographic body with respect to the phonological rime (for monosyllabic words). Here _body_ refers to the portion of the word consisting of the first orthographic vowel and the orthographic structure that follows it (e.g., the `r scaps('umbling')` in `r scaps('fumbling')`, and rime refers to the first vowel phoneme and everything that follows it (i.e, `r scaps('ah1-m-b-l-ih0-ng')` portion of the spoken form). A highly consistent word is one where words with this same body also have a similar pronunciation for this portion of the syllabic structure. We use a proportion for our purposes here, using the total number of words with a given body as the denominator, with the numerator as the number of words that share the rime present in that word [see @Siegelman2020 for other related methods of measurement]. The result is that highly consistent words have values that tend towards one (i.e., all words with a given body share the rime) and inconsistent words have a value that tend towards zero.

The time-varying network shows a similar effect of consistency on accuracy as both the feedforward network as well as human naming data (Figure 6). As consistency increases, we see lower error scores. Consistent words are, in general, easier to read than less consistent words, and this effect is present across both human (left pane) and model data (middle and right panes).

```{r, consistencyHumanComp, echo=FALSE, fig.height=3.5, fig.cap="The relationship between monosyllabic word consistency and error is shown for computational models (feedforward and time-varying) and human naming tasks. In all three there is a relationship between consistency and error such that increases in consistency (across words) are associated with lower error on average. Here errors for human and model data are calculated as previously described with human behavioral data reported as accuracy only, standardizing so that data can share the same y-axis."}

COLORS = c('(C) Time-varying' = 'firebrick', '(B) Feedforward' = 'goldenrod', '(A) Human - Naming Accuracy' = 'Black')


mono %>% 
  filter(stage == STAGE & train_test == 'train') %>% 
  left_join(descriptives, by = 'model') %>% 
  mutate(acc = (accuracy-MEAN)/SD) %>% 
  filter(word %in% elp_words) %>% 
  select(word, model, acc, consistency) %>% 
  rbind(select(d_elp_acc, -freq)) %>%
  mutate(model = case_when(model == 'LSTM' ~ '(C) Time-varying',
                           model == 'Feedforward' ~ '(B) Feedforward',
                           TRUE ~ model)) %>% 
  ggplot(aes(consistency, acc, color = model)) +
  geom_point(size = .6) +
  geom_smooth(method = 'lm', color = 'grey32', fill = 'grey48') +
  scale_color_manual(values = COLORS) +
  facet_grid(~model) +
  labs(x = 'Consistency (standardized)', y = 'Error') +
  theme_apa() +
  theme(legend.position = 'none',
        strip.text = element_text(size = 7))

```

Like the effect of word frequency on accuracy, the effect of consistency diminishes over time. Processing difficulty associated with the consistency of an item can be overcome by having enough experience with it. This can be seen when examining the relationship between consistency and error at two stages of the learning process. Table 7 shows the relationship between consistency and error early in training and late in training, with the correlation decreasing over time as the learner develops experience with words in the training set.

```{r, consistencyEarlyLate, echo=FALSE, fig.height=3.5, fig.cap='The relationship between word consistency and error at two different points in training is shown. With experience, the effect of consistency on errors is reduced.'}

mono %>% 
  filter(stage %in% c('Middle', 'Late') & model == 'LSTM') %>%
  filter(train_test == 'train') %>% 
  mutate(trainstage = case_when(stage == 'Middle' ~ 'Early in Training',
                                stage == 'Late' ~ 'Late in Training')) %>% 
  ggplot(aes(consistency, accuracy)) +
  geom_point(size = .6, color = 'firebrick') +
  geom_smooth(method = 'lm', color = 'grey32', fill = 'grey48') +
  scale_color_manual(values = COLORS) +
  facet_grid(~trainstage) +
  labs(x = 'Consistency (body-rime)', y = 'Mean Squared Error') +
  theme_apa() +
  theme(legend.position = 'none')

```

#### Consistency effects in production mode
These effects can also be seen in accuracy data from production mode of the time-varying network (Figure 8). At the end of training, items from the train and test set were tested for their accuracy, calculated using Euclidean distance between the produced word and the target word. Test items (7% of the corpus held out for test) show the relationship more strongly than train items, but in both sets we see that increases in body-rime consistency are associated with lower error (distance).

```{r, consistencyGeneralization, echo=FALSE, fig.height=3.5, fig.cap='The relationship between (body-rime) consistency and error in production mode are shown, where error is defined in terms of the Euclidean distance between the produced form and its target (higher distance means greater error). This measure is mathematically different from but very similar to mean squared error and other standard error metrics associated with these networks. The left panel shows the trend for test items and the right panel shows training items.'}
mono_lstm_testmode %>% 
  mutate(train_test = case_when(train_test == 'train' ~ 'Training items',
                                train_test == 'test' ~ 'Test items')) %>% 
  ggplot(aes(consistency, wordwise_dist)) +
  facet_grid(~train_test) +
  geom_point(color = 'firebrick') +
  geom_smooth(method = 'lm', color = 'grey32', fill = 'grey48') +
  labs(x = 'Consistency (body-rime)',
       y = 'Euclidean distance (error)') +
  theme_apa()

```

#### The interaction between frequency and consistency
The consistency of a word and the frequency with which it appears during learning have interrelated -- and similar -- effects on learning. Consistency concerns the extent to which a word's structure is shared with other words in the learning environment. Here this is defined in terms of the orthographic body and phonological rime, but we should consider structure more generally outside of this specific operationalization of the construct. More consistent words will contribute in a more substantial way to the weighted connections between units in the network given that they share structure with other words (i.e., that is what makes them consistent). Words that share neighborhood structure (similar pronunciations and similar spellings) will mutually benefit from the weight optimization that happens throughout learning. For example, take the word `r scaps('feed')`. This word is highly consistent because all other words in the training corpus that end in `r scaps('-eed')` (e.g., `r scaps('tweed')`, `r scaps('speed')`, `r scaps('seed')`, etc.) also have the rime `r scaps('...iy-d')`. Therefore learning `r scaps('feed')` benefits knowledge related to the other words in its neighborhood (like `r scaps('tweed')`).

Keep in mind this also applies to other aspects of structure, including structure possessed by words that are greater than one syllable, even though single syllable words are the focus here. For example, the words `r scaps('print')`, `r scaps('sprint')`, and `r scaps('prim')` will benefit from weight updates that result from learning the word `r scaps('prince')` even though this neighborhood of words is defined in a different way than how consistency was defined for the analyses here. Nonetheless, there is reason to believe that it is beneficial to define neighborhoods of monosyllabic words in terms of body-rime units rather than some other portion of syllabic structure due to the reliability of this structure relative to alternatives. For example, @Treiman1995 demonstrated that rime structure is more stable for short monosyllabic words, and that this stability is associated with more explained variance in naming latencies and errors than other syllabic structure.

The effect of frequency of exposure to a word will have a similar result to consistency in that the weights in the network encode properties of frequent words more strongly than infrequent ones. As the frequency of occurrence increases, performance will improve - even for infrequent forms (as in Figure 5). Likewise, as experience increases with words that exhibit atypical structure, performance will improve (Figure 7). These two aspects of learning should be understood as interrelated given their join reliance on experience. Figure 9 shows the relationship between frequency and consistency across training. At advanced levels of training (bottom two panels) the effect of consistency is diminished, and performance on infrequent words is improved with sufficient experience.

```{r, frequencyByConsistency, echo=FALSE, fig.height=4, fig.cap='The relationship between word consistency and errors at two levels of word frequency. Words were binned for frequency based on a mean-split only for purposes of visualization. Early in training the effect of consistency is most prevalent in low frequency words, and this effect is reduced late in training. Also, the overall effect of consistency is attenuated by the end of training, and the effect of frequency becomes less differentiated across levels of frequency.'}
descriptives = mono %>% 
  filter(stage %in% c('Middle', 'Late') & model == 'LSTM') %>% 
  group_by(stage) %>% 
  summarise(M = mean(freq_scaled))

mono %>% 
  filter(stage %in% c('Middle', 'Late') & model == 'LSTM') %>%
  filter(train_test == 'train') %>% 
  mutate(trainstage = case_when(stage == 'Middle' ~ 'Early in Training',
                                stage == 'Late' ~ 'Late in Training')) %>% 
  left_join(descriptives) %>% 
  mutate(freq_f = case_when(freq_scaled <= M ~ 'Low Frequency',
                            freq_scaled > M ~ 'High Frequency')) %>% 
  ggplot(aes(consistency, accuracy)) +
  facet_grid(vars(trainstage), vars(freq_f)) +
  geom_point(size = .6, color = 'firebrick') +
  geom_smooth(method = 'lm', color = 'grey32') +
  labs(x = 'Consistency (body-rime)', y = 'Mean Squared Error') +
  theme_apa()


```

\newpage

These differences in performance between learning stage, frequency, and consistency are seen more precisely in a statistical model of error generated from testing mode. Mean squared error was regressed on the interaction of learning stage, frequency, and consistency (all centered on the mean), yielding significant effects for all predictors and interactions, shown in Table 1.

```{r, stageFrequencyConsistencyModel, echo=FALSE}
descriptives = mono %>% 
  filter(stage %in% c('Middle', 'Late') & model == 'LSTM') %>%
  filter(train_test == 'train') %>% 
  group_by(stage) %>% 
  summarise(frequency_m = mean(freq_scaled),
            consistency_m = mean(consistency)) %>% 
  mutate(stage_f = case_when(stage == 'Middle' ~ -.5,
                             stage == 'Late' ~ .5))

mono %>% 
  filter(stage %in% c('Middle', 'Late') & model == 'LSTM') %>%
  filter(train_test == 'train') %>% 
  left_join(descriptives) %>% 
  mutate(frequency_c = freq_scaled-frequency_m,
         consistency_c = consistency-consistency_m) %>% 
  stats::lm(accuracy ~ stage_f*consistency_c*frequency_c, data = .) -> model

modelCI = confint(model)
modeldata = summary(model)

tabledata = data.frame(modeldata$coefficients) %>% 
  mutate(Estimate = sig_level(Estimate, Pr...t..),
         Pr...t.. = presentp(Pr...t..))

tabledata$CI = squarebracket(modelCI[,1], modelCI[,2], digits = 3)

tabledata$Predictor = c('Intercept', 'Learning stage', 'Consistency', 'Frequency', 'Learn stg x consist', 'Learn stg x freq', 'Consistency x frequency', 'Learn stg x consist x freq')

rownames(tabledata) = NULL
```
\newpage
\ Table 1

```{r, stageFrequencyConsistencyModelRender, echo=FALSE} 

HEADER = c('*b*', '*SE*', '*t*', '*p*', '95% CI', 'Predictor')
names(tabledata) = HEADER

tabledata = tabledata %>% 
  select(Predictor, everything())

apa_table(tabledata, digits = 3, format = 'pandoc', escape = F, align = 'l', landscape = T,
            note = 'Model estimated using lm() in R with confidence intervals estimated using confint(), both methods from the native R stats package [@R2021]. Bold parameter estimates indicate significant *p*-values below the alpha threshold of .05.',
            caption = 'Regression Model of the Interaction Between Learning Stage, Word (Body-Rime) Consistency, and Word Frequency on Mean Squared Error')


```


### Reproducing previous experimental findings
Effects of frequency and consistency have also been established in experiments that manipulate these properties factorially. Results that convey the relationship between a given word property and error of some kind over all words in the training pool are useful insofar as they are suggestive of a given effect, but reproducing effects of such properties from a carefully controlled experiment serves as a more compelling result. In order to do this for the present model, words from several relevant studies were included the training pool, namely @Taraban1987 [also used in @Plaut1996] and @Jared1997. These studies were aimed at examining the graded effects of consistency and the interaction with frequency, all of which focused on these phenomena using monosyllabic words. In the corpus of monosyllabic words assembled, only 19 words from these lists were too short (two phonemes or fewer), and were thus excluded (just as words that were one letter or fewer were excluded). Therefore, the vast majority of the words from these stimuli sets were able to be included for simulations using the time-varying model (`r nrow(filter(taraban_testmode, !is.na(freq_jaredA) | !is.na(freq_taraban)))-19` words total across the two sets).

All of these experimental lists were designed to vary the level of orthography-to-phonology consistency and frequency using a set of categorical distinctions to investigate their graded effect on reading performance. Early experimental work showed that words that are pronounced "by rule" (i.e., are _regular_; like `r scaps('bad')` or `r scaps('save')`) were pronounced more quickly than words that are exceptional in terms of their spelling-sound patterns, as in `r scaps('deaf')` or `r scaps('have')` [@Baron1976; @Forster1973]. It was shown subsequently that naming is affected not so much by whether or not a word is rule-governed of exceptional, but based on the word's neighborhood structure: the extent to which a word is pronounced in the same was as other similarly spelled words [@Glushko1979]. A review of the theoretically critical cases and their implications for models of word recognition can be found in @Seidenberg2022. This neighborhood effect is also exhibited connectionist networks, noting particularly that such models have the ability to generate phonology from orthography despite the absence of rules that map from segments of one perceptual modality to segments of the other (e.g., grapheme-phoneme rules that relate the pronunciation of a letter to a specific phoneme). Computational models that employ a dual-route mechanism require separate processes: one that generates a pronunciation by rule, and another lexical lookup procedure that accesses exceptional cases.

The time-varying architecture proposed here is structured in a similar way, using learning principles like other connectionist networks and should therefore reproduce effects similar to other similar models. Likewise, behavioral effects from controlled experiments with human readers should also be reproducible if the time-varying model is a psychologically valid one. The sections that follow report on several such experiments (some computational, some behavioral) that examine the graded effects of consistency on naming and its interaction with word frequency. Because these stimuli sets are small, in order to avoid results that hinged on a single run of the model^[Keep in mind that some amound of randomness is present in the training process due to (i) random weight initilizations, (ii) randomness due to the batch assembly procedure, and (iii) the particular set of words that are selected for training and hold out sets from the full pool of items.] the model was trained 50 times over with results pooled for a given experimental set.

#### @Taraban1987
Experiment 1a from @Taraban1987 was a simple naming study where participants read aloud words that were either exceptional (words with no neighbors; and example is `r scaps('done')`) or regular but inconsistent (like `r scaps('catch')`). Words that are "regular but inconsistent" are those that can be characterized by grapheme-phoneme rules but have at least one orthographic neighbor which differs in terms of pronunciation. For example, take the word `r scaps('catch')`. This word is regular in that it consists of grapheme-phoneme patterns seen in other words, but is inconsistent because of its orthographic neighbor `r scaps('watch')` with which it differs in pronunciation (specifically in terms of its rime pattern). Additionally, words in the experiment were either high frequency or low frequency. Participants also read a set of control words that had many neighbors. Each control was matched with a test word for the starting letter and phoneme, frequency, and bigram frequency. The study reproduced effects from @Glushko1979 and @Seidenberg1984, but implemented more precise controls for the target words and their corresponding control items than in @Seidenberg1984 (generating pairs that shared a word-initial phoneme). There were 24 words in each group (so with controls, stimuli totaled 24 x 4 x 2 = 192 words). The study found that exception words exhibited slower and more inaccurate responses than inconsistent but regular words. Low frequency words showed this difference more dramatically than high frequency words. The results from the original study can be seen in Figure 10.

```{r, tarabanFig2, echo=FALSE, out.width='250px', fig.align='center', fig.cap='Results for the stimuli from Experiment 1 of Taraban et al. (1987) are shown. Results were calculated over item-level differences in accuracy between experimental words and their controls. This figure is generated from means reported in the original study and does not appear in original study except as a table, but has been included here for a direct comparison to computational model results.'}

COLORS_taraban = c('Regular inconsistent'='grey86', 'Exception'='black', 'regular_inconsistent'='grey86', 'exception'='black', 'Regular consistent'='grey57')


taraban_means %>% 
  group_by(condition, frequency) %>% 
  summarise(accuracy = -difference(accuracy)) %>% 
  mutate(condition = case_when(condition == 'exception' ~ 'Exception',
                               condition == 'regular_inconsistent' ~ 'Regular inconsistent'),
         frequency = case_when(frequency == 'high' ~ 'High',
                              frequency == 'low' ~ 'Low')) %>% 
  ggplot(aes(frequency, accuracy, fill = condition)) +
  geom_bar(stat = 'identity', position = position_dodge(), color = 'black') +
  scale_fill_manual(values = COLORS_taraban) +
  labs(x = 'Frequency', y = 'Errors (difference)') +
  theme_apa() +
  theme(legend.title = element_blank())

```

A similar pattern is seen in the model results. This figure shows the pattern of means across consistency conditions and frequency. The means were calculated as the difference between test and control words in mean squared error. A larger value here indicates that the target word produced more error than its corresponding control. There are clear differences between word types at each level of frequency and across the two levels of frequency. A relevant statistical model is reported on in the section about the @Plaut1996 stimuli which overlap with these. The authors of the original study reported main effects of frequency and word type. This same trend is seen in the model data: high frequency words exhibit lower error than low frequency words, and exceptional words show more error than their more consistent counterparts.

```{r, tarabanFig1, echo=FALSE, out.width='250px', fig.align='center', fig.cap='Model results for the stimuli for Taraban et al. (1987) parallel those of the naming data from that study. Error data are shown as the difference between the target and its corresponding control using mean squared error from the computational model in testing mode.'}

taraban_crossval %>% 
  filter(!is.na(taraban_group)) %>% 
  filter(epoch == 27) %>% 
  group_by(taraban_group, freq_taraban, taraban_test) %>% 
  summarise(mse = mean(mse)) %>% 
  ungroup() %>% 
  mutate(condition = case_when(taraban_group == 1 ~ 'Exception',
                               taraban_group == 2 ~ 'Exception',
                               taraban_group == 3 ~ 'Regular inconsistent',
                               taraban_group == 4 ~ 'Regular inconsistent'),
         freq_taraban = case_when(freq_taraban == 'high' ~ 'High',
                                  freq_taraban == 'low' ~ 'Low')) %>% 
  group_by(condition, freq_taraban) %>% 
  arrange(desc(freq_taraban)) %>% # make sure they are ordered properly
  summarise(mse = -difference(mse)) %>%
  ungroup() %>% 
  ggplot(aes(freq_taraban, mse, fill = condition)) +
  geom_bar(stat='identity', position = position_dodge(), color = 'black') +
  scale_fill_manual(values = COLORS_taraban) +
  labs(x = 'Frequency', y = 'Mean squared error') +
  theme_apa() +
  theme(legend.title = element_blank())

```


Control items in the study we matched for properties with their paired target, but were all from consistent neighborhoods. For example, a word like `r scaps('pull')` (a high frequency exception word) was paired with the control word `r scaps('page')`, which has many orthographic neighbors that share properties of its pronunciation (e.g., `r scaps('page')`, `r scaps('rage')`, etc.). Therefore, we can reorganize the data in order to capture a graded effect of consistency, rather than using the differences in measured responses as was used in the figures above (and in the study). If we collapse across the control items in the study and compare against the exception and regular but inconsistent items, you see a clear and complimentary pattern (this just represents the raw means rather that differences against controls, where the controls are the highest consistency "Regular consistent" condition) ^[In fact the @Plaut1996 study uses these words to populate a condition by that name. They are used as controls in @Taraban1987 because they serve as an appropriate comparison across conditions where items are matched to this control set with a range of structural variables and word frequency.]. Using the control items in this way we see that when they are plotted against means in the other conditions, they come out as low error items (when combined, the lowest error items). This is compatible with the hypothesis that words with the greatest neighborhood structure will present the lowest burden to processing. Figure 12 shows this pattern for the computational model. Low error occurs for words that are highly consistent, high error occurs for words with atypical structure (exceptional words), with words that are regular but inconsistent in between the two.


```{r, tarabanFig3, echo=FALSE, out.width='250px', fig.align='center', fig.cap='Model data for Taraban et al. (1987) stimuli are shown, with the control words reorganized as their own condition (rather than used to caluclate differences between targets and controls. The words (labeled "regular consistent") present the lowest burden on processing during naming.'}

taraban_crossval %>% 
  filter(!is.na(taraban_group)) %>% 
  filter(epoch == 27) %>% 
  mutate(condition = case_when(taraban == 'reg_consistent' ~ 'Regular consistent',
                             taraban == 'regular_control' ~ 'Regular consistent',
                             taraban == 'reg_inconsistent' ~ 'Regular inconsistent',
                             taraban == 'exception' ~ 'Exception'),
         frequency = case_when(freq_taraban == 'high' ~ 'High',
                                  freq_taraban == 'low' ~ 'Low'),
         condition = fct_relevel(condition, c('Regular consistent', 'Regular inconsistent', 'Exception'))) %>% 
  ggplot(aes(frequency, mse, fill = condition)) +
  geom_bar(stat='summary', position = position_dodge(), color = 'black') +
  scale_fill_manual(values = COLORS_taraban) +
  labs(x = 'Frequency', y = 'Mean squared error') +
  theme_apa() +
  theme(legend.title = element_blank())

```

#### @Plaut1996
@Plaut1996 took the word lists from @Taraban1987 and extended them to include _ambiguous_ words, which are words that have approximately equal numbers of friends an enemies, therefore resting in terms of consistency between highly consistent words and (low consistency) exception words. This resulted in the same conditions as those in @Taraban1987 but labeling the controls as "regular consistent", with the new "ambiguous" words, all separated among high and low frequency sets. Their study was a computational one following on from SM89 but using novel representations for phonology in order to mitigate problems associated with slot-based representations. The experiment with stimuli adapted from @Taraban1987 examined the extent to which a feedforward architecture demonstrated the graded effect of consistency in a way that was analogous to skilled readers. Figure 13 shows the cross entropy (the value used as the loss function during training) for each of the four regularity conditions from @Plaut1996 and in the high and low frequency conditions taken from performance half way through training. Cross entropy is used in order to directly compare with the results provided in the original @Plaut1996 study, and is highly correlated with mean squared error (the metric most often used for computational results here).


```{r plautModelFigure, echo=FALSE, fig.height=3, fig.width=6, fig.cap='Time-varying model results across four consistency conditions from Plaut et al. (1996) are shown for both high frequency and low frequency words. Results are averaged across 50 runs of the model.'}

# shapes:
# 0 = ambiguous (square)
# 1 = exception (circle)
# 2 = regular consistent (triangle)
# 5 = regular inconsistent (diamond)
taraban_crossval %>% 
  filter(!is.na(freq_plaut)) %>% 
  filter(epoch == 27) %>% 
  filter(plaut != 'nonword') %>%
  mutate(plaut = case_when(plaut == 'ambiguous' ~ 'Ambiguous',
                             plaut == 'exception' ~ 'Exception',
                             plaut == 'reg_consistent' ~ 'Regular consistent',
                             plaut == 'reg_inconsistent' ~ 'Regular inconsistent'),
         freq_plaut = case_when(freq_plaut == 'low' ~ 'Low',
                                  freq_plaut =='high' ~ 'High'),
         freq_plaut = fct_relevel(freq_plaut, c('Low', 'High')),
         plaut = fct_relevel(plaut, c('Exception', 'Ambiguous', 'Regular inconsistent', 'Regular consistent'))) %>% 
  group_by(freq_plaut, plaut) %>% 
  summarise(loss = mean(loss)) %>% 
  ggplot(aes(freq_plaut, loss, shape = plaut, group = plaut)) +
  geom_line() +
  geom_point(size = 3) +
  scale_shape_manual(values = c(1, 0, 5, 2)) + #old: 0, 1, 2, 5
  labs(x = 'Frequency', y = 'Cross entropy') +
  theme_apa() +
  theme(legend.title = element_blank())

```

The trend seen in the computational results using the time-varying model closely resembles those from @Plaut1996 and follows the expected pattern based on graded effects of consistency as they interact with frequency. Exception words are associated with higher loss (and error) than ambiguous words, which are associated with higher loss than regular words. Among regular words, regular inconsistent words are associated with higher loss than regular consistent words. Additionally, the high frequency group of each consistency category exhibits lower loss than their lower frequency counterparts. The data from the original study is shown in Figure 14.

```{r plautFigureOriginal, echo=FALSE, out.width='200px', fig.align='center', fig.cap='Original results from Plaut et al. (1996) analogous to Figure 13 are provided, taken from the original study. The figure is reproduced by permission from Psychological Review.'}
include_graphics('img/plaut5.png')

```




```{r, echo=FALSE, warning=FALSE}
#####
# only run all this if you want to rerun - estimates were hard coded in manuscript
#####
# runtime is about 25 minutes
# source('scripts/m_plaut1.R')
```


In order to examine the effect more closely, a statistical model was estimated for the interaction between consistency (four levels) and frequency (two levels) on loss halfway through training. The results demonstrate that the effect is similar though not identical to the one observed in @Plaut1996. In this model consistency was expressed as a linearly decreasing set of conditions in the following order: exception, ambiguous, regular inconsistent, and regular consistent. Frequency is coded as either high or low. Using a linear mixed effects model estimated in R [@R2021] in the lme4 package [@Bates2015] with confidence intervals estimated in the stats library [@R2021] and p-values computed using the Anova() method in the car package [@Fox2019] using Kenward-Roger approximation for estimating degrees of freedom with that same package. This model includes by item random intercepts for each run (each simulation).

An increase in condition is, on average, associated with a .01 decrease in loss (*b* = -.01, *SE* = .003, *p* < .001, *F*(1, 172) = 43.31, 95%CI = [-.01, -.005]), with the effect of frequency associated with a decrease in loss of .01 units (*b* = -.01, *SE* = .003, *p* < .001, F(1, 172) = 11.64, 95%CI = [-.014, -.004]). The interaction, unlike what was reported in @Plaut1996, is not significant (*b* = .003, *SE* = .003, *F*(1, 172) = 1.55, *p* = .22, 95%CI = [-.002, .008]). This pattern resembles the effects described earlier looking at frequency and consistency as quantitative predictors (derived from the structure of words in the corpus directly) but differs in the specification of the interaction (because they are defined across conditions rather than as observed). In summary, words of increasing consistency demonstrate lower levels of error, and high frequency words are less error prone than lower frequency words.

```{r,  echo=FALSE, warning=FALSE}
#####
# only run all this if you want to rerun - estimates were hard coded in manuscript
#####

# runtime is about 25 minutes
#source('scripts/m_plaut2.R')

```

@Plaut1996 included a number of statistical tests that aren't central to the demonstration here. Though one is worth considering both because it adds additional precision to the previous results and is important to note for theoretical reasons. In a model of the interaction between regularity (regular versus exception - as was defined in @Plaut1996) and frequency (low versus high), regular words are produced with lower errors than exception words (*b* = -.02, *SE* = .003, *F*(1, 85) = 55.05, *p* < .001, 95%CI = [-.028, -.017]). The effect of frequency maintains as in the previous model reported (*b* = -.009, *SE* = .003, *F*(1, 85) = 8.34, *p* < .01, 95%CI = [-.015, -.003]), as does the non-significance of the interaction term (*b* = .006, *SE* = .006,  *F*(1, 85) = 1.07, *p* = .30, 95%CI = [-.005, .018]). Nonetheless, the test indicates that the time-varying network picks up more narrowly on the difference in processing difficulty between words that can be characterized as regular (both consistent regulars and inconsistent regulars) and those that are exceptional (those that both can't be described via simple rules and those that have no neighbors).

The absence of a significant interaction between consistency and frequency here would require further experimentation to diagnose. @Plaut1996 included a range of experiments examining the variability in effects of including frequency manipulations of different kinds during training, showing that using condensed frequencies when training the network also condenses the effects related to those frequencies. It is quite possible that by expanding the frequency values used to scale gradients during training here would make the effects of frequency (whether raw or categorical) more dramatic. While this additional experimentation is outside the scope of the demonstration of the architecture here, it is worthy of investigation in subsequent work.

In summary, the general trends relating consistency and frequency appear in data from the time-varying network. In statistical models of the experimental effects of words that vary systematically in terms of frequency and consistency, the results are mixed given the absence of a significant interaction between frequency and consistency.


#### Jared (1997)
Behavioral results conveying the tradeoffs between frequency and consistency hold up for the @Jared1997 stimuli as well, with low frequency words in general being more difficult to process than high frequency words, and with inconsistent words being more difficult than consistent ones. This is a complementary study to the replications reported previously, and we will focus on Experiment 1 of @Jared1997 here - a naming task. The task was similar to that of @Taraban1987. In the experiment participants read aloud sets of words (40 words to a list over two separate visits to the lab) that belonged to one of four categories. Words were either low frequency or high frequency and were consistent (all neighbors had the same rime) or inconsistent (neighbors displayed different rimes). For example, the words `r scaps('paid')`, `r scaps('said')`, and `r scaps('plaid')` are inconsistent words used in the experiment given the variability in pronunciation of the portion of the word associated with the letters `r scaps('aid')`.

```{r, echo=FALSE}

jaredA_freq = taraban_crossval %>% 
  filter(epoch == 36) %>% 
  filter(!is.na(jaredA)) %>% 
  group_by(freq_jaredA) %>% 
  summarise(mse = mean(mse)) %>% 
  rename(condition = freq_jaredA)


jaredA_consistency = taraban_crossval %>% 
  filter(epoch == 36) %>% 
  filter(!is.na(jaredA)) %>% 
  group_by(jaredA) %>% 
  summarise(mse = mean(mse)) %>% 
  rename(condition = jaredA)

```

As in the behavioral data in the original study, the model data exhibit the tradeoffs between consistency and frequency. The stimuli were included in the same repeated model runs described previously, where results here are calculated across 50 runs using mean squared error as the metric (again midway through training to examine the frequency effect using testing mode) ^[Note that the use of mean squared error rather than loss never affects the outcome; the effects in one always carry over to the other]. Results can be summarized as follows: low frequency words (*M* = `r as.character(round(pull(filter(jaredA_freq, condition == 'low'), mse), digits = 3))`) are produced with greater error than high frequency words (*M* = `r as.character(round(pull(filter(jaredA_freq, condition == 'high'), mse), digits = 3))`), and inconsistent words are higher in mean squared error (*M* = `r as.character(round(pull(filter(jaredA_consistency, condition == 'inconsistent'), mse), digits = 3))`) than their consistent counterparts (`r as.character(round(pull(filter(jaredA_consistency, condition == 'consistent'), mse), digits = 3))`). Figure 15 shows the pattern in data from the time-varying model, and Figure 16 shows the corresponding data from the original study.


```{r, echo=FALSE, out.width='250px', fig.align='center', fig.cap='Time-varying model results for items from Jared (1997). Like other data concerning a frequency manipulation, the results shown here are taken halfway through training.'}

COLORS_jared = c('Consistent'='grey86', 'Inconsistent'='black')

taraban_crossval %>% 
  filter(epoch == 36) %>% 
  filter(!is.na(jaredA)) %>%
  mutate(freq = case_when(freq_jaredA == 'high' ~ 'High',
                          freq_jaredA == 'low' ~ 'Low'),
         condition = case_when(jaredA == 'consistent' ~ 'Consistent',
                               jaredA == 'inconsistent' ~ 'Inconsistent'),
         condition = fct_relevel(condition, c('Inconsistent', 'Consistent'))) %>% 
  ggplot(aes(freq, loss, fill=condition)) +
  geom_bar(stat= 'summary', position = position_dodge(), color = 'black') +
  scale_fill_manual(values = COLORS_jared) +
  theme_apa() +
  labs(x = 'Frequency', y = 'Mean Squared Error') +
  theme(legend.title = element_blank())
  

```


```{r jared1997Original, echo=FALSE, out.width='200px', fig.align='center', fig.cap='Results for Experiment 1 from Jared (1997) are shown. Naming latency is displayed on the y-axis (with corresponding accuracy above each bar). The figure is reproduced with permission from the Journal of Memory and Language.'}
include_graphics('img/jared1.png')
```

However, a statistical model of the interaction between frequency and consistency on mean squared error show that the effects from the computational data are comparable but not identical to those from the original study. Using a linear mixed effects model with random slopes for words and simulations (again using data from 50 separate runs of the model and the same specifications as previous linear mixed effects models reported), we see a significant effect of frequency and consistency, with no significant effect for the interaction. This is analogous to the results from the stimuli from @Plaut1996, which showed significant main effects but a non-significant interaction.

\ Table 2
```{r, echo=FALSE}
taraban_crossval %>% 
  filter(epoch == 36) %>% 
  filter(!is.na(jaredA)) %>% 
  mutate(frequency = case_when(freq_jaredA == 'high' ~ .5, freq_jaredA == 'low' ~ -.5),
         consistency = case_when(jaredA == 'consistent' ~ .5, jaredA == 'inconsistent' ~ -.5)) %>% 
  lmer(mse ~ frequency*consistency + (1|word) + (1|run_id), data = .)  -> m_jared

#save(m_jared, file = 'data/m_jared.Rda')

#m_jared_anova = car::Anova(m_jared, type=3, test="F")
#save(m_jared_anova, file = 'data/m_jared_anova.Rmd')

#m_jared_confint = confint(m_jared)
#save(m_jared_confint, file = 'data/m_jared_confint.Rda')
load(file = 'data/m_jared.Rda')
load(file = 'data/m_jared_anova.Rda')
load(file = 'data/m_jared_confint.Rda')

x_names = c('Intercept', 'Frequency', 'Consistency', 'Frequency*Consistency')

cap = 'Mixed Effects Model Output for MSE Predicted by Consistency, Frequency, and its Interaction'

t_jared = model_to_table(m_jared, x_names, m_jared_confint, m_jared_anova, caption = cap, include_R_notes = F, notes = 'Model estimated using a linear mixed effects model in R (R Core Team, 2021) in the lme4 package (Bates et al., 2015) with confidence intervals estimated in the native stats library. The p-values were computed using the Anova() method from the car package (Fox & Weisberg, 2019) using Kenward-Roger approximation for estimating degrees of freedom with that same package. This model includes by item random intercepts for each run.')

t_jared

```

When examining the effect of consistency at the level of high frequency words, as in @Jared1997 we see that high frequency consistent words show lower error than their low consistent counterparts, mirroring the earlier analogous effect in naming latency.

```{r, echo=FALSE}
taraban_crossval %>% 
  filter(epoch == 36) %>% 
  filter(!is.na(jaredA)) %>% 
  mutate(frequency = case_when(freq_jaredA == 'high' ~ 0, freq_jaredA == 'low' ~ 1),
         consistency = case_when(jaredA == 'consistent' ~ -.5, jaredA == 'inconsistent' ~ .5)) %>% 
  lmer(mse ~ frequency*consistency + (1|word) + (1|run_id), data = .) -> m_jared2

#save(m_jared2, file = 'data/m_jared2.Rda')

#m_jared2_anova = car::Anova(m_jared2)
#save(m_jared2_anova, file = 'data/m_jared2_anova.Rda')

#m_jared2_confint = confint(m_jared2)
#save(m_jared2_confint, file = 'data/m_jared2_confint.Rda')

load(file = 'data/m_jared2_anova.Rda')
load(file = 'data/m_jared2_confint.Rda')
#summary(m_jared2)
```

#### Variability of effects across training
It should be noted that effects are subjected to variability throughout training. For example, Figure 17 plots consistency and frequency at several different training stages: 27, 36, 45, and 54, 63, and 72 epochs. The pattern for mean squared error looks stable for many points throughout training (and is on average when collapsing across epochs), but at certain points in the learning process the effect may shift. This is clearest in epoch 54 where the pattern shifts completely. Across all training stages, consistent words are on average higher in error than inconsistent words and high frequency words are associated with lower error than their less frequent counterparts - just as in the focal effect described previously and exhibited in behavioral data in Jared (1997). Nonetheless, trajectories in error across learning are clearly subject to variability. While this type of variability isn't commonly described in learning models similar to this one, investigation about its nature and impacts on performance would be worthwhile especially as it compares to analogous behavioral phenomena. A similar result is reported in the final section on multisyllabic words later in this chapter.

```{r, echo=FALSE, fig.cap= 'Error for words from Experiment 1 of Jared (1997) by condition are shown at various points across training for the time-varying model. Data were generated from averaging across 50 runs of the model as in previous results.'}
COLORS_jared = c('Consistent'='grey86', 'Inconsistent'='black')


taraban_crossval %>% 
  filter(!is.na(jaredA)) %>%
  filter(epoch %nin% c(9, 18)) %>% 
  mutate(freq = case_when(freq_jaredA == 'high' ~ 'H',
                          freq_jaredA == 'low' ~ 'L'),
         condition = case_when(jaredA == 'consistent' ~ 'Consistent',
                               jaredA == 'inconsistent' ~ 'Inconsistent'),
         condition = fct_relevel(condition, c('Inconsistent', 'Consistent'))) %>% 
  ggplot(aes(freq, mse, fill=condition)) +
  geom_bar(stat= 'summary', position = position_dodge(), color = 'black') +
  scale_fill_manual(values = COLORS_jared) +
  labs(x = 'Frequency', y = 'Mean Squared Error', fill = 'Consistency', title = 'Epoch') +
  facet_grid(~epoch) +
  theme_apa() +
  theme(legend.title = element_blank(), plot.title = element_text(hjust = .5, size = 12)) 
  

```

### Nonword reading
Looking at performance at the end of training using production mode, the network did quite well in pronouncing monosyllabic nonwords taken from @Plaut1996, which were adapted from @Taraban1987^[Though, see the footnote on Plaut et al., (1996) p. 62 for a discussion of the shortcomings of these stimuli.]. This is taken as a valuable measure of performance of computational models that name words in their capacity to generalize to novel forms [see @Cox2019 for discussion of this point]. Given that in connectionist networks, where there are no "stored" entries of words, the ability to perform on such words evidences the ability of the system to generate reasonable pronunciations via generalization from other similar forms.
\newpage
\ Table 3

```{r plautNonwords, echo=FALSE}
taraban_testmode %>% 
  filter(plaut == 'nonword') %>%
  filter(phonemes_proportion < 1) %>% 
  select(word, phon_read)  %>% 
  filter(word != 'boad') %>% 
  pull(word) -> nonwords_pronounced_wrong

tabledata = taraban_testmode %>% 
  filter(word %in% nonwords_pronounced_wrong) %>% 
  select(Nonword = word,
         Correct = phon,
         Response = phon_read) %>% 
  mutate(Correct = scaps(tolower(Correct)),
         Response = scaps(tolower(Response)))


apa_table(tabledata, caption = 'Errors by Time-Varying Model on Nonwords from Taraban et al. (1987) Study', note = 'See appendix for key to phoneme pronunciations.', format = 'pandoc')

```

Of the 45 from the @Taraban1987 set, `r length(nonwords_pronounced_wrong)` were pronounced incorrectly (i.e., produced something other than the target sequence of phonemes). This amounts to `r length(nonwords_pronounced_wrong)/45`% of words being pronounced incorrectly from this list (compare to `r 13/43`% incorrect from @Plaut1996). Like the errors on nonwords in @Plaut1996, the errors here are all reasonable attempts at a sequence of phonemes given the input orthographic pattern. These errors are shown in Table 3^[Note that the phonological coding (target) for the nonword `r scaps('boad')` was incorrect when the model was trained and has been removed from this table.].

These errors, like those in @Plaut1996 are realistic in the sense that the phones produced approximate what you would expect for the orthographic input, but don't perfectly match the true sequence of phonemes. For example, vowels are produced in the vowel position and consonants in the consonant positions. Additionally, the incorrect vowels and consonants produced are quite close to the proper segments in terms of their phonological similarity to the target segment. The responses for `r scaps('nowl')` involved a vowel with a postvocalic glide (`r  scaps('ow1')`), just not the correct one. Likewise, `r scaps('dase')` and `r scaps('boose')` were incorrect because they produced incorrect voicing on the word-final fricative (either `r scaps('z')` for `r scaps('')` or vice versa).

All of the mistakes can be tied to the neighborhood characteristics of the words as well given that all nonwords here contain ambiguous orthography-to-phonology structure (i.e., their consistency is less than 1 in that they have at least one orthographic neighbor that differs in pronunciation. In this sense the responses can be seen as overgeneralizations based on similar forms. Of the mistakes here, only `r scaps('dase')` and `r scaps('foth')` are surprising given the raw consistency counts (as body-rime calculations) of the neighborhoods involved. Most of the orthographic neighbors of `r scaps('dase')` are not associated with the rime produced here. 67% of words with `r scaps('ase')` are associated with the rime `r scaps('ey-z')`. Likewise, most of `r scaps('foth')`'s orthographic neighbors exhibit the lower unrounded vowel `r scaps('aa')` (like in the words `r scaps('odd')` and `r scaps('lot')`) rather than the vowel produced by mistake (`r scaps('ow')`). This last error can be seen as an effect of an important aspect of the system, though, given that `r scaps('foth')` has a influential high frequency neighbor `r scaps('both')`.

Nonetheless, and consistent with the discussion in @Plaut1996, both humans and computational models produce "incorrect" phonological codes from orthography - though the notion of "correct" pronunciation for novel strings is not straightforward. Behavioral data have shown that reader's responses to nonwords often deviate from the expected production, notwithstanding questions about what the proper production for a nonword is. For example, @Glushko1979 found that nonwords with inconsistent neighborhood structure often resulted in pronunciations that were different than the expected pronunciation based on grapheme-phoneme rules while being consistent with the pronunciation for that orthographic body in some other word [see @Plaut1996 for discussion].

### Discussion
Computational models that learn to name words - the production of a phonological code from an orthographic one - have several established effects that are seen in the time-varying model proposed in this dissertation. The results reported up until this point in the chapter concern several important effects concerning monosyllabic words. These effects are (1) words that are more common are associated with less processing difficulty than less common forms, (2) words that possess uncommon structure (e.g., inconsistent words, exception words, etc.) are more error prone than their typical counterparts, (3) atypical structure can be overcome with ample experience, and (4) generalization to novel forms is possible in a model that learns the mappings of distributed codes of orthography to phonology, and the memorization of such forms isn't necessary to achieve a high level of performance on such words.

The demonstration for monosyllabic words is important given that such words are common in English, and and that monosyllables have been the focus in the dominant theorizing on reading development and its associated phenomena [@Yap2009]. Even though the theory and associated computational model reported on in this dissertation is geared towards accounting for temporal dynamics that play out most dramatically in longer words (e.g., prosody and other syllabic structure), the results reported in the first half of this chapter are important in establishing what have been considered the more basic properties of reading behavior that concern shorter words in the language. Accounting for monosyllabic effects also allows us to draw upon effects that have been well-established through decades of experimentation and theorizing in the learning literature.

Behavioral effects of the interaction of frequency and consistency didn't appear statistically in the results on the time-varying model [those from @Jared1997 and @Taraban1987], while the corresponding main effects did. However, the analysis using data from @Balota2004 did exhibit the standard frequency by consistency interaction, likely capitalizing on the increase in statistical power that comes from using the large sample from that megastudy. Subsequent experimentation with this new architecture will be useful in identifying this interaction, likely additional attention will need to be paid to creating a larger training environment given the additional complexity of the architecture as compared to its simpler, feed-forward counterparts. We take these mixed results to be due to technical aspects of the implementation here rather than the general approach. Similarly, the variability in effects of word structure and frequency across training points to the possibility of follow up work examining the timecourse (across learning) of such effects, and how the properties of computational models in that regard differ to that of human readers.

To summarize, a range of phenomena arising from the computational model reported on this this dissertation reproduce effects seen in other simpler computational models that learn to name words, and (more importantly) effects found in behavioral data from skilled readers. Experimentation with learning monosyllabic words suggests that the architecture here is consistent with models reported elsewhere and inspires confidence in its ability to capture several critical behavioral phenomena. The next section of this chapter extends these findings to longer words that contain more than a single syllable.

## Results for multisyllabic words
The process through which the model was trained on multisyllabic words did not differ from the one used with monosyllabic words, and the architecture is the same for both processes. All that differs is the training corpus. For this purpose, a corpus of 11,103 words was assembled for training and testing in the manner described in Chapter 2. It is important to remember that even though this model processes multisyllabic words and its behavior is often described in subsequent sections in terms of syllabic effects, syllables are not explicitly marked as segments during processing. Stress is marked on vowels, and there are no boundaries in the representations (inputs or outputs) that divide any two syllables. Therefore the model, like humans, is required to learn about prosodic structure over sequences of sounds without feedback about what constitutes a syllable. In addition to the words already described for the multisyllabic corpus, words from @Chateau2003 were included in order to investigate experimental effects of frequency and consistency in multisyllabic words. The model was trained to 100 epochs with training and test data written out across the training process. Prior to examining a range of results comparing the model to behavioral data, a basic characterization of the learning process and associated performance is provided, as in the section of this chapter concerning monosyllabic words. However, in the case of this section more attention is paid to how the model performs on words of varying syllables.

#### Basic description of model behavior
In general, the learning process mirrors that of monosyllabic words, with error decreasing monotonically across training (accuracy monotonically increasing), eventually setting into a stable, asymptotic state. This is seen in Figure 18 with both accuracy and mean squared error plotted at different points across training.

```{r, multiErrorAcrossTraining, echo=FALSE, fig.height=3, fig.cap='Accuracy (left pane) and error (right pane) across training for train and test set of multisyllabic words. Accuracy is calculated as binary accuracy (whether or not a given unit is on the right side of .5) and error is shown as mean squared error. Bars (barely visible) are calculated as the standard error of the mean.'}
N = length(unique(multi$word))

#COLORS_tt = c('Train' = 'darkorchid', 'Test' = 'orangered3')

mse = multi %>% 
  group_by(epoch, train_test) %>% 
  summarise(M = mean(mse),
            SD = sd(mse),
            SEM = SD/sqrt(N)) %>% 
  mutate(train_test = case_when(train_test == 'train' ~ 'Train',
                                train_test == 'test' ~ 'Test')) %>% 
  filter(epoch %in% c(9, 18, 27, 36, 45, 54, 63, 72, 81, 90)) %>% 
  ggplot(aes(epoch, M, group = train_test)) +
  geom_line(aes(linetype = train_test)) +
  geom_errorbar(aes(ymin=M-SEM, ymax=M+SEM), width=.1) +
  #scale_color_manual(values = COLORS_tt) +
  scale_linetype_manual(values = c('Train' = 'solid', 'Test' = 'dashed')) +
  labs(x = 'Epoch', y = 'Mean squared error', linetype = 'Train / Test') +
  theme_apa() +
  theme(legend.background = element_rect(color = 'black'), legend.position = c(.75, .78))

acc = multi %>% 
  group_by(epoch, train_test) %>% 
  summarise(M = mean(binary_acc),
            SD = sd(binary_acc),
            SEM = SD/sqrt(N)) %>% 
  mutate(train_test = case_when(train_test == 'train' ~ 'Train',
                                train_test == 'test' ~ 'Test')) %>% 
  filter(epoch %in% c(9, 18, 27, 36, 45, 54, 63, 72, 81, 90)) %>% 
  ggplot(aes(epoch, M, group = train_test)) +
  geom_line(aes(linetype = train_test)) +
  geom_errorbar(aes(ymin=M-SEM, ymax=M+SEM), width=.1) +
  #scale_color_manual(values = COLORS_tt) +
  scale_linetype_manual(values = c('Train' = 'solid', 'Test' = 'dashed')) +
  labs(x = 'Epoch', y = 'Accuracy', linetype = 'Train / Test') +
  theme_apa() +
  theme(legend.position = 'none')

grid.arrange(acc, mse, ncol = 2)

```

Words with more syllables are associated with greater errors at the end of training. This can be observed using mean squared error as the metric (i.e., using training mode; Table 4) and also by calculating phonemewise errors using production mode. The phonemewise measure provides a different and less graded measure of error, but useful still. Here, if the pattern that the network produces is closer to the correct phoneme than any other, it is considered correct. Once this prediction is made for the entire word, phoneme by phoneme, the calculation is made as the number of phonemes correct divided by the total phonemes in the word. Table 5 shows the accuracy associated with syllable count across all words in the training corpus.

```{r, trainmodeErrorsBySyllable, echo=FALSE}

multi %>% 
  filter(epoch == 99) %>% 
  group_by(nsyll) %>% 
  summarise(M = mean(mse),
            SD = sd(mse)) %>% 
  round(digits = 6) %>% 
  mutate(Syllables = as.character(nsyll),
         M = as.character(M),
         SD = as.character(SD)) %>% 
  select(Syllables, M, SD) %>% 
  apa_table(caption = 'Test Error by Number of Syllables', note = 'Error is calculated as mean squared error at the end of training (using test mode).')

  
```



```{r, testmodeErrorsBySyllable, echo=FALSE}
multi_mistakes = multi_testmode %>% 
  filter(phonemes_proportion < 1) %>%
  select(word, phon_read, nsyll)

multi_nsylls = multi_testmode %>% 
  group_by(nsyll) %>% 
  summarise(N = n())

multi_mistakes_by_syllable = multi_mistakes %>% 
  group_by(nsyll) %>% 
  summarise(n = n()) %>% 
  left_join(multi_nsylls) %>% 
  mutate(Proportion = n/N) %>% 
  select(Syllables = nsyll, `Mistakes` = n, Proportion)

multi_testmode %>% 
  group_by(nsyll) %>% 
  summarise(M = mean(phonemes_proportion),
            SD = sd(phonemes_proportion)) %>% 
  rename(Syllables = nsyll) %>% 
  left_join(multi_mistakes_by_syllable) %>%
  mutate(Syllables = as.character(Syllables),
         M = as.character(round(M, digits = 3)),
         SD = as.character(round(SD, digits = 3)),
         Proportion = as.character(round(Proportion, digits = 3))) %>% 
  apa_table(caption = 'Descriptive Statistics of Errors for Number of Syllables in Each Word in Corpus', note = '"Mistakes" corresponds to the total number of mistakes made for words with that number of syllables, and "Proportion" expresses that number as a proportion of all words with that number of syllables. ')

multi_testmode %>% 
  filter(word %in% multi_mistakes$word) %>% 
  select(Word = word, `Pronunciation by model` = phon_read, `Correct pronciation` = phon) %>% 
  write_csv('data/multisyllabic_model_mistakes.csv')

```


### Examining the model's errors
The types of mistakes made by the model vary, with `r nrow(multi_mistakes)` words out of 11,103 (or `r nrow(multi_mistakes)/nrow(multi_testmode)`% of all words) pronounced incorrectly. A table of all errors can be found at the online repository for the project (https://github.com/MCooperBorkenhagen/dissertation/blob/master/paper/data/multisyllabic_model_mistakes.csv)^[Note that a segment coded as "XX" conveys that the segment was a tie with another phoneme and one coded as "_" means that the model produced a null phoneme (where units are rounded to the nearest whole, in this case being 0).].


```{r, sampleMultiMistakes, echo=FALSE}
mistakes_with_uncertain_phone = multi_testmode %>% 
  filter(str_detect(phon_read, 'XX')) %>% nrow()

mistakes_with_null_phone = multi_testmode %>% 
  filter(str_detect(phon_read, '_')) %>% nrow()
```


Some words are shortened with syllabic reduction, like the word `r scaps('snarled')`, which was trained with the target `r scaps('s-n-aa1-r-ah0-l-d')`, but which produced the single syllable alternative `r scaps('s-n-aa1-r-l-d')`. Other errors were related to stress pattern. For example, the word `r scaps('flatbed')` possesses the stress pattern 1-2 in the training data (where 1 is primary stress and 2 is secondary stress). The produced pattern of stress was 1-0 (i.e., the form `r scaps('f-l-ae1-t-b-eh0-d')`), which doesn't differ from the target in principle and out of context of reading connected text ^[The stress codings were adopted from the CMU dictionary with little change. A vowel possessing a 0 stress designation is considered "unstressed", and whether or not an unstressed vowel is distinct from a vowel with secondary stress is not a clear distinction in this case.]. The pattern produced possessed an explicit distinction between the primary stress (the first syllable) against the subordinate stress (the second syllable). That the stress pattern in this word is 1-2 is arguable. Other words in the training set that showed the 1-0 pattern with `r scaps('eh0')` include `r scaps('t-r-aw1-w-eh0-l')` (`r scaps('trowel')`) and `r scaps('p-r-ih1-n-s-eh0-s')` (`r scaps('princess')`). These words can be thought of as having a weaker ultimate syllable (the unstressed `r scaps('eh0')`), but the extent to which the vowel is distinct from `r scaps('eh2')` (the version displaying secondary stress) is subtle, if not negligible.

A number of mistakes arise from the model either being uncertain about a phoneme (resulting in a tie among phonemes produced; `r mistakes_with_uncertain_phone` total mistakes, or `r round(mistakes_with_uncertain_phone/nrow(multi_mistakes)*100)`% of mistakes) or producing a null phoneme. A null phoneme, is one in which all features are 0^[You can think of such a segment as a vector representation of "`_`".]. This represents `r mistakes_with_null_phone` mistakes, or `r round(mistakes_with_null_phone/nrow(multi_mistakes)*100)` of all mistakes. While a full accounting of this type of error is outside the scope of what will be described here, the error data can be found in the repository referenced earlier. The conditions under which mistakes of this type could be avoided is a matter of future research, but it likely could be avoided through some combination of an expanded training corpus and enhanced training for complex words.

```{r, multiStressMistakes, echo=FALSE}

multi_testmode %>% 
  filter(stress < 1) %>% 
  pull(word) -> multi_stress_mistakes


multi_testmode %>% 
  filter(stress < 1) %>% 
  filter(str_detect(phon_read, 'XX') | str_detect(phon_read, '_')) %>% 
  pull(word) -> multi_stress_indeterminate_pronunciations


multi_testmode %>% 
  filter(stress < 1) %>% 
  filter(word %nin% multi_stress_indeterminate_pronunciations) %>% 
  select(word, phon, phon_read) -> tmp



```

Of the mistakes made, `r length(multi_stress_mistakes)` (or `r round(length(multi_stress_mistakes)/nrow(multi_testmode)*100)`%) involved a mistake associated with the stress pattern. Some stress mistakes relate back to the fact that the model sometimes produces indeterminate phoneme segments (ties between phonemes, or the undetermined "`_`" segment). `r length(multi_stress_indeterminate_pronunciations)` (`r round(length(multi_stress_indeterminate_pronunciations)/length(multi_stress_mistakes)*100)` of all stress errors) can be attributed to such errors. Of those remaining, some mistakes can be attributed to an improper assignment of stress despite the proper approximate contour being present (for example, the stress assigned to `r scaps('reindeer')` incorrectly supplied no stress to the second vowel but in training secondary stress was present). The remaining mistakes involved the wrong stress (e.g., a stressed second syllable present when the first syllable should have been stressed, like in `r scaps('jh-ae1-p-ah0-n')` for `r scaps('Japan')` and `r scaps('sh-ae1-m-p-uw0-z')` for `r scaps('shampoos')`) or the number of vowels being incorrect (e.g, `rscaps('F-AH1-N-IH0-S-T')` instead of `r scaps('f-ah1-n-iy0-ah0-s-t')` for `r scaps('funniest')`). Although, such mistakes only account for `r nrow(filter(filter(multi_testmode, word %nin% multi_stress_indeterminate_pronunciations), stress <1))` words in the corpus and only `r nrow(filter(filter(multi_testmode, word %nin% multi_stress_indeterminate_pronunciations), stress <1))/length(multi_stress_mistakes)`% of total stress mistakes.

#### Comparisons to other datasets
Two types of analyses are presented in order to validate the model of multisyllabic word naming, in a similar fashion to the results for the monosyllabic model. First, a set of relevant predictors are examined for performance over all words in the corpus. These predictors are taken from analyses of ELP data, reported in @Yap2009. Then, experiments that examine the effects of consistency in multisyllabic words are reproduced, namely those from @Chateau2003 and @Jared1990.

```{r ELPModelWords, echo=FALSE}

elp_model_words = multi %>% 
  filter(epoch == 99) %>% # epoch is arbitrary here, values don't change across epochs
  filter(!is.na(elp_rt)) %>% 
  pull(word)
```

### Results from the ELP
Data from the English Lexicon Project [@Balota2007; @Yap2009] were taken and merged with the items from the data from the simulations involving multisyllabic words. A set of standard variables were identified for their applicability to examining the effects of orthography-to-phonology structure on naming: word frequency, orthographic length (number of letters), phonological length (number of phonemes), number of syllables, orthographic neighborhood size, and phonological neighborhood size. The set of words common to both the ELP naming data and the data from the multisyllabic simulations conducted here were subset for the purposes of examining these word-level variables. This yielded data for `r length(elp_model_words)` words in the model corpus (or `r length(elp_model_words)/length(unique(multi$word))`% of words included in the training pool).  The descriptive characteristics of the predictors for this subset of words are shown in Table 6.

\ Table 6
```{r multiWordPredictorsDescriptives, echo=FALSE}
tmp = multi %>% 
  filter(epoch == 99)

predictors =  c('Frequency', 'N syllables', 'Phon length', 'Orth length', 'Orth Lev.', 'Phon Lev.')

tabledata = summarows(tmp, c('freq', 'nsyll',  'phonlen', 'orthlen', 'orth_lev', 'phon_lev'), newnames = predictors, na.rm = T, manuscript = T, notes = 'Frequency values provided from HAL (Lund et al., 1996), which is one of the frequency sources native to the ELP data. Levenshtein distances (orth lev. and phon lev.) were taken from the ELP. Levenshtein distance is calculated as the minimum number of character changes in the sequence that are needed in order to generate a new word from the existing one.')


tabledata
```

These variables were each correlated with the relevant outcome variable from the model and in the ELP. For the model data the mean squared error was used, and for the ELP, the item-level RTs. These correlations (reported as Pearson's product-moment correlation, _r_) appear in Table 7.

\ Table 7
```{r, corsMultiELP, echo=FALSE}

EPOCH = 45

multi %>% 
  filter(!is.na(elp_rt)) %>% 
  filter(epoch == EPOCH) -> tmp

corvars = c('freq', 'nsyll', 'phonlen', 'orthlen', 'orth_lev', 'phon_lev')
header = c('Predictor', '*r* (Model)', '*r* (ELP)')
tabledata = data.frame(matrix(nrow = length(predictors), ncol = length(header)))
names(tabledata) = header
tabledata$Predictor = predictors

tabledata$`*r* (Model)` = sig_level(cortests(tmp, 'mse', corvars, 'estimate'), cortests(tmp, 'mse', corvars, 'p.value'))
tabledata$`*r* (ELP)` = sig_level(cortests(tmp, 'elp_rt', corvars, 'estimate'), cortests(tmp, 'elp_rt', corvars, 'p.value'))


apa_table(tabledata, caption = 'Correlations between Word Variables and Processing Difficulty (MSE for Model and RT for Behavioral Data)', note = "Correlations calculated over the set of words common to both the ELP data and the corpus of words used for training the multisyllabic model. Correlations provided as Pearson's *r*. Bolded coefficients are significant at *p* < .001.", digits = 3, format = 'pandoc', align = 'l', landscape = T)

```

The bivariate relationships presented above, and their direction, are all expected and they are approximately consistent across the model and behavioral data. An increase in frequency is associated with a decrease in processing difficulty (error in the model, latency in humans). The remaining word variables are all positively correlated with processing difficulty: number of syllables, number of phonemes, number of letters, orthographic Levenshtein distance, and phonological Levenshtein distance.

#### Results from experimental wordlists for multisyllabic model
Similar to the results of the model of monosyllabic words, an additional important test of the model's performance concerns the effect of word characteristics on naming in experiments that examine such characteristics in a more controlled manner.

There is a debate in the word recognition literature about whether syllabic processing is a necessary component of word recognition. Experimentation considering this level of representation in the reading system of course focuses on the processing of multisyllabic words. @Jared1990 posited that the processing of multisyllabic words can't be reduced to a syllable-by-syllable process and that their pronunciation is conditioned by the broader orthographic context in which they appear. Results from Experiment 4 from @Jared1990 demonstrate this point. There they showed that when words are delivered one syllable at a time, the magnitude of the effect of inconsistency on naming increases, demonstrating that the context (across syllable boundaries) facilitates their processing^[Data from this experiment weren't available at the time of data analysis for this dissertation and therefore the results weren't included for comparison here. As an alternative, two similar experiments taken from Chateau & Jared (2003) are replicated in this section using data from the time-varying computational model.]. This is expected in a theory where the processing of syllables is simply part of a broader system that associates speech from print by exploiting the statistical (contextual) dependencies in orthography (i.e., a connectionist theory). If syllables were "assembled", you would expect to see left-to-right syllabic processing without regard to the (orthographic context of) the subsequent syllable in the task. @Chateau2003 extended the work in @Jared1990 with experimental results that showed that orthographic segments that extended across traditionally understood orthographic "syllable boundaries"` (i.e., intervocalic prosodic contours characterized by a dramatic reduction and shift in amplitude).

### Chateau & Jared (2003)
The experiments in @Chateau2003 sought to investigate consistency and frequency effects in naming tasks using words with two syllables. They extended the consistency constructs that were articulated in @Jared1990 as well as those in @Taft1992 (the different constructs used are described in this section). Analyses of performance in word naming tasks were performed to determine how atypical structure impacts reading behavior for multisyllabic words and the extent to which appealing to syllabic structure in such measurements enhances predictions about processing difficulty in naming.

Measuring orthography-phonology structure in multisyllabic words is difficult because the established methods for doing so require appealing to syllabic structure in orthography. As described earlier, the most common way to measure consistency involves calculating the predictability of a phonological rime from an orthographic body in monosyllabic words. This is straightforward in words with one syllable because there is no ambiguity about where the syllable boundary is given that such words only possess a single syllable. @Jared1990 was the first study to try to characterize structure in an analogous way in multisyllabic words, with @Chateau2003 as a follow up account.

#### "Orthographic syllabic structure"
More specifically, the concept of consistency articulated in this work concerns a portion of the printed word that the authors (originating in @Taft1992) call the "basic orthographic syllabic structure" (or "BOSS" for short). The idea behind this unit of representation is that it is the portion of the printed form that includes the first orthographic vowel plus all the orthographic consonants that lead up to the second orthographic vowel of the word, provided those orthographic consonants occur in word-ending positions in other (monosyllabic) words in English^[You can find a full description of this construct and related terminology in @Taft1992. It should be noted that the concepts "orthographic vowel" and "orthographic consonant" are problematic in that they assume a fundamental homology between speech sounds and the letters that represent them. This assumption runs against the ideas behind the underlying theory of learning presented in this dissertation. Nonetheless, these terms are used for convenience. These terms are also used in order to describe the effects reported in Chateau & Jared (2003) because their findings serve as an important comparison for the time-varying computational model described here.]. The orthographic "body" they define is the sequence of letters that span from the first orthographic vowel to last orthographic consonant defined in this way.

As an example, consider the word `r scaps('blemish')`. Using the heuristic described, the basic orthographic syllabic structure would be defined as `r scaps('blem')`, and its body would be `r scaps('em')`. Note that, importantly, this definition avoids the issue of where the syllable boundary is with respect to the orthography of the word. This allows for a calculation of the neighborhood statistic (consistency) over such a unit with respect to the phonemes associated with the orthographic form because we can assume that such an orthographic sequence would be pronounced `r scaps('b-l-eh-m')`, despite the fact that `r scaps('blem')` is not a word one would regularly encounter yet has the appearance, at some level, of an orthotactically legal sequence of letters in English.

The assumption that underlies the psychological validity of this orthographic segment as described in @Taft1992 runs contrary to ideas about learning proposed in this dissertation, including how phonology and orthography are represented and processed in the time-varying model. In essence, @Taft1992 articulates a theory about syllabic structure in orthography. This doesn't square with the theoretical assumption that orthographic units aren't processed in syllabic segments. It also assumes that orthography contains homologous structure to that of spoken language syllabic structure. The computational model developed here and the theory of cross-modal perceptual processing conveys a different assumption: that printed words are processed in a continuous fashion (over some grain size) and not syllable by syllable, just like spoken language. These ideas about the syllabic structure of orthography and its processing in @Taft1992 are indeed contested [@Chateau2003; @Jared1990], and the empirical findings about the nature of the basic orthographic syllabic structure have been challenged experimentally [@Seidenberg1987]. More generally, the extent to which syllables (like other subcomponents of words) function independently in as perceptual subcomponents has been debated as well [@Seidenberg1987].

Nonetheless, the experiments in @Chateau2003 are important because they account for the fact that atypical structure at several orthographic grain-sizes predicts processing difficulty in words with more than one syllable. This includes structure in the latter part of a multisyllbic word, namely in the second syllable of a two-syllable word (experiment 4). This is theoretically important because it can only be predicted by a model of reading where these visual segments condition the onset of pronunciation, which isn't possible if phonology is "assembled" in a left-to-right fashion as in the dual-route procedure.

Experiment 3 in @Chateau2003 manipulated the basic orthographic syllabic structure unit factorially across high and low frequency words. Common words were processed more easily (lower response times) than uncommon ones, and words with more consistent basic orthographic syllabic structure were associated with faster RTs than their low consistency counterparts with stimuli across groups being matched for other properties. Similar to the result for @Jared1990 experiment 4, the result for consistency at this level of orthographic grain suggests that printed words aren't decomposed syllable by syllable during reading given that the orthographic segment manipulated spans what would considered to be a syllabic boundary (if orthographic segments were processed syllable-by-syllable in terms of their spoken syllabic segments). In their experiment, even high frequency multisyllabic words show a (simple) effect of neighborhood consistency, similar to the effects for monosyllabic words from @Jared1997.

Experiment 4 in @Chateau2003 showed that the consistency of the orthographic vowel in the second syllable (high versus low) of two syllable words predicted meaningful differences in reaction times in a naming task. For example, the `r scaps('e')` in `r scaps('funnel')` is associated with a more predictable pronunciation (higher in consistency) than the `r scaps('o')` in `r scaps('fathom')` (lower in consistency). The results (data and means plotted in bottom right of Figure 19) show that RTs differ as a function of the consistency of the pronunciation of the orthographic vowel in the latter part of the word.

The patterns of means and datapoints for stimuli from experiments 3 and 4 from @Chateau2003 for the model (taken halfway through training; epoch 45) are depicted in Figure 19 (bottom row). The figure also shows the behavioral data the original experiment on the top row. Graphical depiction appears to show differences in consistency (both experiments 3 and 4) and frequency (experiment 3).

```{r Jared2003Figure, echo=FALSE, out.width='500px', fig.align='left', fig.cap='Processing difficulty (mean squared error for computational model; RT for behavioral data) for stimuli from Chateau et al. (2003) experiments 3 and 4 are shown. Points are individual words and bars are calculated as the standard error of the mean.'}
include_graphics('img/figure19.png')
```

```{r ChateauModels, echo=FALSE}

m_chateau3 = multi %>% 
  filter(epoch == 45) %>% 
  filter(!is.na(chateauB_consistency)) %>%
  mutate(consistency = case_when(chateauB_consistency == 'high' ~ .5,
                                 chateauB_consistency == 'low' ~ -.5),
         frequency = case_when(chateauB_frequency == 'high' ~ .5,
                               chateauB_frequency == 'low' ~ -.5)) %>% 
  lm(log(mse) ~ consistency*frequency, data = .)



m_chateau4 = multi %>% 
  filter(epoch == 45) %>% 
  filter(!is.na(chateauC_consistency)) %>%
  lm(log(mse) ~ chateauC_consistency, data = .)

```

Only the consistency effects from experiment 4 hold up to a statistical model, however. A model was estimated predicting mean squared error (log transformed to adjust skew) from consistency condition (low versus high). The difference in error between the low and high consistency conditions in the model data from that experiment are associated with a difference of 2.61 (log transformed) units (*b* = 2.61, *SE* = 1.25, *F*(1, 48) = 4.34, *p* < .05, 95%CI = [.09, 5.12]). None of the conditions, including the interaction term, for experiment 3 are significant in the model data. These can be found in Table 8.

\ Table 8

```{r, ChateauExp3Table, echo=FALSE} 
modelCI = confint(m_chateau3)
modeldata = summary(m_chateau3)

tabledata = data.frame(modeldata$coefficients)

tabledata$Estimate = round(tabledata$Estimate, digits = 2)
tabledata$Std..Error = round(tabledata$Std..Error, digits = 2)
tabledata$t.value = round(tabledata$t.value, digits = 2)

tabledata = tabledata %>% 
  mutate(Estimate = sig_level(Estimate, Pr...t..),
         Pr...t.. = presentp(Pr...t..))

tabledata$CI = squarebracket(round(modelCI[,1], digits = 2), round(modelCI[,2], digits = 2))

tabledata$Predictor = c('Intercept', 'Consistency', 'Frequency', 'Consistency x frequency')

rownames(tabledata) = NULL

HEADER = c('*b*', '*SE*', '*t*', '*p*', '95% CI', 'Predictor')
names(tabledata) = HEADER

tabledata = tabledata %>% 
  select(Predictor, everything())

apa_table(tabledata, digits = 3, format = 'pandoc', escape = F, align = 'l', landscape = T,
            note = 'Model estimated using lm() in R with confidence intervals estimated using confint(), both methods from the native R stats package [@R2021]. Bold parameter estimates indicate significant *p*-values below the alpha threshold of .05.',
            caption = 'Regression Model of the Interaction Between Consistency Word Frequency on Mean Squared Error (Log Transformed)')


```

Follow up experimentation will need to be done in order to examine the the role of frequency and consistency as implemented in @Chateau2003 and why the results don't hold up in the computational data. One simple issue is statistical power. The design from their study yields 16 words per condition (64 words total across two levels of frequency and two levels of consistency). A study could be devised that extends the paradigm to include more words. The effects in the original study were significant fir their RT data for frequency and consistency in their analyses by participant, but were marginal by items. They also found no significant interaction between the two variables. The model results come from a single run of the model and therefore can't capitalize on variance across "subjects" (i.e., runs of the model, as was done in the simpler monosyllabic simulations). Additionally, the skewed outcome variable may be a contributing factor. Even after log transforming mean squared error, the data aren't normally distributed, likely biasing the model results. The finding from the simulation data using stimuli from experiment 4 are suggestive that effects from the behavioral tasks from @Chateau2003 carry over to the time-varying computational model, and that the model exhibits consistency effects even for orthographic segments in the latter portion of words. Further experimentation with the architecture will investigate the role of atypical print-to-speech structure on producing a phonological code, and characterize more precisely how the location of atypical structure affects such production.

Finally, simulations should be run on a larger set of words, which may yield more reliable effects in general. @Sibley2010 and @Sibley2008 sought to simulate at a level that approached the "scale of real wordform lexicons" [@Sibley2010]. Their simulations included training corpora of approximately 75,000 and 28,000 words respectively - vastly more than the approximately 11,000 words used in training the time-varying model. It may well be that a limitation of the results on the model reported here concerns the size of the training set, and enhancing this aspect of the architecture and its training is a worthwhile future direction.

#### Variable effects over time
Similar to the variability found across training for the stimuli from @Jared1997 the pattern for the stimuli from @Chateau2003 also shift over time in the computational data. This variability isn't typically reported in analogous computation work related to reading development, but it bears mention here. Figure 17 shows the patterns of means for the consistency and frequency for a range of timepoints across training using the @Chateau2003 experimental stimuli (their Experiment 3).

```{r cj2003AcrossTraining, echo=FALSE, warning=FALSE, fig.height=3, fig.cap='Error as a function of frequency and consistency for a range of timepoints across time-varyig model training. Mean squared error has been standardized within each epoch in order to put barplots on a common scale (i.e., in terms of standard deviations of MSE).'}
N = multi %>% filter(epoch == 45) %>% filter(!is.na(chateauB_consistency)) %>% nrow()

COLORS_chateau = c('H' = 'grey49', 'L' = 'black', 'High' = 'grey49', 'Low' = 'black')

tmp = multi %>% 
  filter(!is.na(chateauB_consistency)) %>% 
  mutate(consistency = case_when(chateauB_consistency == 'low' ~ -.5,
                                 chateauB_consistency == 'high' ~ .5),
         frequency = case_when(chateauB_frequency == 'low' ~ -.5,
                               chateauB_frequency == 'high' ~ .5)) %>% 
  group_by(epoch) %>% 
  summarise(SD = sd(mse),
            M = mean(mse))


multi %>% 
  left_join(tmp) %>% 
  filter(!is.na(chateauB_consistency)) %>% 
  filter(epoch %in% c(45, 54, 63, 72, 81, 90, 99)) %>% 
  mutate(consistency = case_when(chateauB_consistency == 'high' ~ 'High',
                                 chateauB_consistency == 'low' ~ 'Low'),
         frequency = case_when(chateauB_frequency == 'high' ~ 'H',
                               chateauB_frequency == 'low' ~ 'L')) %>% 
  group_by(epoch, word) %>% 
  summarise(Z = (mse-M)/SD,
            consistency = first(consistency),
            frequency = first(frequency)) %>%
  ungroup() %>% 
  group_by(epoch, consistency, frequency) %>% 
  summarize(Z = mean(Z)) %>% 
  ggplot(aes(factor(frequency), Z, color = factor(consistency), group = consistency)) +
  geom_point() +
  geom_line() +
  facet_grid(~epoch) +
  scale_color_manual(values = COLORS_chateau) +
  labs(x = 'Frequency', y = 'MSE (standardized)', color = 'Consistency') +
  theme_apa() +
  theme(legend.position = 'top',
        legend.title = element_text(size = 12),
        legend.text = element_text(size = 12))

```

However, using the standardized outcome variable (mean squared error here) within each training cycle and averaging across training epochs you see the expected pattern emerge (Figure 20). Calculating error in this way captures a (nonindependent) sample of model states, and allows us to observe the relationship between consistency (defined over the basic orthographic syllabic structure) and frequency on error during training. This captures the expected trend, with lower error for high frequency and high consistency words, and is even suggestive of an interaction between the two variables. Aggregating model performance in this way may in fact accomplish they type of sampling that could facilitate estimating more reliable effects of word properties on processing difficulty in the computational model, and thus avoid problems of having too little data in estimating effects of model behavior. Future work could be done to capture more precise characteristics of these dynamics especially as they relate to the behavior of cognitive computational systems like this one and the analogous behavior in humans.

```{r, cj2003AveragedAcrossTraining, echo=FALSE, warning=FALSE, fig.cap='Error plotted against frequency and consistency for the Chateau et al. (2003) stimuli standardized and collapsed across epochs for the time-varying model.', fig.height=3, fig.width=5}


tmp = multi %>% 
  filter(!is.na(chateauB_consistency)) %>%
  group_by(epoch) %>% 
  summarise(SD = sd(mse),
            M = mean(mse),
            SEM = SD/sqrt(N))



multi %>% 
  left_join(tmp) %>% 
  filter(!is.na(chateauB_consistency)) %>% 
  mutate(consistency = case_when(chateauB_consistency == 'low' ~ 'Low',
                                 chateauB_consistency == 'high' ~ 'High'),
         frequency = case_when(chateauB_frequency == 'low' ~ 'Low',
                               chateauB_frequency == 'high' ~ 'High')) %>% 
  filter(epoch %in% c(45, 54, 63, 72, 81, 90)) %>% 
  group_by(epoch, word) %>% 
  summarise(Z = (mse-M)/SD,
            consistency = first(consistency),
            frequency = first(frequency)) %>%
  ungroup() %>% 
  group_by(frequency, consistency) %>% 
  summarise(Z = mean(Z),
            frequency = first(frequency), 
            consistency = first(consistency)) %>% 
  ggplot(aes(factor(frequency), Z, color = factor(consistency), group = consistency)) +
  geom_point() +
  geom_line() +
  scale_color_manual(values = COLORS_chateau) +
  labs(x = 'Frequency', y = 'MSE (standardized)', color = 'Consistency') +
  theme_apa()

```

### Discussion
The aim of this section was to examine aspects of the time-varying model's behavior with respect to how it processes multisyllabic words. This was done through several different types of analyses: by examining the type of phonological codes (including errors) it produces at the end of training using the model's production mode, examining nonword production using data from the English Lexicon Project [@Balota2004], and comparing performance to other behavioral studies concerning multisyllabic words. Results were surveyed at a relatively high level given the novelty of the architecure and the fact that there are a range of phenomena left to analyze and explain relevant to behavioral aspects of reading long words aloud.

The results reported here are mixed. In general, a range of effects are suggestive of processing taking place in computational model in an analogous way to skilled readers. This includes the ways in which processing is sensitive to atypical structure and other word variables that have been demonstrated to impact performance in naming for skilled readers. Importantly, the phonological codes produced at the end of training to a high level of skill (and inspected using output from the model's production mode) are realistic in a number of ways. The productions associated with nonwords and the segmental mistakes made by the model can easily be understood in terms of patterns of generalization from other words in the language. While production of indeterminate segments remains relatively common, there are a range of possibilities that could enhance production and improve generalization. Several have been mentioned previously. The most obvious, discussed in the previous section, is to grow the training environment beyond the relatively limited corpus that was used for training in the simulations included in this report.

The failure to reproduce the effect from expriment 3 in @Chateau2003 is a concern, especially given the importance in reading aloud as a function of experience (word frequency as a proxy) and the processing atypical spelling-sound structure [operationalized as consistency in the way described by @Taft1992]. Possible ways in which that effect may be investigated more fully were discussed in the section devoted to those experiments. However, it is important to note that those effects have been reported in other analyses here, namely in the results for multisyllabic words for words included in the ELP megastudy [@Balota2004; @Yap2009] and replicating the effect from experiment 4 from @Chateau2003. It should also be considered, that experimentation concerning neighborhood effects in multisyllabic words is still relatively young. Analyses concerning monosyllabic words have the benefit of drawing from decades of research on the myriad effects that are relevant for understanding the reading system, especially nuanced and graded effects of spelling-sound structure. Research and experimentation on longer words is less mature; far less attention has been paid to models of learning and performance concerning multisyllabic words.

\newpage
