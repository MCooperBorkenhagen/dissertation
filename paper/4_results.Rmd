---
title: "4. Behavioral experiments"
output: html_document
---

# Chapter 4: Experimentation


The purpose of the model developed here is to read printed words aloud. A number of results are presented based on results from behavioral research, features of the cognitive model the architecture represents, and comparisons to similarly oriented models of word reading from elsewhere in the literature. The purpose of this experimentation is twofold: to present results that convey important features of the architecture as it relates to cognition and development and to make comparisons to experimentation elsewhere of similar phenomena, whether based on a computational approach or behavioral one.

## Accuracy
Results of the model's ability to correctly produce a spoken form of a word can be based on a number of different measures arising from the same computational system. The most common measure of the model's ability is an accuracy score, which itself can be structured in a few different ways - similar to that of a human reader. Given that the goal of the model is to take a sequence of letters and recode them in the form of a sequence of speech sounds, individual sounds can be inspected for their accuracy along with the entire sequence of sounds - that is, the entire word. Both the phoneme-wise and the word-wise accuracy metrics are used in reporting, depending on the purpose of the result shown. They are of course correlated given that a word that is produced correctly is one in which all the phonemes are produced correctly (or at least each phoneme produced is closer to the correct phoneme than all other possible phonemes).

A limited corpus of words has been selected for training purposes here, though the model is capable of attempting to produce phonology from any sequence of letters provided on the input - a feature we see as a virtue of the implementation given that humans are also capable of this.

## Training details

## Testing trials
The model is tested in a way that follows a different procedure than that used in training, allowing for more flexible and deeper introspection about the dynamics at play in the system when operating. When a word is submitted for testing (or "production") an orthographic pattern is introduced to the the orthographic input - the model's visual input system. This pattern is structured just like that of the orthographic input during training: it is a two dimensional array with as many letter representations along the first axis as there are letters in the word. This time-varying input pattern is input to the orthographic input, and its representation decomposed into its two critical states for use in phonology: the cell state and the final hidden state. Once these vectors are achieved for the orthographic form of the word, that portion of the model rests and is unchanged for the remainder of the trial.

The states of the orthographic layer are passed to phonology to generate the phonological code output. This process is initiated by the input of the start of word segment (`#`), the representation of which was described earlier. The fact that the model has been trained with an explicit start of word marker enables the phonological layer to produce a next phoneme given the current state of the layer and its input pattern. At the start of production this context is provided completely from the orthographic layer. That is, the role of the phonological portion of the model is to produce a phonological code given the orthographic context.

Once the first phonemic pattern is produced it is saved for subsequent phonemes to be appended to it in order to form a full sequence of phonemes (a production). While the weights in the network are frozen at this stage, the state of the phonological LSTM is updated at each timestep following the same processing flow as described in the architecture section. Once a phoneme is output at each timestep, it is fed back to the input of the phonological layer to support subsequent productions. This differs in process from training trials but is the same conceptually given that in training, the entire phonological sequence is processed one segment at a time without the feedback process from the output layer back to the input. Because the output for a given phoneme is never specified as a perfect veridical binary pattern, any phoneme output is compared to all possible phoneme outputs using the L2 norm and the nearest phoneme is selected and recorded as the produced phoneme. The production trial progresses until the phonological output layer produces the "stop producing" segment (`%`). At this point the word produced is recorded for analysis.

## Dispersion
Sequential approaches to orthography-to-phonology conversion solve this problem by processing discrete speech segments from discrete print segments in a left-to-right, segment-by-segment manner. These solutions always involve some form of justification (sometimes called "alignment" in the literature), though the alignment procedure differs across sequential implementations. For example, @Sejnowski1987 provided orthographic input strings as sequences of letters and mapped them to equal-length sequences of phonemes. This allowed their network to implement a kind of temporal processing over visual and its corresponding spoken pattern, but required architectural specifications that deviate from assumptions about processing in other connectionist networks. Their procedure required inserting null elements in slots where a phoneme wasn't present for the corresponding input letter (or grapheme) given that the input and output sequences were always equal length. So, for the input pattern "late" the corresponding output would be `L - EY - T - __`, with the final phoneme segment `__` being the null one given the word-final silent "e" in "late". This avoids the type of dispersion found in feedforward networks, where knowledge about letters and their corresponding sounds is localized to weighted connections associated with specific slots given the way that input and output patterns are specified during learning, but introduces a different type of knowledge localization that is undesirable: that letters on the input layer become associated too narrowly with phonemic segments on the output given the segment-by-segment processing mechanism (i.e., they lose the context sensitive nature of processing due to the segment-by-segment way that print and speech are associated in the network).

Dual-route models use an assembly method, where a letter or letter segment (e.g., grapheme) is associated with a phonemic segment by assembly rule, where "assembly" in this literature refers to the association of phonological segments to its corresponding orthographic segment. Processing speech from print involves assembling the phonology from the pre-specified letter-sound rules, which are derived via an analytic technique that resides outside the scope of the computational architecture itself (i.e, specified by the experimenter). The assembly process happens alongside a lexical lookup procedure, which has its own separate timecourse. The relative timecourse of these processes determine whether a word is produced via assembly rules or as a structured lexical object. This is the extreme end of the spectrum of methods that result in knowledge dispersion. Here knowledge about the relationship between visual and spoken elements is dispersed across the symbolic rules that operate over the crossmodal assembly that takes place during learning and performance. This form of knowldge dispersion is different implementationally but related conceptually in that what is known about similar elements in the domain becomes dissociated based on the structure of the architecture, which of course has implications on the learning that takes place given that architecture.

Other approaches handle input strings with the use of slots, but in a different way than in @Sejnowski1987. Commonly in connectionist feedforward implementations input patterns are specified in a vowel-centered arrangement (see @Plaut1996 for a discussion). A similar solution exists in hybrid connectionist dual route architectures like @Perry2010. Here the input patterns are fit into a template defined by structure in terms of orthography across syllables, which is specified by the limits of the structure of the training set. So, for example the template used for the two syllable template in @Perry2010 involves fitting orthographic inputs into a `CCCVCCCC` pattern on each of the two (possible) syllables via a "graphemic buffer". This is the mechanism that defines the "slots" on the visual input to which any given input pattern are aligned to the available phonemes.

In the present computational system there is no alignment, justification, or slots. However, an important question concerns whether or not knowledge is dispersed in an architecture that learns in the way that this one does. The most direct comparison is to models that contain weighted connections between input and output units, and intermediary hidden ones too. The issue of dispersion is investigated in the present architecture in two ways. First, we present data about the extent to which identical input patterns in different (orthographic) contexts are associated with different units in the network and, second, the extent to which the overall pattern of representation of identical input patterns are similar to each other.

A further piece of supporting data concerns a manipulation to the masking mechanism used in the model. Masking introduces a kind of justification, but one where masked segments should never be realized in the knowledge of the learner.






## Benchmarks
*jared1990: latencies increase with number of syllables, but is moderated by frequency (Experiment)
*jared1990: exceptional and regular inconsistent words exhibit longer latencies as compared to regular words, but only for low frequency words (see yap2009, top right of p. 503 for discussion of the effect)
*position of irregularity effects


kawamoto1998 showed that words with less predictable pronunciations of the vowel exhibit longer latencies for the consonant in the onset, rather than simply longer overall naming latency of the entire word. This is due to the fact that the vowel pronunciations that are less predictable require more processing time, and this processing manifests in the system dwelling on pre-vocalic consonants, rather than being distributed throughout the naming process for the entire word. Their studies were performed using monosyllabic stimuli that were quite simple, though well established in the word reading literature related to naming and related phenomena.

## Canonical naming effects
### Frequency
### Regularity
### Frequency by Regularity

## Latency
Given that computational processing times on a GPU haven't been validated as an analogous method for naming latency in humans, similar to Sibley et al. (2010) a measure of model confidence for processing an orthographic word was designed to measure the latency for reading that word. 


## Overall results
*Provide an overall summary of results here before going into the piecemeal results. See Seidenberg & McClelland (1989) pages 531-532 for examples