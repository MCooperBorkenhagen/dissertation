---
title: "3. Experimentation"
output: html_document
---

```{r ch3_src, include=FALSE}
source('chapter3_data.R')
source('chapter3_taraban_data.R')
source('plot-specs.R')
```

# Chapter 3: Experimentation
The purpose of the model developed here is to read printed words aloud. A number of results are presented based on results from behavioral research, features of the cognitive model the architecture represents, and comparisons to similarly oriented models of word reading from elsewhere in the literature. The purpose of this experimentation is twofold: to present results that convey important features of the architecture as it relates to cognition and development and to make comparisons to experimentation elsewhere of similar phenomena, both from work in computational models of reading and human behavior.

## Accuracy
Results of the model's ability to correctly produce a spoken form of a word can be based on a number of different measures. The most common measure of the model's ability is an accuracy score, which itself can be structured in a few different ways - similar to that of a human reader. Given that the goal of the model is to take a sequence of letters and recode them in the form of a sequence of speech sounds, individual sounds can be inspected for their accuracy along with the entire sequence of sounds - that is, the entire word. Both the phoneme-wise and the word-wise accuracy metrics are used in reporting, depending on the purpose of the result shown. They are of course correlated given that a word that is produced correctly is one in which all the phonemes are produced correctly (or at least each phoneme produced is closer to the correct phoneme than all other possible phonemes).


## Training details
### Batch assembly
In order to avoid padded input and output representations for training, words were assembled for learning based on phonological length for reasons discussed previously. Batches consisted of words of the same phonological length, where the orthographic length of words within a batch could vary but were accommodated with masked timesteps on input (i.e., timesteps that are skipped rather than empty). This is due to the fact that words that share the same number of phonemes don't necessarily share the same number of letters. For simplicity during training, batches were selected from all words of the same phonological length until all the words of that length were trained, then a new length was selected for training. This was repeated until all words were trained during that epoch. Within each epoch, a set of shorter words (e.g., words with a phonological length of five phonemes) were more likely to come earlier in the epoch than words of a longer length (e.g., those with 8 phonemes). This probabalistic sampling based on length doesn't have significant effects on learning because within each epoch all words are presented at some point, the sampling procedure only dictates the set of words (based on phonological length) that is selected and trained before moving onto words of the next phonological length.

Alternatives to this process are possible. This one was selected for its simplicity. Possibilities include specifying batches of one training example at a time (i.e., stochastic gradient descent). Investigating the role of batch size on learning, and the behavioral correlates in humans would no doubt be a worthwhile avenue of future research but is outside the scope of the work being done here. The assembly of batches in this way isn't intended to convey a hypothesis about how processing in humans works, rather is specified as it is for efficiency reasons - alternative such as stochastic gradient descent can lead to training times of over an order of magnitude longer than those where larger batches are assembled.

## "Testing" trials
For generalization tests, the model can be tested in a way that follows a different procedure than that used in training. These will be called _generalization trials_, and are performed as follows. When a word is submitted for testing an orthographic pattern is introduced to the the orthographic input - the model's visual input system. This pattern is structured just like that of the orthographic input during training: it is a two dimensional array with as many letter representations along the first axis as there are letters in the word. This time-varying input pattern is input to the orthographic input, and its representation decomposed into its two critical states for use in phonology: the cell state and the final hidden state. Once these vectors are achieved for the orthographic form of the word, that portion of the model rests and is unchanged for the remainder of the trial.

The states of the orthographic layer are passed to phonology to generate the phonological code output. This process is initiated by the input of the start of word segment (which we can think of as the symbol `#`), the representation of which was described earlier. The fact that the model has been trained with an explicit start of word marker enables the phonological layer to produce a next phoneme given the current state of the layer and its input pattern. At the start of production this context is provided completely from the orthographic layer. That is, the role of the phonological portion of the model is to produce a phonological code given the orthographic context.

Once the first phonemic pattern is produced it is saved for subsequent phonemes to be appended to it in order to form a full sequence of phonemes that constitutes a phonological output. While the weights in the network are frozen at this stage prior to testing, the states of the phonological LSTM is updated at each timestep following the same processing flow as described in the architecture section. Once a phoneme is output at each timestep, it is fed back to the input of the phonological layer to support the prediction of subsequent phonemes. This differs in process from training trials but is the same conceptually given that in training, the entire phonological sequence is processed one segment at a time without the feedback process from the output layer back to the input. Because the output for a given phoneme is very unlikely to be specified as a perfect binary pattern (because predictions almost always have until that approach zero or one, but aren't actually those values), any phoneme output is compared to all possible phoneme outputs using an L2 norm and the nearest phoneme is selected and recorded as the produced phoneme (both the symbol representing the phoneme and the activity pattern are saved). The production trial progresses until the phonological output layer produces the stop producing segment (`%`) or exhausts the total possible number of segments given the longest words included in training. At this point the word produced is recorded for analysis.

The produced pattern (in array form) is a time-distributed series of vectors representing patterns of activation over phonological features. These patterns can be thought of as a pattern that might pre-specify a motor command to be passed to an articulatory mechanism during production (see @Seidenberg1989 for more of a discussion on the process). The quantitative objects produced by the model can be examined for their accuracy in a number of ways, including how close each unit is to its target activation, how close each phonemic pattern is to its corresponding target, and how close the entire word is to its target word. The concept of time in terms of naming latency in this architecture is much less clear, especially given the novelty of application of LSTM architectures to understanding human cognitive processes related to reading and speech. This is a topic for further research, and is likely to be fruitful given the temporal nature of such a computational system. Nonetheless, only accuracy metrics are considered here.

### Frequency
The role of experience in the development of reading skill is an essential one to the reading process [@Seidenberg2018]. More experience with a word facilitates knowledge about that word and the ability to use the word in production and naming. The simplest and most common way that experience effects in word reading have been investigated has been in terms of word frequency as estimated from corpora of text or speech. Word frequency can be implemented in computational models of reading in different ways. Implementations have included the selection of words for training in a probabilistic fashion with the probability of selection determined by the word's frequency of occurrence (e.g. @Seidenberg1989) and the scaling of the learning signal during training proportional to the frequency of that word. The latter was used here for computational efficiency given the implementation of the batch training regimen used, where the gradients calculated for a learning trial were scaled proportional to the frequency of that word in a large corpus of text (see @Sibley2010 for a similar implementation). Frequencies from the hyperspace analogue to language (HAL) were used [@Lund1996], accessed through the data provided by the English Lexicon Project [@Balota2007]. These frequency calculations were utilized to facilitate a comparison to the behavioral data also provided in the ELP, such that more direct comparisons could be made between the computational model here and the human behavioral data there.

Because word frequencies exhibit a dramatic range, a scaling operation was used to both capture the distribution and translate the value into a proportion (where a value of one would cause the full gradient for that word to participate in the weight update, and a value of zero would mean throwing that gradient away). To accomplish this, the formula from @Seidenberg1989 (p. 530) was used to calculate $p$, the proportion applied to the gradient update for the word being learned.

$$
p = K \cdot log(frequency + 2)
$$
This results in a distribution of proportions for each word that is monotonically related to the raw frequencies of those words. The value of the constant $K$ was selected such that the highest value for $p$ was `.93` for the most frequent word (just as in @Seidenberg1989) - the word `and` (raw frequency of `r prettyNum(freq_for_word(mono, 'and'), big.mark=',')`). For the monosyllabic corpus this resulted in `K` being equal to `r round(monosyllabic_k, 3)`. A plot of the distribution of values for $p$ against raw values for frequency are shown in Figure X.

```{r monopbyraw, echo=FALSE, message=FALSE, warning=FALSE, fig.width=3, fig.height=3, fig.cap='Scaled word frequency by raw frequency for words included for training in the time-varying monosyllabic model implemented. For reference, the most frequent word, "and", can be seen in the upper right hand corner.'}
mono %>% 
  ggplot(aes(freq, freq_scaled)) +
  geom_point(color = 'firebrick', size = .05) +
  theme_bw() +
  labs(x = 'Raw frequency', y = 'Scaled frequency (p)')
```


## Monosyllabic words
Given that a number of behavioral effects for reading monosyllabic words are well established, along with investigation of those effects using computational models of such processes, a set of monosyllabic words was assembled from the full corpus of words and used for experimentation. A list used in other connectionist modeling exercises was used for this list [@Cox2019]. Morphological variants that were related with the addition of `r scaps('s')` or `r scaps('ed')`, and verbs with the  This resulted in a training set of `r length(unique(mono$word))` words. In order to avoid spurious effects of length (either too short or too long), words ranging from three to seven phonemes were included in the training set. This included phonologically short words like `r scaps('and')` and `r scaps('through')`, and longer words like `r scaps('strength')` and `r scaps('sprints')`.


## Results
In order to link model behavior to that of humans, a standard approach was taken to establish the plausibility of the computation model here as a cognitive model. Known effects of the structure of words on learning and performance are demonstrated, and comparison findings from other computational implementations and behavioral experiments are provided. Those that pertain to monosyllabic word naming are included in this section on monosyllabic words, and a subsequent section deals with words that are greater than one syllable, with effects unique to polysyllabic words included there.

## Basic characterization of model behavior
A base model was developed in order to determine whether or not, like human readers, the model was able to produce well-formed phonological output from orthography after learning the mappings between the two for a large set of monosyllabic words. The model for monosyllabic words was trained to a high level of performance on all the training items and tested on a range of words to inspect the production of the model for trained and untrained words. 7% of words were held out for the purposes of generalization testing at the end of training. 

## Frequency
More experience with a word leads to more robust representation of the word, which in turn enhances the ability to act on it in some form. The most common proxy for experience in the reading literature is the frequency of a word's occurrence in a set of texts or other corpus. Connectionist models of reading aloud have established the effect of experience in a number of studies, where experience is implemented by proxy as a frequency-influenced procedure during learning (XX).

Word frequencies were used in order to scale gradients during training for the time-varying model reported on here. One issue is whether the LSTM architecture is sensitive to experience given this implementation, and whether or not this sensitivity resembles that of other computational architectures and human behavioral data. In other work this was done through constructing and comparing a model "latency" of some kind to latency data in humans or other models. For example, in @Sibley2010 "latency" was defined using an accuracy metric, and in @Seidenberg1989 this was the sum of squared errors. Rather than assuming the link between latency and accuracy here, I will call examine performance in these analyses by referencing accuracy directly. Of course, the assumption is that higher accuracy (lower error) is associated with faster responses in naming and other tasks, thoough this assumption doesn't play out directly in the analyses reported here, in part because the concept of "latency" in long short term memory units isn't well established but is likely to be an area of future research, especially related to models of human cognition and performance.

Early increases in word frequency are associated with lower errors, as shown in Figure XX. This association is lower in both computational models than in the naming accuracy data for the training words from @Balota2007, but in all three we see a relationship between frequency and errors. This comparison demonstrates that by including frequency as an aspect of training we see an impact on performance, and that the effect is present in both behavioral data and computational implementations.

```{r, freqeffectHumanComp, echo=FALSE, fig.cap="The relationship between word frequency (scaled) and word-level processing difficulty is shown for human behavioral data (panel A shows naming accuracy and panel B shows naming latency), a feedforward network architecture (C), and the time-varying/ LSTM architecture (D). For computational models (C-D) error is calculated as mean squared error. In all cases, an increase in frequency is associated with an increase in processing difficulty. Points are individual words that participated in training. In the case of human accuracy (A) errors are computed as accuracy of naming the word in a megastudy of word reading [@Balota2007] where the accuracy value of a word has the sign reversed. Human RTs are msecs standardized. Model errors are mean squared error, calculated unitwise on the phonological output layer of the network (over differences between the produced output for a unit and the target output). Error metrics are standardized across the three panels so that points (words) could share a common y-axis."}

STAGE = 'Middle'

elp_words = mono %>% 
  select(word, elp_acc) %>% 
  filter(!is.na(elp_acc)) %>% 
  pull(word) %>% 
  unique()


 
descriptives = mono %>%
  filter(train_test == 'train') %>% 
  filter(stage == STAGE) %>% 
  group_by(model, stage) %>% 
  summarise(MEAN = mean(accuracy),
            SD = sd(accuracy))


COLORS = c('LSTM' = 'firebrick', 'Feedforward' = 'goldenrod', '(D) Time-varying' = 'firebrick', 'Time-varying' = 'firebrick',  '(C) Feedforward' = 'goldenrod', 'Human - Accuracy' = 'Black', 'Human - RT' = 'Grey20', '(A) Human - Naming Accuracy' = 'Black', '(B) Human - Naming RT' = 'Grey20')

descriptives_elp_acc = mono %>% 
  filter(word %in% elp_words) %>% 
  filter(stage == STAGE & model == 'LSTM') %>% #arbitrary because values repeat
  filter(train_test == 'train') %>% 
  summarise(accMEAN = mean(elp_acc),
            accSD = sd(elp_acc),
            rtMEAN = mean(elp_rt),
            rtSD = sd(elp_rt)) %>% 
  mutate(model = '(A) Human - Naming Accuracy')

d_elp_acc = mono %>% 
  filter(word %in% elp_words) %>% 
  filter(stage == STAGE & model == 'LSTM') %>% #arbitrary because values repeat
  filter(train_test == 'train') %>% 
  mutate(model = '(A) Human - Naming Accuracy') %>% 
  left_join(descriptives_elp_acc) %>% 
  mutate(acc = -(elp_acc-accMEAN)/accSD) %>% 
  select(word, model, acc, freq, consistency)


descriptives_elp_rt = mono %>% 
  filter(word %in% elp_words) %>% 
  filter(stage == STAGE & model == 'LSTM') %>% #arbitrary because values repeat
  filter(train_test == 'train') %>% 
  summarise(accMEAN = mean(elp_acc),
            accSD = sd(elp_acc),
            rtMEAN = mean(elp_rt),
            rtSD = sd(elp_rt)) %>% 
  mutate(model = '(B) Human - Naming RT')

d_elp_rt = mono %>% 
  filter(word %in% elp_words) %>% 
  filter(stage == STAGE & model == 'LSTM') %>% #arbitrary because values repeat
  filter(train_test == 'train') %>% 
  mutate(model = '(B) Human - Naming RT') %>% 
  left_join(descriptives_elp_rt) %>% 
  mutate(acc = (elp_rt-rtMEAN)/rtSD) %>% 
  select(word, model, acc, freq, consistency)



mono %>% 
  filter(stage == STAGE & train_test == 'train') %>% 
  left_join(descriptives, by = c('model')) %>% 
  mutate(acc = (accuracy-MEAN)/SD) %>% 
  filter(word %in% elp_words) %>% 
  select(word, model, acc, freq) %>% 
  rbind(select(d_elp_acc, -consistency)) %>%
  rbind(select(d_elp_rt, -consistency)) %>% 
  mutate(model = case_when(model == 'LSTM' ~ '(D) Time-varying',
                           model == 'Feedforward' ~ '(C) Feedforward',
                           TRUE ~ model)) %>% 
  ggplot(aes(log(freq), acc, color = model)) +
  geom_point(size = .2) +
  geom_smooth(method = 'lm', color = 'grey32', span = 1/3) +
  scale_color_manual(values = COLORS) +
  facet_grid(~model) +
  labs(x = 'Frequency (log)', y = 'Processing difficulty') +
  theme_bw() +
  theme(legend.position = 'none') +
  ylim(c(-1, 5))
```

Furthermore, the effect of frequency fades as learning progresses; by the end of training this effect is attenuated substantially such that accuracy is consistently high among most words. With enough experience, challenges (errors) associated with processing are minimized due to familiarity. In table XX we see this in the relationship between raw error (as mean square error) plotted at two points in training: once early on (at 27 epochs) and again late in training (at 63 epochs). Errors become low even for the least frequent words.

```{r, freqeffectEarlyLate, echo=FALSE, fig.cap= 'The effect of word frequency early and late in training on the time-varying network differs such that with ample experience errors on infrequent words are minimized and the overall effect of frequency on error is reduced. Raw errors are plotted for words (points) using mean squared error.' }

mono %>% 
  filter(stage == c('Middle', 'Late')) %>% 
  filter(model == 'LSTM') %>% 
  filter(train_test == 'train') %>% 
  mutate(trainstage = case_when(stage == 'Middle' ~ 'Early in training',
                                stage == 'Late' ~ 'Late in training')) %>% 
  ggplot(aes(freq_scaled, accuracy)) +
  geom_point(size = .5, color = 'firebrick') +
  geom_smooth(method = 'lm', color = 'grey32') +
  scale_color_manual(values = COLORS) +
  facet_grid(~trainstage) +
  labs(x = 'Frequency (scaled)', y = 'Mean Squared Error') +
  theme_bw() +
  theme(legend.position = 'none')
```


## Consistency
Another important aspect of learning to read and reading behavior concerns how structural complexity affects reading performance. Readers are sensitive to the degree of coherent covariation among the mappings between printed and spoken words; the less covariation that exists between letters and sounds, the slower and more error prone performance is. Debates about how readers deal with this complexity and the role of experience in development and performance have been important for understanding the cognitive system that underlies reading development and performance, with different theoretical frameworks approaching the computations (and associated measurement) in different ways.

Parallel distributed processing theories of reading and lexical processing, like the model reported in @Seidenberg1989 and extended in @Plaut1996, hold that complexity leads to more burdensome processing (i.e., longer naming latencies), which can be overcome with experience. Networks that implement the theory encode information about the mappings between perceptual modalities in neuron-like processing units, with a single architecturally homogenous process accounting for words learned and read. In a model that learns to map print to speech over the course of experience (perception, action) these units (weights) encode the relationships between printed and spoken words with increasing robustness. This robustness is affected by the amount of experience the reader (or computational model) has in learning the words during training and the reliability with which the mappings across modalities occur across those words.

In connectionist models of reading, the complexity of the mappings between print and speech is described in terms of _consistency_: the extent to which the pronunciation of a word is similar other similarly spelled words. Defining consistency in a measurable way is important for understanding the behavior of the network. The approach here is similar to @Plaut1996, where the consistency of a word is defined by the orthographic body with respect to the phonological rime. Here _body_ refers to the portion of the word consisting of the first orthographic vowel and the orthographic structure that follows it (e.g., the _umbling_ in _fumbling_), and rime refers to the first vowel phoneme and everything that follows it (i.e, `AH1 - M - B - L - IH0 - NG` portion of the spoken form). A highly consistent word is one where words with this same body also have a similar pronunciation for this portion of the syllabic structure. We use a proportion for our purposes here, using the total number of words with a given body as the denominator, with the numerator as the number of words that share the rime present in that word. The result is that highly consistent words have values that tend towards one (i.e., all words with a given body share the rime) and inconsistent words have a value that tend towards zero.

The time-varying network shows a similar effect of consistency on accuracy as both the traditional feedforward networks of word reading as well as human naming data. As consistency increases, we see lower error scores. Consistent words are, in general, easier to read than less consistent words, and this effect is present across both human (left pane) and model data (middle and right panes).

```{r, consistencyHumanComp, echo=FALSE, fig.cap='The relationship between monosyllabic word consistency and error is shown for computational models (feedforward and time-varying) and human naming tasks. In all three there is a relationship between consistency and error such that increases in consistency (across words) are associated with lower error on average. Here errors for human and model data are processed as previously described, standardizing so that data can share the same y-axis.'}

COLORS = c('(C) Time-varying' = 'firebrick', '(B) Feedforward' = 'goldenrod', '(A) Human - Naming Accuracy' = 'Black')


mono %>% 
  filter(stage == STAGE & train_test == 'train') %>% 
  left_join(descriptives, by = 'model') %>% 
  mutate(acc = (accuracy-MEAN)/SD) %>% 
  filter(word %in% elp_words) %>% 
  select(word, model, acc, consistency) %>% 
  rbind(select(d_elp_acc, -freq)) %>%
  mutate(model = case_when(model == 'LSTM' ~ '(C) Time-varying',
                           model == 'Feedforward' ~ '(B) Feedforward',
                           TRUE ~ model)) %>% 
  ggplot(aes(consistency, acc, color = model)) +
  geom_point(size = .6) +
  geom_smooth(method = 'lm', color = 'grey32') +
  scale_color_manual(values = COLORS) +
  facet_grid(~model) +
  labs(x = 'Consistency', y = 'Error') +
  theme_bw() +
  theme(legend.position = 'none')

```

Like the effect of word frequency on accuracy, the effect of consistency diminishes over time. Processing difficulty associated with the consistency of an item can be overcome by having enough experience with the item. This can be seen when examining the relationship between consistency and error at two stages of the learning process. Table XX shows the relationship between consistency and error early in training and late in training, with the correlation decreasing over time as the learner develops experience with words in the training set.

```{r, consistencyEarlyLate, echo=FALSE, fig.cap='The relationship between word consistency and error at two different points in training is shown. With experience, the effect of consistency on errors is reduced.'}

mono %>% 
  filter(stage %in% c('Middle', 'Late') & model == 'LSTM') %>%
  filter(train_test == 'train') %>% 
  mutate(trainstage = case_when(stage == 'Middle' ~ 'Early in Training',
                                stage == 'Late' ~ 'Late in Training')) %>% 
  ggplot(aes(consistency, accuracy)) +
  geom_point(size = .6, color = 'firebrick') +
  geom_smooth(method = 'lm', color = 'grey32') +
  scale_color_manual(values = COLORS) +
  facet_grid(~trainstage) +
  labs(x = 'Consistency (body-rime)', y = 'Mean Squared Error') +
  theme_bw() +
  theme(legend.position = 'none')

```

## Consistency effects in generalization
These effects can also be seen in errors collected using the offline generalization mode of the network. The network was tested on a set of words held out during training (test items) using the offline method. The results are similar to those of the other accuracy tests reported earlier. The higher the consistency of a given word is, the less error you see on average at the time of test. Generalization results produced from the offline testing method are important because they demonstrate the capacity of the network in absence of phonological input at the time of test. These test trials are completed by providing the orthographic state (cell and hidden) to the phonological portion of the network along with the word-initial segment. A phonological wordform is then produced, one segment at a time. Those segments (vectors) are then combined and compared against the corresponding vector for the entire target word ^[Both the produced vector and the target vectors are all zero padded on the right side for this calculation to account for the fact that it is possible to produce a sequence of phonemes that is a different length than the target sequence. This padding does not affect the calculation of L2 distance, it only allows the calculation for vectors that are not of equal length]. 

```{r, consistencyGeneralization, echo=FALSE, fig.cap='The relationship between (body-rime) consistency and errors in offline generalization mode are shown, with the left panel showing errors for held out test items and the right panel showing errors on training items. Here error is calculated as the distance between the produced output and the target output for the entire word, a measure that is mathematically different from but very highly correlated with mean square error and other standard error metrics associated with these networks.'}
mono_lstm_testmode %>% 
  #filter(train_test == 'test') %>% 
#  filter(consistency < 1) %>% 
  mutate(train_test = case_when(train_test == 'train' ~ 'Training items',
                                train_test == 'test' ~ 'Test items')) %>% 
  ggplot(aes(consistency, wordwise_dist)) +
  facet_grid(~train_test) +
  geom_point(color = 'firebrick') +
  geom_smooth(method = 'lm', color = 'grey32') +
  labs(x = 'Consistency (body-rime)',
       y = 'Error')

```


## The interaction between frequency and consistency
The consistency of a word and the frequency with which it appears during learning have interrelated -- and similar -- effects on learning (and accuracy). Consistency concerns the extent to which a word's structure is shared with other words in the learning environment. Here this is defined in terms of the orthographic body and phonological rime, but we should consider structure more generally outside of this operationalization of the concept. More consistent words will contribute in a more substantial way to the weighted connections between units in the network by virtue of the learning rule used in the network. A two words that share neighborhood structure (similar pronunciations and similar spellings) will mutually benefit from the weight optimization that happens throughout learning. For example, take the word `r scaps('feed')`. This word is highly consistent because all other words in the training corpus that end in `r scaps('-eed')` (e.g., `r scaps('tweed')`, `r scaps('speed')`, `r scaps('seed')`, etc.) also have the rime `r scaps('...iy-d')`. Therefore learning `r scaps('feed')` benefits knowledge related to the other words in its neighborhood (like `r scaps('tweed')`).

This also applies to other aspects of structure, including structure possessed by words that are greater than one syllable, even though single syllable words are the focus here. For example, the words `r scaps('print')`, `r scaps('sprint')`, and `r scaps('prim')` will benefit from weight updates that result from learning the word `r scaps('prince')` even though this neighborhood of words is defined in a different way than how consistency was defined for the analyses here. Nonetheless, there is reason to believe that it is beneficial to define neighborhoods of monosyllabic words in terms of body-rime units rather than some other portion of syllabic structure due to the reliability of this structure relative to alternatives. For example, @Treiman1995 demonstrated that rime structure is more stable for short monosyllabic words, and that this stability is associated with more explained variance in naming latencies and errors than other syllabic structure.


```{r, frequencyByConsistency, echo=FALSE, fig.cap='The relationship between word consistency and errors at two levels of word frequency. Words were binned for frequency based on a mean-split only for purposes of visualization. Early in training the effect of consistency is most prevalent in low frequency words, and this effect is reduced late in training. Also, the overall effect of consistency is attenuated by the end of training, and the effect of frequency becomes less differentiated across levels of frequency.'}
descriptives = mono %>% 
  filter(stage %in% c('Middle', 'Late') & model == 'LSTM') %>% 
  group_by(stage) %>% 
  summarise(M = mean(freq_scaled))

mono %>% 
  filter(stage %in% c('Middle', 'Late') & model == 'LSTM') %>%
  filter(train_test == 'train') %>% 
  mutate(trainstage = case_when(stage == 'Middle' ~ 'Early in Training',
                                stage == 'Late' ~ 'Late in Training')) %>% 
  left_join(descriptives) %>% 
  mutate(freq_f = case_when(freq_scaled <= M ~ 'Low Frequency',
                            freq_scaled > M ~ 'High Frequency')) %>% 
  ggplot(aes(consistency, accuracy)) +
  facet_grid(vars(trainstage), vars(freq_f)) +
  geom_point(size = .6, color = 'firebrick') +
  geom_smooth(method = 'lm', color = 'grey32') +
  labs(x = 'Consistency (body-rime)', y = 'Mean Squared Error') +
  theme_bw()


```

These differences in performance between learning stage, frequency, and consistency are seen more precisely in a statistical model of error. Mean squared error was regressed on the interaction of learning stage, frequency, and consistency (all centered on the mean), yielding significant effects for all predictors and interactions, shown in Table XX.


```{r, stageFrequencyConsistencyModel, echo=FALSE}
descriptives = mono %>% 
  filter(stage %in% c('Middle', 'Late') & model == 'LSTM') %>%
  filter(train_test == 'train') %>% 
  group_by(stage) %>% 
  summarise(frequency_m = mean(freq_scaled),
            consistency_m = mean(consistency)) %>% 
  mutate(stage_f = case_when(stage == 'Middle' ~ -.5,
                             stage == 'Late' ~ .5))

mono %>% 
  filter(stage %in% c('Middle', 'Late') & model == 'LSTM') %>%
  filter(train_test == 'train') %>% 
  left_join(descriptives) %>% 
  mutate(frequency_c = freq_scaled-frequency_m,
         consistency_c = consistency-consistency_m) %>% 
  stats::lm(accuracy ~ stage_f*consistency_c*frequency_c, data = .) -> model

modelCI = confint(model)
modeldata = summary(model)

tabledata = data.frame(modeldata$coefficients) %>% 
  mutate(Estimate = sig_level(Estimate, Pr...t..),
         Pr...t.. = presentp(Pr...t..))

tabledata$CI = squarebracket(modelCI[,1], modelCI[,2], digits = 3)

tabledata$Predictor = c('Intercept', 'Learning stage', 'Consistency', 'Frequency', 'Learn stg x consist', 'Learn stg x freq', 'Consistency x frequency', 'Learn stg x consist x freq')

rownames(tabledata) = NULL
```


  
  
\newpage

```{r, stageFrequencyConsistencyModelRender, echo=FALSE} 

HEADER = c('*b*', '*SE*', '*t*', '*p*', '95% CI', 'Predictor')
names(tabledata) = HEADER

tabledata %>% 
  select(Predictor, everything()) %>% 
  apa_table(digits = 3, format = 'pandoc', align = 'l', landscape = T,
            note = 'Model estimated using lm() in R with confidence intervals estimated using confint(), both methods from the native R stats package [@R2021]. Bold parameter estimates indicate significant *p*-values below the alpha threshold of .05.',
            caption = 'Regression Model of the Interaction Between Learning Stage, Word (Body-Rime) Consistency, and Word Frequency on Mean Squared Error')


```


## Graded effects of consistency and frequency: replicating previous experimental findings
An additional simulation was conducted in order to replicate the graded effects of consistency and the interaction with frequency. This section reports data with the real words from the @Taraban1987 and @Jared1997 word lists included during training, both of which investigated these phenomena in monosyllabic words. These additional simulations were very similar to those reported in the previous sections but with the training environments including all the words from @Taraban19876 and @Jared1997, with the exception of words that were too short phonologically (less than two phonemes). This exclusion was made due to the relative few very short words in the training corpus and the possible idiosyncrasies that such words have in time-varying models of phonological processing, rather than deeper theoretical reasons. In the corpus of monosyllabic words assembled, only 19 eligible words were two phonemes or fewer, and were thus excluded (just as words that were one letter or fewer were excluded). The model reported on here is entirely capable of learning and processing such words successfully.

These experimental lists were designed in order to systematically vary the level of orthography-to-phonology consistency but using a set of categorical distinctions across consistency in order to investigate the trade-offs on reading performance as a function of consistency and whether or not the word can be produced "by rule" (i.e., if it is _regular_ or _irregular_). This distinction is important because connectionist networks show the ability to generate phonology from orthography despite the fact that such systems do not operate by way of rules that map orthography to phonology, as computational models using dual-route mechanisms do.

If the time-varying model proposed here operates in a similar fashion to the connectionist architectures developed using feedforward architectures then results should replicate for previous experiments with such architectures. Specifically, the graded effects of consistency on error (or loss) should be observed, in addition to their interaction with frequency.

In Figure X you we see the cross entropy (calculated with the loss function used) for each of the four regularity conditions from @Taraban1987 and in the high and low frequency conditions taken from performance data early on in training (27 epochs). The trend observed closely resembles that of the computational results from @Plaut1997 and follows the expected pattern based on graded effects of consistency as they interact with frequency.

```{r}

# shapes:
# 0 = ambiguous (square)
# 1 = exception (circle)
# 2 = regular consistent (triangle)
# 5 = regular inconsistent (diamond)


taraban %>% 
  filter(!is.na(freq_taraban)) %>% 
  filter(epoch == 27) %>% 
  filter(taraban != 'nonword') %>%
  mutate(taraban = case_when(taraban == 'ambiguous' ~ 'Ambiguous',
                             taraban == 'exception' ~ 'Exception',
                             taraban == 'reg_consistent' ~ 'Regular consistent',
                             taraban == 'reg_inconsistent' ~ 'Regular inconsistent'),
         freq_taraban = case_when(freq_taraban == 'low' ~ 'Low',
                                  freq_taraban =='high' ~ 'High'),
         freq_taraban = fct_relevel(freq_taraban, c('Low', 'High')),
         taraban = fct_relevel(taraban, c('Exception', 'Ambiguous', 'Regular inconsistent', 'Regular consistent'))) %>% 
  group_by(freq_taraban, taraban) %>% 
  summarise(loss = mean(loss)) %>% 
  ggplot(aes(freq_taraban, loss, shape = taraban, group = taraban)) +
  geom_line() +
  geom_point(size = 3) +
  scale_shape_manual(values = c(0, 1, 2, 5)) +
  labs(x = 'Frequency', y = 'Cross entropy') +
  theme_apa() +
  theme(legend.title = element_blank())
```

In order to avoid results that rely on a single, perhaps idiosyncratic simulation, the simulation was conducted 50 times over and the same trend results when examining performance relatively early in training. The figure below shows these results averaging across runs of the model at an early stage of learning (here epoch 27).

```{r}

taraban_crossval %>% 
  filter(!is.na(freq_taraban)) %>% 
  filter(epoch == 27) %>% 
  filter(taraban != 'nonword') %>%
  mutate(taraban = case_when(taraban == 'ambiguous' ~ 'Ambiguous',
                             taraban == 'exception' ~ 'Exception',
                             taraban == 'reg_consistent' ~ 'Regular consistent',
                             taraban == 'reg_inconsistent' ~ 'Regular inconsistent'),
         freq_taraban = case_when(freq_taraban == 'low' ~ 'Low',
                                  freq_taraban =='high' ~ 'High'),
         freq_taraban = fct_relevel(freq_taraban, c('Low', 'High')),
         taraban = fct_relevel(taraban, c('Exception', 'Ambiguous', 'Regular inconsistent', 'Regular consistent'))) %>% 
  group_by(freq_taraban, taraban) %>% 
  summarise(loss = mean(loss)) %>% 
  ggplot(aes(freq_taraban, loss, shape = taraban, group = taraban)) +
  geom_line() +
  geom_point(size = 3) +
  scale_shape_manual(values = c(0, 1, 2, 5)) +
  labs(x = 'Frequency', y = 'Cross entropy') +
  theme_apa() +
  theme(legend.title = element_blank())

```



```{r, echo=FALSE}
taraban_crossval %>% 
  filter(!is.na(freq_taraban)) %>% 
  filter(epoch == 27) %>% 
  filter(taraban != 'nonword') %>%
  mutate(taraban = case_when(taraban == 'ambiguous' ~ 'Ambiguous',
                             taraban == 'exception' ~ 'Exception',
                             taraban == 'reg_consistent' ~ 'Regular consistent',
                             taraban == 'reg_inconsistent' ~ 'Regular inconsistent'),
         freq_taraban = case_when(freq_taraban == 'low' ~ -.5,
                                  freq_taraban =='high' ~ .5),
         taraban = fct_relevel(taraban, c('Exception', 'Ambiguous', 'Regular inconsistent', 'Regular consistent')),
         taraban_num = as.numeric(taraban)) %>%
  mutate(taraban_c = taraban_num-mean(c(1, 2, 3, 4))) %>% 
  lmer(loss ~ taraban_c*freq_taraban + (1|word) + (1|run_id), data = .) -> m_plaut1

save(m_plaut1, file = 'data/m_plaut1.Rda')

load(file = 'data/m_plaut1.Rda')



# recompute confints:
#m_plaut1_confint = confint(m_plaut1)
#m_plaut1_confint %>% 
#  as.data.frame() %>% 
#  rownames_to_column(var = 'predictor') %>% 
#  write_csv('data/m_plaut1_confint.csv')
summary(m_plaut1)


m_plaut1_confint = read_csv('data/m_plaut1_confint.csv')

#m_plaut1_summary = summary(m_plaut1)
#save(m_plaut1_summary, file = 'data/m_plaut1_summary.Rda')
load(file = 'data/m_plaut1_summary.Rda')


m_plaut1_confint

# runtime is about 25 minutes:
#m_plaut1_anova = car::Anova(m_plaut1, type = 3, test = 'F')
# note that the anova table reflects the following usage:
# Analysis of Deviance Table (Type III Wald F tests with Kenward-Roger df)
#write_csv(m_plaut1_anova, 'data/m_plaut1_anova.csv')
#m_plaut1_anova = read_csv('data/m_plaut1_anova.csv')
m_plaut1_anova

```


Furthermore the effect is similar though not identical to those observed in @Plaut1997, as demonstrated in a statistical model of consistency condition (expressed as a linearly decreasing set of conditions in the following order: exception, ambiguous, regular inconsistent, and regular consistent) with frequency (high versus low). Using a linear mixed effects model estimated in R [@R2021] in the lme4 package [@Bates2015] with confidence intervals estimated in the stats library [@R2021] and p-values computed using the Anova() method in the car package [@Fox2019]. This model includes by item random intercepts as well as random intercepts for each run (each simulation). Here an increase in condition is, on average, associated with a `.01` decrease in loss (*b* = -.01, *SE* = .003, 95%CI = [-.01, -.005]), with the effect of frequency associated with a decrease in loss of .01 units. The interaction, unlike what was reported in @Plaut1997, is not significant (*b* = .003, *SE* = .003, *F*(1, 172) = 1.55, *p* = .22, 95%CI = [-.002, .008]). This pattern resembles the effects described earlier looking at frequency and consistency as quantitative predictors (derived from the structure of words in the corpus directly) but differs in the specification of the interaction (because they are defined across conditions rather than as observed). Words of increasing consistency demonstrate lower levels of error, and high frequency words are less error prone than lower frequency words.

```{r}


taraban_crossval %>% 
  filter(!is.na(freq_taraban)) %>% 
  filter(epoch == 27) %>% 
  filter(taraban != 'nonword') %>%
  mutate(taraban_c = case_when(taraban == 'exception' ~ -.5,
                               taraban == 'reg_inconsistent' ~ .5,
                             taraban == 'reg_consistent' ~ .5),
         freq_taraban = case_when(freq_taraban == 'low' ~ -.5,
                                  freq_taraban =='high' ~ .5)) %>%
  lmer(loss ~ taraban_c*freq_taraban + (1|word) + (1|run_id), data = .) -> m_plaut2

summary(m_plaut2)

# runtime is about 25 minutes:
m_plaut2_anova = car::Anova(m_plaut2, type = 3, test = 'F')
save(m_plaut2_anova, file = 'data/m_plaut2_anova.Rmd')

load(file = 'data/m_plaut2_anova.Rmd')

```
@Plaut1997 included a number of statistical tests that aren't central to the demonstration here. One is worth observing both because it adds additional precision to the previous results and is important to note for theoretical reasons. In a model of the interaction between word regularity (regular versus exception - as was defined in @Plaut1997) amd frequency (low versus high), regular words are reliably produced with lower errors than exception words (*b* = -.02, *SE* = .003, *F*(1, 85) = 35.35, ). The effect of frequency maintains as in the previous model reported, as does the non-significance of the interaction term. This indicates that the LSTM network picks up more narrowly on the difference in complexity between words that can be characterized as regular (both consistent regulars and inconsistent regulars) and those that are exceptional (those that both can't be described via simple rules and those that have no neighbors).

The absence of a significant interaction between consistency and frequency here would require further experimentation, which would require analyses of loss and other relevant model behavior across training. @Plaut1997 included a range of experiments examining the variability in effects of including frequency manipulations of different kinds during training, showing that using condensed frequencies when training the network also condenses the effects related to those frequencies. It is quite possible that by expanding the frequency values used to scale gradients during training here would make the effects of frequency (whether raw or categorical) more dramatic. While this additional experimentation is outside the scope of the demonstration of the architecture here, it is worthy of future investigation in subsequent work.

In sum, the general trends relating consistency and frequency appear in data from the time-varying network architecture being proposed here. In statistical models of the experimental effects of words that vary systematically in terms of frequency and consistency, the results are mixed despite the appearance of very similar trends across the two variables from a descriptive account.

## Nonword reading
Looking at performance at the end of training, the network did quite well in pronouncing nonwords (taken from @Taraban1987; though see the footnote on @Plaut1997 p.62 for a discussion of the shortcomings of these stimuli). This is taken as a good measure of performance of models in the history of such work due to the fact that the words (should) bear a meaningful resemblance to real words and that in connectionist networks, where there are no "stored" entries of words, the ability to perform on such words evidences the ability of a rule-less system to generate reasonable pronunciations due to generalization from other similar forms.

```{r, taraban_nonwords, echo=FALSE}
taraban_testmode %>% 
  filter(taraban == 'nonword') %>%
  filter(phonemes_proportion < 1) %>% 
  select(word, phon_read)  %>% 
  filter(word != 'boad') %>% 
  pull(word) -> nonwords_pronounced_wrong

nonwords = taraban_testmode %>% 
  filter(word %in% nonwords_pronounced_wrong) %>% 
  select(Nonword = word,
         Correct = phon,
         Response = phon_read) 


apa_table(nonwords)

  
```

Of the `r length(nonwords)` from the @Taraban1987 set, `r length(nonwords_pronounced_wrong)` were pronounced incorrectly (i.e., produced something other than the target sequence of phonemes). This amounts to `r length(nonwords)/length(nonwords_pronounced_wrong)`% of words being pronounced incorrectly from this list (compare to `r 13/43`% incorrect from a similar list in @Plaut1997, but using a much simpler computational system). Like the errors on nonwords in @Plaut1997, the errors here are all reasonable attempts at a sequence of phonemes given the input orthographic pattern. These errors are shown in Table X \footnote{Note that the phonological coding for the nonword `r scaps('boad')` was incorrect and has been removed from this table}.

While performance on made up words will be discussed further in the subsequent section devoted to producing multisyllabic words, we can say a few things here about the performance on this set of words in a model of monosyllabic words. These errors, like those in @Plaut1997 are realistic in the sense that the phones produced approximate what you would expect for the orthographic input, but don't perfectly match the true sequence. For example, vowels are produced in the vowel position and consonants in the consonant positions. Additionally, the incorrect vowels and consonants produced are quite close to the proper segments in terms of their phonological similarity to the target segment. The responses for `r scaps(nowl)` involved a vowel with a postvocalic glide (`r  scaps('OW1')`), but ot the correct one. Likewise, `r scaps('dase')` and `r scaps('boose')` were incorrect because they produced incorrect voicing on the word-final fricative (either `r scaps('Z')` for `r scaps('S')` or vice versa).

All of the mistakes can be tied to the neighborhood characteristics of the words as well given that all nonwords here contain ambiguous orthography-to-phonology structure (i.e., their consistency is less than `1`). In this sense the responses can be seen as overgeneralization based on similar forms. Of the mistakes here, only `r scaps('dase')` and `r scaps('foth')` are surprising given the raw consistency counts (as body-rime calculations) of the neighborhoods involved. Most of the orthographic neighbors of `r scaps('dase')` are not associated with the rime produced here. `.67` of words with `r scaps('ase')` are associated with the rime `r scaps('EY - Z')`. Likewise, most of `r scaps('foth')`'s orthographic neighbors exhibit the lower unrounded vowel `r scaps('AA')` rather than the vowel produced by mistake (`r scaps('OW')`). This last error can be seen as an effect of an important aspect of the system, though, given that `r scaps('foth')` has a influential high frequency neighbor `r scaps('both')`.

Nonetheless, and as consistent with the discussion in @Plaut1997, both humans and computational models produce "incorrect" phonological codes from orthography - though the notion of "correct" pronunciation for novel strings is not straightforward. Behavioral data have shown that reader's responses to nonwords often deviate from the expected production, notwithstanding questions about the proper production for a nonword is. For example, @Glushko1979 found that nonwords with inconsistent neighborhood structure often resulted in pronunciations that were different than the expected pronunciation based on grapheme-phoneme rules while being consistent with the pronunciation for that orthographic body in some other word (see @Plaut1996 for discussion). The issue of nonword naming will be returned to later in the discussion of a model that learns multisyllabic words.

# Jared (1997)
Behavioral results conveying the tradeoffs between frequency and consistency hold up for the @Jared1997 stimuli as well, with low frequency words in general being more difficult to process than high frequency words, and with inconsistent words being more difficult than consistent ones. We focus on experiment 1 in @Jared1997. In that experiment the task was simple: participants read aloud sets of words (40 words to a list over two separate visits to the lab) that belonged to one of four categories. Words were either low frequency or high frequency and were consistent (all neighbors had the same rime) or inconsistent (neighbors displayed different rimes). For example, the words `r scaps('paid')`, `r scaps('said')`, and `r scaps('plaid')` are inconsistent. These neighborhood characteristics were tested in the corpus of words used for these simulations and the characteristics were confirmed. Additionally, frequencies were set to high and low prior to training given the possibility that the frequencies used for gradient scaling differed from those consulted for the experiments in @Jared1997.

```{r echo=FALSE}

jaredA_freq = taraban_crossval %>% 
  filter(epoch == 36) %>% 
  filter(!is.na(jaredA)) %>% 
  group_by(freq_jaredA) %>% 
  summarise(mse = mean(mse)) %>% 
  rename(condition = freq_jaredA)


jaredA_consistency = taraban_crossval %>% 
  filter(epoch == 36) %>% 
  filter(!is.na(jaredA)) %>% 
  group_by(jaredA) %>% 
  summarise(mse = mean(mse)) %>% 
  rename(condition = jaredA)

```

As in the behavioral data in the original paper, the model data support the tradeoffs between consistency and frequency. Taking data from 50 runs of the paradigm and using mean squared error as the metric (again early in training), low frequency words (mse = `r pull(filter(jaredA_freq, condition == 'low'), mse)`) are produced with greater error than high frequency words (mse = `r pull(filter(jaredA_freq, condition == 'high'), mse)`), and inconsistent words are higher in mse (`r pull(filter(jaredA_consistency, condition == 'inconsistent'), mse)`) than their consistent counterparts (`r pull(filter(jaredA_consistency, condition == 'consistent'), mse)`).

Here is that trend early on in training at 36 epochs, with the comparable figure reproduced from @Jared1997.

```{r}
COLORS_jared = c('Consistent'='grey86', 'Inconsistent'='black')

taraban_crossval %>% 
  filter(epoch == 36) %>% 
  filter(!is.na(jaredA)) %>%
  mutate(freq = case_when(freq_jaredA == 'high' ~ 'High',
                          freq_jaredA == 'low' ~ 'Low'),
         condition = case_when(jaredA == 'consistent' ~ 'Consistent',
                               jaredA == 'inconsistent' ~ 'Inconsistent'),
         condition = fct_relevel(condition, c('Inconsistent', 'Consistent'))) %>% 
  ggplot(aes(freq, loss, fill=condition)) +
  geom_bar(stat= 'summary', position = position_dodge(), color = 'black') +
  scale_fill_manual(values = COLORS_jared) +
  theme_bw() +
  labs(x = 'Frequency', y = 'Mean Squared Error') +
  theme(legend.title = element_blank())

```

```{r, echo=FALSE}
taraban_crossval %>% 
  filter(epoch == 36) %>% 
  filter(!is.na(jaredA)) %>% 
  mutate(frequency = case_when(freq_jaredA == 'high' ~ .5, freq_jaredA == 'low' ~ -.5),
         consistency = case_when(jaredA == 'consistent' ~ .5, jaredA == 'inconsistent' ~ -.5)) %>% 
  lmer(mse ~ frequency*consistency + (1|word) + (1|run_id), data = .)  -> m_jared

save(m_jared, file = 'data/m_jared.Rda')

m_jared_anova = car::Anova(m_jared, type=3, test="F")
save(m_jared_anova, file = 'data/m_jared_anova.Rmd')

m_jared_confint = confint(m_jared)
save(m_jared_confint, file = 'data/m_jared_confint.Rda')

```



```{r, echo=FALSE}
taraban_crossval %>% 
  filter(epoch == 36) %>% 
  filter(!is.na(jaredA)) %>% 
  mutate(frequency = case_when(freq_jaredA == 'high' ~ 0, freq_jaredA == 'low' ~ 1),
         consistency = case_when(jaredA == 'consistent' ~ -.5, jaredA == 'inconsistent' ~ .5)) %>% 
  lmer(mse ~ frequency*consistency + (1|word) + (1|run_id), data = .) -> m_jared2

save(m_jared2, file = 'data/m_jared2.Rda')

m_jared2_anova = car::Anova(m_jared2)
save(m_jared2_anova, file = 'data/m_jared2_anova.Rmd')

m_jared2_confint = confint(m_jared2)
save(m_jared2_confint, file = 'data/m_jared2_confint.Rda')

```

It should be noted that these effects are subjected to variability throughout training. For example, the plot below shows the interaction between consistency and frequency at several different training stages: 9, 18, 27, 36, 45, and 54, 63, and 72 epochs. The interaction looks stable across training for the most part (and is on average when collapsing across epochs), but at certain points in the development of expertise in the domain the effect may shift. For example, in epoch 54 error on consistent words is on average higher than on inconsistent words, though the trend for frequency at that point is similar to other epochs.

```{r, echo=FALSE}
COLORS_jared = c('Consistent'='grey86', 'Inconsistent'='black')


taraban_crossval %>% 
  filter(!is.na(jaredA)) %>%
  mutate(freq = case_when(freq_jaredA == 'high' ~ 'High',
                          freq_jaredA == 'low' ~ 'Low'),
         condition = case_when(jaredA == 'consistent' ~ 'Consistent',
                               jaredA == 'inconsistent' ~ 'Inconsistent'),
         condition = fct_relevel(condition, c('Inconsistent', 'Consistent'))) %>% 
  ggplot(aes(freq, mse, fill=condition)) +
  geom_bar(stat= 'summary', position = position_dodge(), color = 'black') +
  scale_fill_manual(values = COLORS_jared) +
  theme_bw() +
  labs(x = 'Frequency', y = 'Mean Squared Error') +
  theme(legend.title = element_blank()) +
  facet_grid(~epoch)

```

It is expected that the process of acquiring knowledge that is subject to constraints like those that operate on the stimuli here, you will see stochastic variability throughout learning, but with overall trends emerging when averaging across individuals or, in the case of the simulation here, across time.

As an alternative method of assessing model production, additional information about performance can be observed using the network's offline test mode. This method is dictated by a different sort of recurrent process that settles into a final state through and internally dictated mechanism rather than by the number of timesteps provided at the phonological input (see the previous section on the method for details). Summing the accuracy across phonological segments using this testing mechanism yields separate converging evidence about processing difficulty at a finer grain than that offered through mean squared error. Figure X shows the cumulative (sum) phoneme-wise distances for the @Jared1997 stimuli using production mode at the end of training, the trend of which is very similar to that seen in the previous analyses of these items.

```{r, echo=FALSE}


taraban_testmode %>% 
  filter(!is.na(jaredA)) %>% 
  mutate(freq = case_when(freq_jaredA == 'high' ~ 'High',
                          freq_jaredA == 'low' ~ 'Low'),
         condition = case_when(jaredA == 'consistent' ~ 'Consistent',
                               jaredA == 'inconsistent' ~ 'Inconsistent'),
         condition = fct_relevel(condition, c('Inconsistent', 'Consistent'))) %>% 
  ggplot(aes(freq, phonemes_sum, fill=condition)) +
  geom_bar(stat= 'summary', position = position_dodge(), color = 'black') +
  scale_fill_manual(values = COLORS_jared) +
  labs(x = 'Frequency', y = 'Sum of phoneme-wise distance') +
  theme_apa() +
  theme(legend.title = element_blank())

```



```{r, echo=FALSE}
taraban_crossval %>% 
  filter(!is.na(jaredA)) %>% 
  mutate(freq = case_when(freq_jaredA == 'high' ~ 'High',
                          freq_jaredA == 'low' ~ 'Low'),
         condition = case_when(jaredA == 'consistent' ~ 'Consistent',
                               jaredA == 'inconsistent' ~ 'Inconsistent'),
         condition = fct_relevel(condition, c('Inconsistent', 'Consistent'))) %>% 
  group_by(epoch, freq, condition) %>% 
  summarise(mse = mean(mse),
            freq = first(freq),
            condition = first(condition)) %>% 
  ggplot(aes(freq, mse, fill=condition)) +
  geom_bar(stat= 'summary', position = position_dodge(), color = 'black') +
  facet_grid(~epoch) +
  scale_fill_manual(values = COLORS_jared) +
  labs(x = 'Frequency', y = 'Mean squared error') +
  theme_apa() +
  theme(legend.title = element_blank())



```


```{r, echo=FALSE}
taraban_crossval %>% 
  filter(!is.na(freq_taraban)) %>% 
  #filter(epoch %nin% c(9, 18)) %>% 
  filter(epoch == 54) %>% 
  filter(taraban != 'nonword') %>%
  mutate(taraban = case_when(taraban == 'ambiguous' ~ 'Ambiguous',
                             taraban == 'exception' ~ 'Exception',
                             taraban == 'reg_consistent' ~ 'Regular consistent',
                             taraban == 'reg_inconsistent' ~ 'Regular inconsistent'),
         freq_taraban = case_when(freq_taraban == 'low' ~ 'Low',
                                  freq_taraban =='high' ~ 'High'),
         freq_taraban = fct_relevel(freq_taraban, c('Low', 'High')),
         taraban = fct_relevel(taraban, c('Exception', 'Ambiguous', 'Regular inconsistent', 'Regular consistent'))) %>% 
  group_by(epoch, freq_taraban, taraban) %>% 
  summarise(loss = mean(loss)) %>% 
  ggplot(aes(freq_taraban, loss, shape = taraban, group = taraban)) +
  geom_line() +
  geom_point(size = 3) +
  scale_shape_manual(values = c(0, 1, 2, 5)) +
  labs(x = 'Frequency', y = 'Cross entropy') +
  facet_grid(~epoch) +
  theme_apa() +
  theme(legend.title = element_blank())

```



## Number of syllables






## Common words are easier to read, and more accurate
Words that are more frequently encountered tend to be easier to read, resulting in general in faster reading times and fewer errors [@Balota2004; @Forster1973; @Stanovich1978]. The first analysis reported here concerns this effect. For this analysis a training process was completed where the frequency with which words were included in training was proportional to their text frequency. In order to determine how often a word should be selected for training, the following method was used. *DESCRIBE FREQUENCY CONDITIONED TRAINING PROCESS HERE*.


## Tradeoff between frequency and consistency





## Dispersion
Sequential approaches to orthography-to-phonology conversion solve this problem by processing discrete speech segments from discrete print segments in a left-to-right, segment-by-segment manner. These solutions always involve some form of justification (sometimes called "alignment" in the literature), though the alignment procedure differs across sequential implementations. For example, @Sejnowski1987 provided orthographic input strings as sequences of letters and mapped them to equal-length sequences of phonemes. This allowed their network to implement a kind of temporal processing over visual and its corresponding spoken pattern, but required architectural specifications that deviate from assumptions about processing in other connectionist networks. Their procedure required inserting null elements in slots where a phoneme wasn't present for the corresponding input letter (or grapheme) given that the input and output sequences were always equal length. So, for the input pattern `r scaps('late')` the corresponding output would be `L - EY - T - __`, with the final phoneme segment `__` being the null one given the word-final silent `r scaps('a')` in `r scaps('late')`. This avoids the type of dispersion found in feedforward networks, where knowledge about letters and their corresponding sounds is localized to weighted connections associated with specific slots given the way that input and output patterns are specified during learning, but introduces a different type of knowledge localization that is undesirable: that letters on the input layer become associated too narrowly with phonemic segments on the output given the segment-by-segment processing mechanism (i.e., they lose the context sensitive nature of processing due to the segment-by-segment way that print and speech are associated in the network).

Dual-route models use an assembly method, where a letter or letter segment (e.g., grapheme) is associated with a phonemic segment by assembly rule, where "assembly" in this literature refers to the association of phonological segments to its corresponding orthographic segment. Processing speech from print involves assembling the phonology from the pre-specified letter-sound rules, which are derived via an analytic technique that resides outside the scope of the computational architecture itself (i.e, specified by the experimenter). The assembly process happens alongside a lexical lookup procedure, which has its own separate timecourse. The relative timecourse of these processes determine whether a word is produced via assembly rules or as a structured lexical object. This is the extreme end of the spectrum of methods that result in knowledge dispersion. Here knowledge about the relationship between visual and spoken elements is dispersed across the symbolic rules that operate over the crossmodal assembly that takes place during learning and performance. This form of knowldge dispersion is different implementationally but related conceptually in that what is known about similar elements in the domain becomes dissociated based on the structure of the architecture, which of course has implications on the learning that takes place given that architecture.

Other approaches handle input strings with the use of slots, but in a different way than in @Sejnowski1987. Commonly in connectionist feedforward implementations input patterns are justified in a vowel-centered way (see @Plaut1996 for a discussion). A similar solution exists in hybrid connectionist/ dual route architectures like @Perry2010. Here the input patterns are fit into a template defined by structure in terms of orthography across syllables, which is specified by the limits of the structure of the training set. So, for example the template used for the two syllable template in @Perry2010 involves fitting orthographic inputs into a `CCCVCCCC` pattern on each of the two (possible) syllables via a "graphemic buffer". This is the mechanism that defines the "slots" on the visual input to which any given input pattern are aligned to the available phonemes.

In the present computational system there is no alignment, justification, or slots. However, an important question concerns whether or not knowledge is dispersed in a model specified in this way. The most direct comparison is to models that contain weighted connections between input and output units, and intermediary hidden ones too. The issue of dispersion is investigated in the present architecture in two ways. First, we present data about the extent to which identical input patterns in different (orthographic) contexts are associated with different units in the network and, second, the extent to which the overall pattern of representation of identical input patterns are similar to each other.

A further piece of supporting data concerns a manipulation to the masking mechanism used in the model. Masking introduces a kind of justification, but one where masked segments should never be realized in the knowledge of the learner.






## Benchmarks
*jared1990: latencies increase with number of syllables, but is moderated by frequency (Experiment)
*jared1990: exceptional and regular inconsistent words exhibit longer latencies as compared to regular words, but only for low frequency words (see yap2009, top right of p. 503 for discussion of the effect)
*position of irregularity effects


kawamoto1998 showed that words with less predictable pronunciations of the vowel exhibit longer latencies for the consonant in the onset, rather than simply longer overall naming latency of the entire word. This is due to the fact that the vowel pronunciations that are less predictable require more processing time, and this processing manifests in the system dwelling on pre-vocalic consonants, rather than being distributed throughout the naming process for the entire word. Their studies were performed using monosyllabic stimuli that were quite simple, though well established in the word reading literature related to naming and related phenomena.

## Canonical naming effects
### Frequency
### Regularity
### Frequency by Regularity

## Latency
Given that computational processing times on a GPU haven't been validated as an analogous method for naming latency in humans, similar to Sibley et al. (2010) a measure of model confidence for processing an orthographic word was designed to measure the latency for reading that word. 


## Overall results
*Provide an overall summary of results here before going into the piecemeal results. See Seidenberg & McClelland (1989) pages 531-532 for examples