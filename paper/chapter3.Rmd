---
title: "4. Behavioral experiments"
output: html_document
---

# Chapter 4: Experimentation
The purpose of the model developed here is to read printed words aloud. A number of results are presented based on results from behavioral research, features of the cognitive model the architecture represents, and comparisons to similarly oriented models of word reading from elsewhere in the literature. The purpose of this experimentation is twofold: to present results that convey important features of the architecture as it relates to cognition and development and to make comparisons to experimentation elsewhere of similar phenomena, whether based on a computational approach or behavioral one.

## Accuracy
Results of the model's ability to correctly produce a spoken form of a word can be based on a number of different measures arising from the same computational system. The most common measure of the model's ability is an accuracy score, which itself can be structured in a few different ways - similar to that of a human reader. Given that the goal of the model is to take a sequence of letters and recode them in the form of a sequence of speech sounds, individual sounds can be inspected for their accuracy along with the entire sequence of sounds - that is, the entire word. Both the phoneme-wise and the word-wise accuracy metrics are used in reporting, depending on the purpose of the result shown. They are of course correlated given that a word that is produced correctly is one in which all the phonemes are produced correctly (or at least each phoneme produced is closer to the correct phoneme than all other possible phonemes).

A limited corpus of words has been selected for training purposes here, though the model is capable of attempting to produce phonology from any sequence of letters provided on the input - a feature we see as a virtue of the implementation given that humans are also capable of this.

## Training details

## Testing trials
The model is tested in a way that follows a different procedure than that used in training, allowing for more flexible and deeper introspection about the dynamics at play in the system when operating. When a word is submitted for testing (or "production") an orthographic pattern is introduced to the the orthographic input - the model's visual input system. This pattern is structured just like that of the orthographic input during training: it is a two dimensional array with as many letter representations along the first axis as there are letters in the word. This time-varying input pattern is input to the orthographic input, and its representation decomposed into its two critical states for use in phonology: the cell state and the final hidden state. Once these vectors are achieved for the orthographic form of the word, that portion of the model rests and is unchanged for the remainder of the trial.

The states of the orthographic layer are passed to phonology to generate the phonological code output. This process is initiated by the input of the start of word segment (`#`), the representation of which was described earlier. The fact that the model has been trained with an explicit start of word marker enables the phonological layer to produce a next phoneme given the current state of the layer and its input pattern. At the start of production this context is provided completely from the orthographic layer. That is, the role of the phonological portion of the model is to produce a phonological code given the orthographic context.

Once the first phonemic pattern is produced it is saved for subsequent phonemes to be appended to it in order to form a full sequence of phonemes that constitutes a production. While the weights in the network are frozen at this stage prior to testing, the state of the phonological LSTM is updated at each timestep following the same processing flow as described in the architecture section. Once a phoneme is output at each timestep, it is fed back to the input of the phonological layer to support subsequent productions. This differs in process from training trials but is the same conceptually given that in training, the entire phonological sequence is processed one segment at a time without the feedback process from the output layer back to the input. Because the output for a given phoneme is never specified as a perfect veridical binary pattern, any phoneme output is compared to all possible phoneme outputs using the L2 norm and the nearest phoneme is selected and recorded as the produced phoneme. The production trial progresses until the phonological output layer produces the "stop producing" segment (`%`). At this point the word produced is recorded for analysis.

## Basic characterization of model behavior
A base model was developed in order to determine whether or not, like human readers, the model was able to produce well-formed phonological output from orthography after learning the mappings between the two. For this purpose a model was trained to a high level of performance on all the training items and tested on a range of words to inspect the production of the model for trained and untrained words. In order to expedite training, items were assembled into batches based on their phonological length. This is done given the way that weights are implemented in the python-based architecture used; variable length patterns can be trained in the same model as long as batch training occurs for items of the same length. This applies to batches containing only a single training item, where weights would be updated after a single trial. Larger batches are simply used for training efficiency. As an example, an alternative approach could be to train using batch size as one training example (i.e., stochastic gradient descent), which is very processing intensive and time consuming even on modern graphical processing hardware of the kind used for experimentation here. The definition of batches isn't intended to convey a hypothesis about how processing in humans works, rather is specified as it is for efficiency reasons.



## Consistency
One important aspect of reading behavior concerns how readers deal with structural complexity during naming, among other reading activities. Readers are sensitive to complexity in print, speech, and the mappings between them such that the more complex the form, the slower and more error prone the associated production is. Debates about how readers deal with this complexity and the role of experience in development and performance have been important for understanding the reading system, with different theoretical frameworks approaching the computations (and associated measurement) in different ways.

Parallel distributed processing theories of reading and lexical processing, like the distributed, developmental model developed in @Seidenberg1989 and extended in @Plaut1996, hold that complexity leads to more burdensome processing (i.e., longer naming latencies), which can be overcome with experience. Networks that implement the theory encode information about the mappings between perceptual modalities in neuron-like processing units. In a model that learns to map print to speech over the course of experience (perception, action) these units (weights) encode the relationships between printed and spoken words with increasing robustness. This robustness is affected by the amount of experience the reader (model) has in learning the words during training and the reliability with which the mappings across modalities occur across those words.

In this line of work the complexity of the mappings between print and speech is described in terms of _consistency_: the extent to which the pronunciation of a word is similar other similarly spelled words. Defining consistency in a measurable way is important for understanding the behavior of the network. The approach here is similar to @Plaut1996, where the consistency of a word is defined by the orthographic body with respect to the phonological rime. Here _body_ refers to the portion of the word consisting of the first orthographic vowel and the orthographic structure that follows it (e.g., the _umbling_ in _fumbling_), and rime refers to the first vowel phoneme and everything that follows it (i.e, `AH1 - M - B - L - IH0 - NG` portion of the spoken form). An highly consistent word is one where words with this same body also have a similar pronunciation for this portion of the syllabic structure. We use a proportion for our purposes here, using the total number of words with a given body as the denominator, with the numerator as the number of words that share the rime present in that word. The result is that highly consistent words have values that tend towards one (i.e., all words with a given body share the rime) and inconsistent words have a value that tend towards zero.




## Number of syllables






## Common words are easier to read, and more accurate
Words that are more frequently encountered tend to be easier to read, resulting in general in faster reading times and fewer errors [@Balota2004; @Forster1973; @Stanovich1978]. The first analysis reported here concerns this effect. For this analysis a training process was completed where the frequency with which words were included in training was proportional to their text frequency. In order to determine how often a word should be selected for training, the following method was used. *DESCRIBE FREQUENCY CONDITIONED TRAINING PROCESS HERE*.


## Tradeoff between frequency and consistency





## Dispersion
Sequential approaches to orthography-to-phonology conversion solve this problem by processing discrete speech segments from discrete print segments in a left-to-right, segment-by-segment manner. These solutions always involve some form of justification (sometimes called "alignment" in the literature), though the alignment procedure differs across sequential implementations. For example, @Sejnowski1987 provided orthographic input strings as sequences of letters and mapped them to equal-length sequences of phonemes. This allowed their network to implement a kind of temporal processing over visual and its corresponding spoken pattern, but required architectural specifications that deviate from assumptions about processing in other connectionist networks. Their procedure required inserting null elements in slots where a phoneme wasn't present for the corresponding input letter (or grapheme) given that the input and output sequences were always equal length. So, for the input pattern "late" the corresponding output would be `L - EY - T - __`, with the final phoneme segment `__` being the null one given the word-final silent "e" in "late". This avoids the type of dispersion found in feedforward networks, where knowledge about letters and their corresponding sounds is localized to weighted connections associated with specific slots given the way that input and output patterns are specified during learning, but introduces a different type of knowledge localization that is undesirable: that letters on the input layer become associated too narrowly with phonemic segments on the output given the segment-by-segment processing mechanism (i.e., they lose the context sensitive nature of processing due to the segment-by-segment way that print and speech are associated in the network).

Dual-route models use an assembly method, where a letter or letter segment (e.g., grapheme) is associated with a phonemic segment by assembly rule, where "assembly" in this literature refers to the association of phonological segments to its corresponding orthographic segment. Processing speech from print involves assembling the phonology from the pre-specified letter-sound rules, which are derived via an analytic technique that resides outside the scope of the computational architecture itself (i.e, specified by the experimenter). The assembly process happens alongside a lexical lookup procedure, which has its own separate timecourse. The relative timecourse of these processes determine whether a word is produced via assembly rules or as a structured lexical object. This is the extreme end of the spectrum of methods that result in knowledge dispersion. Here knowledge about the relationship between visual and spoken elements is dispersed across the symbolic rules that operate over the crossmodal assembly that takes place during learning and performance. This form of knowldge dispersion is different implementationally but related conceptually in that what is known about similar elements in the domain becomes dissociated based on the structure of the architecture, which of course has implications on the learning that takes place given that architecture.

Other approaches handle input strings with the use of slots, but in a different way than in @Sejnowski1987. Commonly in connectionist feedforward implementations input patterns are justified in a vowel-centered way (see @Plaut1996 for a discussion). A similar solution exists in hybrid connectionist/ dual route architectures like @Perry2010. Here the input patterns are fit into a template defined by structure in terms of orthography across syllables, which is specified by the limits of the structure of the training set. So, for example the template used for the two syllable template in @Perry2010 involves fitting orthographic inputs into a `CCCVCCCC` pattern on each of the two (possible) syllables via a "graphemic buffer". This is the mechanism that defines the "slots" on the visual input to which any given input pattern are aligned to the available phonemes.

In the present computational system there is no alignment, justification, or slots. However, an important question concerns whether or not knowledge is dispersed in a model specified in this way. The most direct comparison is to models that contain weighted connections between input and output units, and intermediary hidden ones too. The issue of dispersion is investigated in the present architecture in two ways. First, we present data about the extent to which identical input patterns in different (orthographic) contexts are associated with different units in the network and, second, the extent to which the overall pattern of representation of identical input patterns are similar to each other.

A further piece of supporting data concerns a manipulation to the masking mechanism used in the model. Masking introduces a kind of justification, but one where masked segments should never be realized in the knowledge of the learner.






## Benchmarks
*jared1990: latencies increase with number of syllables, but is moderated by frequency (Experiment)
*jared1990: exceptional and regular inconsistent words exhibit longer latencies as compared to regular words, but only for low frequency words (see yap2009, top right of p. 503 for discussion of the effect)
*position of irregularity effects


kawamoto1998 showed that words with less predictable pronunciations of the vowel exhibit longer latencies for the consonant in the onset, rather than simply longer overall naming latency of the entire word. This is due to the fact that the vowel pronunciations that are less predictable require more processing time, and this processing manifests in the system dwelling on pre-vocalic consonants, rather than being distributed throughout the naming process for the entire word. Their studies were performed using monosyllabic stimuli that were quite simple, though well established in the word reading literature related to naming and related phenomena.

## Canonical naming effects
### Frequency
### Regularity
### Frequency by Regularity

## Latency
Given that computational processing times on a GPU haven't been validated as an analogous method for naming latency in humans, similar to Sibley et al. (2010) a measure of model confidence for processing an orthographic word was designed to measure the latency for reading that word. 


## Overall results
*Provide an overall summary of results here before going into the piecemeal results. See Seidenberg & McClelland (1989) pages 531-532 for examples