---
title: "3. Experimentation"
output: html_document
---

```{r ch3_src, include=FALSE}
source('chapter3_data.R')
source('plot-specs.R')
```

# Chapter 3: Experimentation
The purpose of the model developed here is to read printed words aloud. A number of results are presented based on results from behavioral research, features of the cognitive model the architecture represents, and comparisons to similarly oriented models of word reading from elsewhere in the literature. The purpose of this experimentation is twofold: to present results that convey important features of the architecture as it relates to cognition and development and to make comparisons to experimentation elsewhere of similar phenomena, both from work in computational models of reading and human behavior.

## Accuracy
Results of the model's ability to correctly produce a spoken form of a word can be based on a number of different measures. The most common measure of the model's ability is an accuracy score, which itself can be structured in a few different ways - similar to that of a human reader. Given that the goal of the model is to take a sequence of letters and recode them in the form of a sequence of speech sounds, individual sounds can be inspected for their accuracy along with the entire sequence of sounds - that is, the entire word. Both the phoneme-wise and the word-wise accuracy metrics are used in reporting, depending on the purpose of the result shown. They are of course correlated given that a word that is produced correctly is one in which all the phonemes are produced correctly (or at least each phoneme produced is closer to the correct phoneme than all other possible phonemes).


## Training details
### Batch assembly
In order to avoid padded input and output representations for training, words were assembled for learning based on phonological length for reasons discussed previously. Batches consisted of words of the same phonological length, where the orthographic length of words within a batch could vary but were accommodated with masked timesteps on input (i.e., timesteps that are skipped rather than empty). This is due to the fact that words that share the same number of phonemes don't necessarily share the same number of letters. For simplicity during training, batches were selected from all words of the same phonological length until all the words of that length were trained, then a new length was selected for training. This was repeated until all words were trained during that epoch. Within each epoch, a set of shorter words (e.g., words with a phonological length of five phonemes) were more likely to come earlier in the epoch than words of a longer length (e.g., those with 8 phonemes).

Alternatives to this process are possible, including specifying batches of one training example at a time (i.e., stochastic gradient descent). Investigating the role of batch size on learning, and the behavioral correlates in humans would no doubt be a worthwhile avenue of future research but is outside the scope of the work being done here. The assembly of batches in this way isn't intended to convey a hypothesis about how processing in humans works, rather is specified as it is for efficiency reasons - alternative such as stochastic gradient descent can lead to training times of over an order of magnitude longer than those where larger batches are assembled.

## "Testing" trials
The model is tested in a way that follows a different procedure than that used in training, allowing for more flexible and deeper introspection about the dynamics at play in the learning system when operating. These will be called _testing trials_, and are performed as follows. When a word is submitted for testing an orthographic pattern is introduced to the the orthographic input - the model's visual input system. This pattern is structured just like that of the orthographic input during training: it is a two dimensional array with as many letter representations along the first axis as there are letters in the word. This time-varying input pattern is input to the orthographic input, and its representation decomposed into its two critical states for use in phonology: the cell state and the final hidden state. Once these vectors are achieved for the orthographic form of the word, that portion of the model rests and is unchanged for the remainder of the trial.

The states of the orthographic layer are passed to phonology to generate the phonological code output. This process is initiated by the input of the start of word segment (which we can think of as the symbol `#`), the representation of which was described earlier. The fact that the model has been trained with an explicit start of word marker enables the phonological layer to produce a next phoneme given the current state of the layer and its input pattern. At the start of production this context is provided completely from the orthographic layer. That is, the role of the phonological portion of the model is to produce a phonological code given the orthographic context.

Once the first phonemic pattern is produced it is saved for subsequent phonemes to be appended to it in order to form a full sequence of phonemes that constitutes a phonological output. While the weights in the network are frozen at this stage prior to testing, the states of the phonological LSTM is updated at each timestep following the same processing flow as described in the architecture section. Once a phoneme is output at each timestep, it is fed back to the input of the phonological layer to support the prediction of subsequent phonemes. This differs in process from training trials but is the same conceptually given that in training, the entire phonological sequence is processed one segment at a time without the feedback process from the output layer back to the input. Because the output for a given phoneme is very unlikely to be specified as a perfect binary pattern (because predictions almost always have until that approach zero or one, but aren't actually those values), any phoneme output is compared to all possible phoneme outputs using an L2 norm and the nearest phoneme is selected and recorded as the produced phoneme (both the symbol representing the phoneme and the activity pattern are saved). The production trial progresses until the phonological output layer produces the stop producing segment (`%`) or exhausts the total possible number of segments given the longest words included in training. At this point the word produced is recorded for analysis.

The produced pattern (in array form) is a time-distributed series of vectors representing patterns of activation over phonological features. These patterns can be thought of as a pattern that might pre-specify a motor command to be passed to an articulatory mechanism during production (see @Seidenberg1989 for more of a discussion on the process). The quantitative objects produced by the model can be examined for their accuracy in a number of ways, including how close each unit is to its target activation, how close each phonemic pattern is to its corresponding target, and how close the entire word is to its target word. The concept of time in terms of naming latency in this architecture is much less clear, especially given the novelty of application of LSTM architectures to understanding human cognitive processes related to reading and speech. This is a topic for further research, and is likely to be fruitful given the temporal nature of such a computational system. Nonetheless, only accuracy metrics are considered here.

### Frequency
The role of experience in the development of reading skill is an essential one to the reading process [@Seidenberg2018]. More experience with a word facilitates knowledge about that word and the ability to use the word in production and naming. The simplest and most common way that experience effects in word reading have been investigated has been in terms of word frequency as estimated from corpora of text or speech. Word frequency can be implemented in computational models of reading in different ways. Implementations have included the selection of words for training in a probabilistic fashion with the probability of selection determined by the word's frequency of occurrence (e.g. @Seidenberg1989) and the scaling of the learning signal during training proportional to the frequency of that word. The latter was used here for computational efficiency given the implementation of the batch training regimen used, where the gradients calculated for a learning trial were scaled proportional to the frequency of that word in a large corpus of text (see @Sibley2010 for a similar implementation). Frequencies from the hyperspace analogue to language (HAL) were used [@Lund1996], accessed through the data provided by the English Lexicon Project [@Balota2007]. These frequency calculations were utilized to facilitate a comparison to the behavioral data also provided in the ELP, such that more direct comparisons could be made between the computational model here and the human behavioral data there.

Because word frequencies exhibit a dramatic range, a scaling operation was used to both capture the distribution and translate the value into a proportion (where a value of one would cause the full gradient for that word to participate in the weight update, and a value of zero would mean throwing that gradient away). To accomplish this, the formula from @Seidenberg1989 (p. 530) was used to calculate $p$, the proportion applied to the gradient update for the word being learned.

$$
p = K \cdot log(frequency + 2)
$$
This results in a distribution of proportions for each word that is monotonically related to the raw frequencies of those words. The value of the constant $K$ was selected such that the highest value for $p$ was `.93` for the most frequent word (just as in @Seidenberg1989) - the word `and` (raw frequency of `r prettyNum(freq_for_word(mono, 'and'), big.mark=',')`). For the monosyllabic corpus this resulted in `K` being equal to `r round(monosyllabic_k, 3)`. A plot of the distribution of values for $p$ against raw values for frequency are shown in Figure X.

```{r monopbyraw, echo=FALSE, message=FALSE, warning=FALSE, fig.width=3, fig.height=3, fig.cap='Scaled word frequency by raw frequency for words included for training in the time-varying monosyllabic model implemented. For reference, the most frequent word, "and", can be seen in the upper right hand corner.'}
mono %>% 
  ggplot(aes(freq, freq_scaled)) +
  geom_point(color = 'firebrick', size = .05) +
  theme_bw() +
  labs(x = 'Raw frequency', y = 'Scaled frequency (p)')
```


## Monosyllabic words
Given that a number of behavioral effects for reading monosyllabic words are well established, along with investigation of those effects using computational models of such processes, a set of monosyllabic words was assembled from the full corpus of words and used for experimentation. A list used in other connectionist modeling exercises was used for this list [@Cox2019]. Morphological variants that were related with the addition of `r scaps('s')` or `r scaps('ed')`, and verbs with the  This resulted in a training set of `r length(unique(mono$word))` words. In order to avoid spurious effects of length (either too short or too long), words ranging from three to seven phonemes were included in the training set. This included phonologically short words like `r scaps('and')` and `r scaps('through')`, and longer words like `r scaps('strength')` and `r scaps('sprints')`.


## Results
In order to link model behavior to that of humans, a standard approach was taken to establish the plausibility of the computation model here as a cognitive model. Known effects of the structure of words on learning and performance are demonstrated, and comparison findings from other computational implementations and behavioral experiments are provided. Those that pertain to monosyllabic word naming are included in this section on monosyllabic words, and a subsequent section deals with words that are greater than one syllable, with effects unique to polysyllabic words included there.

## Basic characterization of model behavior
A base model was developed in order to determine whether or not, like human readers, the model was able to produce well-formed phonological output from orthography after learning the mappings between the two for a large set of monosyllabic words. The model for monosyllabic words was trained to a high level of performance on all the training items and tested on a range of words to inspect the production of the model for trained and untrained words. 7% of words were held out for the purposes of generalization testing at the end of training. 

## Frequency
More experience with a word leads to more robust representation of the word, which in turn enhances the ability to act on it in some form. The most common proxy for experience in the reading literature is the frequency of a word's occurrence in a set of texts or other corpus. Connectionist models of reading aloud have established the effect of experience in a number of studies, where experience is implemented by proxy as a frequency-influenced procedure during learning (XX).

Word frequencies were used in order to scale gradients during training for the time-varying model reported on here. One issue is whether the LSTM architecture is sensitive to experience given this implementation, and whether or not this sensitivity resembles that of other computational architectures and human behavioral data. In other work this was done through constructing and comparing a model "latency" of some kind to latency data in humans or other models. For example, in @Sibley2010 "latency" was defined using an accuracy metric, and in @Seidenberg1989 this was the sum of squared errors. Rather than assuming the link between latency and accuracy here, I will call examine performance in these analyses by referencing accuracy directly. Of course, the assumption is that higher accuracy (lower error) is associated with faster responses in naming and other tasks, thoough this assumption doesn't play out directly in the analyses reported here, in part because the concept of "latency" in long short term memory units isn't well established but is likely to be an area of future research, especially related to models of human cognition and performance.

Earlyincreases in word frequency are associated with lower errors, as shown in Figure XX. This association is lower in both computational models than in the naming accuracy data for the training words from @Balota2007, but in all three we see a relationship between frequency and errors. This comparison demonstrates that by including frequency as an aspect of training we see an impact on performance, regardless of architecture.

```{r, fig.cap="The relationship between word frequency (scaled) and word-level error is shown for human behavioral data (left), a feedforward network architecture (middle), and the time-varying/ LSTM architecture (right). Here error is calculated as mean squared error. In all three cases, an increase in frequency is associated with an increase in errors."}

STAGE = 'Middle'

elp_words = mono %>% 
  select(word, elp_acc) %>% 
  filter(!is.na(elp_acc)) %>% 
  pull(word) %>% 
  unique()

descriptives_model = mono %>%
  filter(train_test == 'train') %>% 
  filter(stage == STAGE) %>% 
  group_by(model, stage) %>% 
  summarise(MEAN = mean(accuracy),
            SD = sd(accuracy))


COLORS = c('LSTM' = 'firebrick', 'Feedforward' = 'goldenrod', 'Behavioral' = 'Grey74')

descriptives_elp = mono %>% 
  filter(word %in% elp_words) %>% 
  filter(stage == STAGE & model == 'LSTM') %>% #arbitrary because values repeat
  filter(train_test == 'train') %>% 
  summarise(MEAN = mean(elp_acc),
            SD = sd(elp_acc)) %>% 
  mutate(model = 'Behavioral')

d_elp = mono %>% 
  filter(word %in% elp_words) %>% 
  filter(stage == STAGE & model == 'LSTM') %>% #arbitrary because values repeat
  filter(train_test == 'train') %>% 
  mutate(model = 'Behavioral') %>% 
  left_join(descriptives_elp) %>% 
  mutate(acc = -(elp_acc-MEAN)/SD) %>% 
  select(word, model, acc, freq_scaled)




mono %>% 
  filter(stage == STAGE & train_test == 'train') %>% 
  left_join(descriptives_model, by = c('model')) %>% 
  #group_by(model, stage) %>% 
  mutate(acc = (accuracy-MEAN)/SD) %>% 
  #ungroup() %>% 
  filter(word %in% elp_words) %>% 
  select(word, model, acc, freq_scaled) %>% 
  rbind(d_elp) %>% 
  ggplot(aes(freq_scaled, acc, color = model)) +
  geom_point(size = .6) +
  geom_smooth(method = 'lm', color = 'grey32') +
  scale_color_manual(values = COLORS) +
  facet_grid(~model) +
  labs(x = 'Frequency (scaled)', y = 'Error') +
  theme_bw() +
  theme(legend.position = 'none') +
  ylim(c(-1, 5))

```

Furthermore, the effect of frequency fades as learning progresses; by the end of training this effect is attenuated substantially such that accuracy is consistently high among most words. Here we see the raw error (as mean square error) plotted at two points in training: once early on (at 27 epochs) and again late in training (at 63 epochs).

```{r}

STAGE = 'Late'
mono %>% 
  filter(stage == c('Middle', 'Late')) %>% 
  filter(train_test == 'train') %>% 
  mutate(trainstage = case_when(stage == 'Middle' ~ 'Early',
                                stage == 'Late' ~ 'Late')) %>% 
  ggplot(aes(freq_scaled, accuracy)) +
  geom_point(size = .6) +
  geom_smooth(method = 'lm', color = 'grey32') +
  scale_color_manual(values = COLORS) +
  facet_grid(~trainstage) +
  labs(x = 'Frequency (scaled)', y = 'Mean Squared Error') +
  theme_bw() +
  theme(legend.position = 'none')
```


## Consistency
One important aspect of reading behavior concerns how structural complexity affects reading performance. Readers are sensitive to the complexity of the mappings between print and speech; the more complex the form, the slower and more error prone performance is. Debates about how readers deal with this complexity and the role of experience in development and performance have been important for understanding the cognitive system that underlies reading development and performance, with different theoretical frameworks approaching the computations (and associated measurement) in different ways.

Parallel distributed processing theories of reading and lexical processing, like the model reported in @Seidenberg1989 and extended in @Plaut1996, hold that complexity leads to more burdensome processing (i.e., longer naming latencies), which can be overcome with experience. Networks that implement the theory encode information about the mappings between perceptual modalities in neuron-like processing units, with a single architecturally homogenous process accounting for words learned and read. In a model that learns to map print to speech over the course of experience (perception, action) these units (weights) encode the relationships between printed and spoken words with increasing robustness. This robustness is affected by the amount of experience the reader (or computational model) has in learning the words during training and the reliability with which the mappings across modalities occur across those words.

In this line of work the complexity of the mappings between print and speech is described in terms of _consistency_: the extent to which the pronunciation of a word is similar other similarly spelled words. Defining consistency in a measurable way is important for understanding the behavior of the network. The approach here is similar to @Plaut1996, where the consistency of a word is defined by the orthographic body with respect to the phonological rime. Here _body_ refers to the portion of the word consisting of the first orthographic vowel and the orthographic structure that follows it (e.g., the _umbling_ in _fumbling_), and rime refers to the first vowel phoneme and everything that follows it (i.e, `AH1 - M - B - L - IH0 - NG` portion of the spoken form). A highly consistent word is one where words with this same body also have a similar pronunciation for this portion of the syllabic structure. We use a proportion for our purposes here, using the total number of words with a given body as the denominator, with the numerator as the number of words that share the rime present in that word. The result is that highly consistent words have values that tend towards one (i.e., all words with a given body share the rime) and inconsistent words have a value that tend towards zero.

In the corpus of words used here for modeling monosyllabic word reading

```{r}

mono %>% 
  filter(stage %in% c('Middle', 'Late') & model == 'LSTM') %>%
  filter(train_test == 'train') %>% 
  mutate(trainstage = case_when(stage == 'Middle' ~ 'Early',
                                stage == 'Late' ~ 'Late')) %>% 
  ggplot(aes(consistency, accuracy)) +
  geom_point(size = .6) +
  geom_smooth(method = 'lm', color = 'grey32') +
  scale_color_manual(values = COLORS) +
  facet_grid(~trainstage) +
  labs(x = 'Consistency (body-rime)', y = 'Mean Squared Error') +
  theme_bw() +
  theme(legend.position = 'none')

```


```{r}
descriptives_model = mono %>% 
  filter(stage %in% c('Middle', 'Late') & model == 'LSTM') %>% 
  group_by(stage) %>% 
  summarise(M = mean(freq_scaled))

mono %>% 
  filter(stage %in% c('Middle', 'Late') & model == 'LSTM') %>%
  filter(train_test == 'train') %>% 
  mutate(trainstage = case_when(stage == 'Middle' ~ 'Early',
                                stage == 'Late' ~ 'Late')) %>% 
  left_join(descriptives_model) %>% 
  mutate(freq_f = case_when(freq_scaled <= M ~ 'LF',
                            freq_scaled > M ~ 'HF')) %>% 
  ggplot(aes(consistency, accuracy)) +
  facet_grid(vars(trainstage), vars(freq_f)) +
  geom_point(size = .6) +
  geom_smooth(method = 'lm', color = 'grey32')



```

## Number of syllables






## Common words are easier to read, and more accurate
Words that are more frequently encountered tend to be easier to read, resulting in general in faster reading times and fewer errors [@Balota2004; @Forster1973; @Stanovich1978]. The first analysis reported here concerns this effect. For this analysis a training process was completed where the frequency with which words were included in training was proportional to their text frequency. In order to determine how often a word should be selected for training, the following method was used. *DESCRIBE FREQUENCY CONDITIONED TRAINING PROCESS HERE*.


## Tradeoff between frequency and consistency





## Dispersion
Sequential approaches to orthography-to-phonology conversion solve this problem by processing discrete speech segments from discrete print segments in a left-to-right, segment-by-segment manner. These solutions always involve some form of justification (sometimes called "alignment" in the literature), though the alignment procedure differs across sequential implementations. For example, @Sejnowski1987 provided orthographic input strings as sequences of letters and mapped them to equal-length sequences of phonemes. This allowed their network to implement a kind of temporal processing over visual and its corresponding spoken pattern, but required architectural specifications that deviate from assumptions about processing in other connectionist networks. Their procedure required inserting null elements in slots where a phoneme wasn't present for the corresponding input letter (or grapheme) given that the input and output sequences were always equal length. So, for the input pattern `r scaps('late')` the corresponding output would be `L - EY - T - __`, with the final phoneme segment `__` being the null one given the word-final silent `r scaps('a')` in `r scaps('late')`. This avoids the type of dispersion found in feedforward networks, where knowledge about letters and their corresponding sounds is localized to weighted connections associated with specific slots given the way that input and output patterns are specified during learning, but introduces a different type of knowledge localization that is undesirable: that letters on the input layer become associated too narrowly with phonemic segments on the output given the segment-by-segment processing mechanism (i.e., they lose the context sensitive nature of processing due to the segment-by-segment way that print and speech are associated in the network).

Dual-route models use an assembly method, where a letter or letter segment (e.g., grapheme) is associated with a phonemic segment by assembly rule, where "assembly" in this literature refers to the association of phonological segments to its corresponding orthographic segment. Processing speech from print involves assembling the phonology from the pre-specified letter-sound rules, which are derived via an analytic technique that resides outside the scope of the computational architecture itself (i.e, specified by the experimenter). The assembly process happens alongside a lexical lookup procedure, which has its own separate timecourse. The relative timecourse of these processes determine whether a word is produced via assembly rules or as a structured lexical object. This is the extreme end of the spectrum of methods that result in knowledge dispersion. Here knowledge about the relationship between visual and spoken elements is dispersed across the symbolic rules that operate over the crossmodal assembly that takes place during learning and performance. This form of knowldge dispersion is different implementationally but related conceptually in that what is known about similar elements in the domain becomes dissociated based on the structure of the architecture, which of course has implications on the learning that takes place given that architecture.

Other approaches handle input strings with the use of slots, but in a different way than in @Sejnowski1987. Commonly in connectionist feedforward implementations input patterns are justified in a vowel-centered way (see @Plaut1996 for a discussion). A similar solution exists in hybrid connectionist/ dual route architectures like @Perry2010. Here the input patterns are fit into a template defined by structure in terms of orthography across syllables, which is specified by the limits of the structure of the training set. So, for example the template used for the two syllable template in @Perry2010 involves fitting orthographic inputs into a `CCCVCCCC` pattern on each of the two (possible) syllables via a "graphemic buffer". This is the mechanism that defines the "slots" on the visual input to which any given input pattern are aligned to the available phonemes.

In the present computational system there is no alignment, justification, or slots. However, an important question concerns whether or not knowledge is dispersed in a model specified in this way. The most direct comparison is to models that contain weighted connections between input and output units, and intermediary hidden ones too. The issue of dispersion is investigated in the present architecture in two ways. First, we present data about the extent to which identical input patterns in different (orthographic) contexts are associated with different units in the network and, second, the extent to which the overall pattern of representation of identical input patterns are similar to each other.

A further piece of supporting data concerns a manipulation to the masking mechanism used in the model. Masking introduces a kind of justification, but one where masked segments should never be realized in the knowledge of the learner.






## Benchmarks
*jared1990: latencies increase with number of syllables, but is moderated by frequency (Experiment)
*jared1990: exceptional and regular inconsistent words exhibit longer latencies as compared to regular words, but only for low frequency words (see yap2009, top right of p. 503 for discussion of the effect)
*position of irregularity effects


kawamoto1998 showed that words with less predictable pronunciations of the vowel exhibit longer latencies for the consonant in the onset, rather than simply longer overall naming latency of the entire word. This is due to the fact that the vowel pronunciations that are less predictable require more processing time, and this processing manifests in the system dwelling on pre-vocalic consonants, rather than being distributed throughout the naming process for the entire word. Their studies were performed using monosyllabic stimuli that were quite simple, though well established in the word reading literature related to naming and related phenomena.

## Canonical naming effects
### Frequency
### Regularity
### Frequency by Regularity

## Latency
Given that computational processing times on a GPU haven't been validated as an analogous method for naming latency in humans, similar to Sibley et al. (2010) a measure of model confidence for processing an orthographic word was designed to measure the latency for reading that word. 


## Overall results
*Provide an overall summary of results here before going into the piecemeal results. See Seidenberg & McClelland (1989) pages 531-532 for examples