


# Chapter 3: Experimentation
The purpose of the model developed here is to produce a phonological code from a word's corresponding printed code. A number of results are presented based on comparing the model's behavior to experimentation from behavioral research with humans. Additionally, this chapter contains comparisons to similarly oriented models of word reading from elsewhere in the cognitive science literature. The purpose of this experimentation is twofold: to present results that convey important features of the architecture as it relates to cognition and development and to make comparisons to experimentation elsewhere of similar phenomena, both from work in computational models of reading and human behavior.

The organization of this chapter is as follows. A few additional specifications about testing and production trials are provided first, then results for several different experiments are reported. The first half of the results concern the model learning words with one syllable. The second half concerns words that are multisyllabic. For each section there is an account of the basic performance of the model, comparisons to human data using a megastudy of word naming [@Balota2007], and comparisons to experiments that manipulate the properties of words and examine the associated naming behavior of the model (and correspondingly, humans in the behavioral data). First, some additional implementation details of the simulations are discussed in order to properly frame the results and corresponding discussion of the model behavior in this chapter.

## Additional training and testing specifications
### Testing model accuracy: mean squared error
Results of the model's ability to correctly produce a spoken form of a word can be based on a number of different measures. The most common measure of the model's ability is an accuracy score. Accuracy can be structured in a few different ways - similar to that of a human reader. Given that the goal of the model is to take a sequence of letters and recode them in the form of a sequence of speech sounds, individual sounds can be inspected for their accuracy along with the entire sequence of sounds. This includes the vector output produced, and the string analogue: the symbolic phonemes that the vectors represent. Production mode is used for this purpose (which is described further below). Alternatively, testing mode is used to report accuracy as mean squared error (MSE). Testing mode (which was described in Chapter 2) is used when reporting accuracy at different points throughout training (as opposed to testing the network's ability to produce a phoneme sequence at the end of training). This measure of accuracy is convenient because it expresses accuracy as a single value for any given example (word), and is also a frequently used metric in the literature on computational models of word recognition and naming. Alternative metrics of model performance are provided occasionally and described along with results reported. The formula for mean squared error is shown in 7.

\begin{equation}
MSE = \frac{1}{2}\sum_{i=1}^{n}(Y_i-\widehat{Y}_i)^2
\end{equation}



### Production trials
In order to inspect the sequence of phonemes produced at the end of training, _production mode_ is used. This process was described earlier in a conceptual way, and is different than the procedure for producing output that is used in training and testing. It is reviewed here and described in a bit more detail for clarity. When a word is submitted for testing an orthographic pattern is introduced to the the orthographic input layer. This pattern is structured just like that of the orthographic input during training: it is a two dimensional array with as many letter representations along the first axis as there are letters in the word. This time-varying input pattern is input to the orthographic input layer, and its representation decomposed into its two critical states for use in phonology: the cell state and the final hidden state. Once these vectors are achieved for the orthographic form of the word, that portion of the model rests and is unchanged for the remainder of the trial.

The states of the orthographic layer are passed to phonology to generate the phonological code output. This process is initiated by the input of the start of word segment (which we can think of as the symbol `#`), the representation of which was described earlier. The fact that the model has been trained with an explicit start of word marker enables the phonological layer to produce a next phoneme given the current state of the layer and its input pattern. At the start of production this context is provided completely from the orthographic layer. That is, the role of the phonological portion of the model is to produce a phonological code given the orthographic context.

Once the first phonemic pattern is produced it is saved for subsequent phonemes to be appended to it in order to form a full sequence of phonemes that constitutes a phonological output. While the weights in the network are frozen at this stage prior to testing, the states of the phonological LSTM is updated at each timestep following the same processing flow as described in the architecture section. Once a phoneme is output at each timestep, it is fed back to the input of the phonological layer to support the prediction of the subsequent phoneme. This differs in process from training trials but is the same conceptually given that in training, the entire phonological sequence is processed one segment at a time without the feedback process from the output layer back to the input. Because the output for a given phoneme is very unlikely to be specified as a perfect binary pattern (because predictions almost always have until that approach zero or one, but aren't actually those values), any phoneme output is compared to all possible phoneme outputs using an L2 norm and the nearest phoneme is selected and recorded as the produced phoneme (both the symbol representing the phoneme and the activity pattern are saved). The production trial progresses until the phonological output layer produces the stop producing segment (`%`) or exhausts the total possible number of segments given the longest words included in training. At this point the word produced is recorded for analysis.

The produced pattern (in array form) is a time-distributed series of vectors representing patterns of activation over phonological features. These patterns can be thought of as a pattern that might pre-specify a motor command to be passed to an articulatory mechanism during production (see @Seidenberg1989 for more of a discussion on the process). Measures of phoneme accuracy are reported either using Euclidean distance or using the symbolic, string representation of the phoneme produced (using the Euclidean measure).

### Comparisons to naming latency
Most often, accuracy measures (whether from production or testing mode) are compared to naming latency taking from behavioral data, as in other similar reporting on computational models [@Sibley2010]. This is a proxy measurement, rather than one that conveys true temporal dynamics of the computational model. The concept of time in terms of computational latency in this architecture not clear, especially given the novelty of application of LSTM architectures to understanding human cognitive processes related to reading and speech. This is a topic for further research, and is likely to be fruitful given the temporal nature of such a computational system. Nonetheless, the graded measures of accuracy described above are used here as an approximate analogue to naming latency in comparisons to behavioral data, with further technical discussion relative to a given result were applicable.


## Experiments with monosyllabic words
### Monosyllabic word corpus
Given that a number of behavioral effects for reading monosyllabic words are well established, along with investigation of those effects using computational models of such processes, a set of monosyllabic words was subset from the full corpus of words and used for simulations reported here. A list used in other connectionist modeling was used for this purpose [@Cox2019]. Words from a number of experiments were included so that model behavior could be compared directly to that of human readers. These sets are described along with specific results reported throughout this section. The resulting training set totaled `r length(unique(mono$word))` words. Words were included using the same criteria reported in Chapter 2.

In order to link model behavior to that of humans, a standard approach was taken. Effects of word-level characteristics on model accuracy are provided first. Then more granular effects related to experimental manipulations of those word characteristics are reported. Many effects that are relevant to monosyllabic words are subsequently reproduced in the model that also learns multisyllabic words. The results for the monosyllabic words start with word frequency, move on to print-speech consistency, and then examine their interaction before reproducing effects found in experimental manipulations of these factors taken from prior studies. In examining initial results of these word characteristics, results from a standard feedforward network that maps orthography to phonology are also reported in order to establish a connection between the time-varying model reported here and another standard model architecture from the learning literature.

The time-varying model was trained to 72 epochs, and achieved asymptotic performance at that point. Accuracy data were generated using testing mode halfway through training (at epoch 36) in order to examine the effect of the frequency manipulation on learning. A comparison feedforward model was trained over the same set of training items, also to asymptote (300 epochs) using all the same specifications as the model reported in @Cox2019. The same gradient-based frequency manipulation was used in the feedforward model as with the time-varying model.

## Frequency effects
More experience with a word leads to more robust representation of the word, which in turn enhances the ability to act on it in some form. The most common proxy for experience in the reading literature is the frequency of a word's occurrence in a set of texts. Words that are more frequently encountered tend to be easier to read, resulting in general in faster reading times and fewer errors [@Balota2004; @Forster1973; @Stanovich1978]. Models of reading aloud have simulated the effect of experience in a number of studies, where experience is sometimes implemented by proxy as a frequency-influenced procedure during learning [@Sibley2010; @Plaut1996]. Similar to other models word frequencies were used here in order to scale gradients during training for the time-varying and feedforward models. One issue is whether the time-varying architecture is sensitive to experience implemented in this way, and concerns whether or not this sensitivity resembles that of other computational architectures and human behavioral data.

In order to examine the effect of frequency on accuracy, mean squared error for the time-varying and feedforward models were generated halfway through training. These are reported alongside both latency and accuracy values taken from @Balota2007. All values were standardized in order to share a y-axis. This involved arranging the accuracy scores such that increases in the values represented increases in processing difficulty (i.e., mirroring the distribution of response times). Figure 4 shows the relationship for the two computational models alongside behavioral data from the ELP. Frequencies have been log transformed. Increases in word frequency are associated with a reduction in processing difficulty. This trend is reduced in both computational models relative to the data from @Balota2007, but in all four we see a common pattern.

```{r, freqeffectHumanComp, echo=FALSE, warning=FALSE, fig.height=3, fig.cap="The relationship between word frequency (scaled) and word-level processing difficulty is shown for human behavioral data (panel A shows naming accuracy and panel B shows naming latency), a feedforward network architecture (C), and the time-varying/ LSTM architecture (D). Points are individual words that participated in training. In the case of human accuracy (A) errors are computed as accuracy of naming the word in a megastudy of word reading [@Balota2007] where the accuracy value of a word has the sign reversed. Human RTs are standardized milliseconds. Model errors (C-D) are mean squared error, calculated unitwise on the phonological output layer of the network (over differences between the produced output for a unit and the target output). All metrics are standardized so that points (words) could share a common y-axis."}

STAGE = 'Middle'

elp_words = mono %>% 
  select(word, elp_acc) %>% 
  filter(!is.na(elp_acc)) %>% 
  pull(word) %>% 
  unique()


 
descriptives = mono %>%
  filter(train_test == 'train') %>% 
  filter(stage == STAGE) %>% 
  group_by(model, stage) %>% 
  summarise(MEAN = mean(accuracy),
            SD = sd(accuracy))


COLORS = c('LSTM' = 'firebrick', 'Feedforward' = 'goldenrod', '(D) Time-varying' = 'firebrick', 'Time-varying' = 'firebrick',  '(C) Feedforward' = 'goldenrod', 'Human - Accuracy' = 'Black', 'Human - RT' = 'Grey20', '(A) Human - Naming Accuracy' = 'Black', '(B) Human - Naming RT' = 'Grey20')

descriptives_elp_acc = mono %>% 
  filter(word %in% elp_words) %>% 
  filter(stage == STAGE & model == 'LSTM') %>% #arbitrary because values repeat
  filter(train_test == 'train') %>% 
  summarise(accMEAN = mean(elp_acc),
            accSD = sd(elp_acc),
            rtMEAN = mean(elp_rt),
            rtSD = sd(elp_rt)) %>% 
  mutate(model = '(A) Human - Naming Accuracy')

d_elp_acc = mono %>% 
  filter(word %in% elp_words) %>% 
  filter(stage == STAGE & model == 'LSTM') %>% #arbitrary because values repeat
  filter(train_test == 'train') %>% 
  mutate(model = '(A) Human - Naming Accuracy') %>% 
  left_join(descriptives_elp_acc) %>% 
  mutate(acc = -(elp_acc-accMEAN)/accSD) %>% 
  select(word, model, acc, freq, consistency)


descriptives_elp_rt = mono %>% 
  filter(word %in% elp_words) %>% 
  filter(stage == STAGE & model == 'LSTM') %>% #arbitrary because values repeat
  filter(train_test == 'train') %>% 
  summarise(accMEAN = mean(elp_acc),
            accSD = sd(elp_acc),
            rtMEAN = mean(elp_rt),
            rtSD = sd(elp_rt)) %>% 
  mutate(model = '(B) Human - Naming RT')

d_elp_rt = mono %>% 
  filter(word %in% elp_words) %>% 
  filter(stage == STAGE & model == 'LSTM') %>% #arbitrary because values repeat
  filter(train_test == 'train') %>% 
  mutate(model = '(B) Human - Naming RT') %>% 
  left_join(descriptives_elp_rt) %>% 
  mutate(acc = (elp_rt-rtMEAN)/rtSD) %>% 
  select(word, model, acc, freq, consistency)



mono %>% 
  filter(stage == STAGE & train_test == 'train') %>% 
  left_join(descriptives, by = c('model')) %>% 
  mutate(acc = (accuracy-MEAN)/SD) %>% 
  filter(word %in% elp_words) %>% 
  select(word, model, acc, freq) %>% 
  rbind(select(d_elp_acc, -consistency)) %>%
  rbind(select(d_elp_rt, -consistency)) %>% 
  mutate(model = case_when(model == 'LSTM' ~ '(D) Time-varying',
                           model == 'Feedforward' ~ '(C) Feedforward',
                           TRUE ~ model)) %>% 
  ggplot(aes(log(freq), acc, color = model)) +
  geom_point(size = .2) +
  geom_smooth(method = 'lm', color = 'grey32', span = 1/3, fill = 'grey48') +
  scale_color_manual(values = COLORS) +
  facet_grid(~model) +
  labs(x = 'Frequency (log)', y = 'Processing difficulty') +
  theme_apa() +
  theme(legend.position = 'none', strip.text = element_text(size = 5.25)) +
  ylim(c(-1, 5))
```

\newpage
Furthermore, the effect of frequency fades as learning progresses; by the end of training this effect is attenuated substantially such that accuracy is consistently high among most words. With enough experience, processing difficulty is minimized even for the most infrequent words. In table 5 we see this in the cumulative error scores at two points in training the time-varying network: once early on (at 36 epochs) and again late in training (at 72 epochs). Errors become low even for the least frequent words.

```{r, freqeffectEarlyLate, echo=FALSE, fig.height=3.5, fig.cap= 'The effect of word frequency early and late in training on the time-varying network differs such that with ample experience errors on infrequent words are minimized and the overall effect of frequency on error is reduced. Raw errors are plotted for words (points) using mean squared error.' }

mono %>% 
  filter(stage == c('Middle', 'Late')) %>% 
  filter(model == 'LSTM') %>% 
  filter(train_test == 'train') %>% 
  mutate(trainstage = case_when(stage == 'Middle' ~ 'Early in training',
                                stage == 'Late' ~ 'Late in training')) %>% 
  ggplot(aes(log(freq), accuracy)) +
  geom_point(size = .5, color = 'firebrick') +
  geom_smooth(method = 'lm', color = 'grey32', fill = 'grey48') +
  scale_color_manual(values = COLORS) +
  facet_grid(~trainstage) +
  labs(x = 'Frequency (log)', y = 'Mean Squared Error') +
  theme_apa() +
  theme(legend.position = 'none')
```


## Consistency effects
Another important aspect of learning to read 
concerns how structural complexity affects reading performance. Readers are sensitive to the degree of coherent covariation among the mappings between printed and spoken words; the less covariation that exists between letters and sounds, the slower and more error prone performance is. Debates about how readers deal with this complexity and the role of experience in development and performance have been important for understanding the cognitive system that underlies reading development and performance, with different theoretical frameworks approaching the computations (and associated measurement) in different ways [see @Seidenberg2022 for a thorough review].

Parallel distributed processing theories of reading and lexical processing [@Plaut1996; @Seidenberg1989], have established that complexity of this kind leads to more burdensome processing (e.g., longer naming latencies, higher error) which can be overcome with sufficient experience. Connectionist models encode information about the mappings between perceptual modalities in neuron-like processing units, which come to encode the relationships between printed and spoken words with increasing robustness. This robustness is affected by the amount of experience the reader (or computational model) has in learning the words during training and the reliability with which the mappings across modalities occur across those words.

The complexity of the mappings between print and speech is defined in terms of _consistency_: the extent to which the pronunciation of a word is similar other similarly spelled words^[While different theoretical approaches define this structural properties in different ways, it is outside the scope of the current work to adjudicate between different approaches. This issue is discussed thoroughly elsewhere [@Seidenberg2022] and would be worthwhile for consideration in future modeling exercises using the architecture reported here.]. Defining consistency in a measurable way is important for understanding the behavior of the network and corresponding reading behavior in humans. The approach taken here is similar to @Plaut1996, where the consistency of a word is defined by the orthographic body with respect to the phonological rime (for monosyllabic words). Here _body_ refers to the portion of the word consisting of the first orthographic vowel and the orthographic structure that follows it (e.g., the `r scaps('umbling')` in `r scaps('fumbling')`, and rime refers to the first vowel phoneme and everything that follows it (i.e, `r scaps('ah1-m-b-l-ih0-ng')` portion of the spoken form). A highly consistent word is one where words with this same body also have a similar pronunciation for this portion of the syllabic structure. We use a proportion for our purposes here, using the total number of words with a given body as the denominator, with the numerator as the number of words that share the rime present in that word [see @Siegelman2020 for other related methods of measurement]. The result is that highly consistent words have values that tend towards one (i.e., all words with a given body share the rime) and inconsistent words have a value that tend towards zero.

The time-varying network shows a similar effect of consistency on accuracy as both the feedforward network as well as human naming data (Figure 6). As consistency increases, we see lower error scores. Consistent words are, in general, easier to read than less consistent words, and this effect is present across both human (left pane) and model data (middle and right panes).

```{r, consistencyHumanComp, echo=FALSE, fig.height=3.5, fig.cap="The relationship between monosyllabic word consistency and error is shown for computational models (feedforward and time-varying) and human naming tasks. In all three there is a relationship between consistency and error such that increases in consistency (across words) are associated with lower error on average. Here errors for human and model data are calculated as previously described with human behavioral data reported as accuracy only, standardizing so that data can share the same y-axis."}

COLORS = c('(C) Time-varying' = 'firebrick', '(B) Feedforward' = 'goldenrod', '(A) Human - Naming Accuracy' = 'Black')


mono %>% 
  filter(stage == STAGE & train_test == 'train') %>% 
  left_join(descriptives, by = 'model') %>% 
  mutate(acc = (accuracy-MEAN)/SD) %>% 
  filter(word %in% elp_words) %>% 
  select(word, model, acc, consistency) %>% 
  rbind(select(d_elp_acc, -freq)) %>%
  mutate(model = case_when(model == 'LSTM' ~ '(C) Time-varying',
                           model == 'Feedforward' ~ '(B) Feedforward',
                           TRUE ~ model)) %>% 
  ggplot(aes(consistency, acc, color = model)) +
  geom_point(size = .6) +
  geom_smooth(method = 'lm', color = 'grey32', fill = 'grey48') +
  scale_color_manual(values = COLORS) +
  facet_grid(~model) +
  labs(x = 'Consistency (standardized)', y = 'Error') +
  theme_apa() +
  theme(legend.position = 'none',
        strip.text = element_text(size = 7))

```

Like the effect of word frequency on accuracy, the effect of consistency diminishes over time. Processing difficulty associated with the consistency of an item can be overcome by having enough experience with it. This can be seen when examining the relationship between consistency and error at two stages of the learning process. Table 7 shows the relationship between consistency and error early in training and late in training, with the correlation decreasing over time as the learner develops experience with words in the training set.

```{r, consistencyEarlyLate, echo=FALSE, fig.height=3.5, fig.cap='The relationship between word consistency and error at two different points in training is shown. With experience, the effect of consistency on errors is reduced.'}

mono %>% 
  filter(stage %in% c('Middle', 'Late') & model == 'LSTM') %>%
  filter(train_test == 'train') %>% 
  mutate(trainstage = case_when(stage == 'Middle' ~ 'Early in Training',
                                stage == 'Late' ~ 'Late in Training')) %>% 
  ggplot(aes(consistency, accuracy)) +
  geom_point(size = .6, color = 'firebrick') +
  geom_smooth(method = 'lm', color = 'grey32', fill = 'grey48') +
  scale_color_manual(values = COLORS) +
  facet_grid(~trainstage) +
  labs(x = 'Consistency (body-rime)', y = 'Mean Squared Error') +
  theme_apa() +
  theme(legend.position = 'none')

```

### Consistency effects in production mode
These effects can also be seen in accuracy data from production mode of the time-varying network (Figure 8). At the end of training, items from the train and test set were tested for their accuracy, calculated using Euclidean distance between the produced word and the target word. Test items (7% of the corpus held out for test) show the relationship more strongly than train items, but in both sets we see that increases in body-rime consistency are associated with lower error (distance).

```{r, consistencyGeneralization, echo=FALSE, fig.height=3.5, fig.cap='The relationship between (body-rime) consistency and error in production mode are shown, where error is defined in terms of the Euclidean distance between the produced form and its target (higher distance means greater error). This measure is mathematically different from but very highly correlated with mean squared error and other standard error metrics associated with these networks. The left panel shows the trend for test items and the right panel shows training items.'}
mono_lstm_testmode %>% 
  mutate(train_test = case_when(train_test == 'train' ~ 'Training items',
                                train_test == 'test' ~ 'Test items')) %>% 
  ggplot(aes(consistency, wordwise_dist)) +
  facet_grid(~train_test) +
  geom_point(color = 'firebrick') +
  geom_smooth(method = 'lm', color = 'grey32', fill = 'grey48') +
  labs(x = 'Consistency (body-rime)',
       y = 'Euclidean distance (error)') +
  theme_apa()

```


## The interaction between frequency and consistency
The consistency of a word and the frequency with which it appears during learning have interrelated -- and similar -- effects on learning (and accuracy). Consistency concerns the extent to which a word's structure is shared with other words in the learning environment. Here this is defined in terms of the orthographic body and phonological rime, but we should consider structure more generally outside of this operationalization of the concept. More consistent words will contribute in a more substantial way to the weighted connections between units in the network by virtue of the learning rule used in the network. A two words that share neighborhood structure (similar pronunciations and similar spellings) will mutually benefit from the weight optimization that happens throughout learning. For example, take the word `r scaps('feed')`. This word is highly consistent because all other words in the training corpus that end in `r scaps('-eed')` (e.g., `r scaps('tweed')`, `r scaps('speed')`, `r scaps('seed')`, etc.) also have the rime `r scaps('...iy-d')`. Therefore learning `r scaps('feed')` benefits knowledge related to the other words in its neighborhood (like `r scaps('tweed')`).

This also applies to other aspects of structure, including structure possessed by words that are greater than one syllable, even though single syllable words are the focus here. For example, the words `r scaps('print')`, `r scaps('sprint')`, and `r scaps('prim')` will benefit from weight updates that result from learning the word `r scaps('prince')` even though this neighborhood of words is defined in a different way than how consistency was defined for the analyses here. Nonetheless, there is reason to believe that it is beneficial to define neighborhoods of monosyllabic words in terms of body-rime units rather than some other portion of syllabic structure due to the reliability of this structure relative to alternatives. For example, @Treiman1995 demonstrated that rime structure is more stable for short monosyllabic words, and that this stability is associated with more explained variance in naming latencies and errors than other syllabic structure.


```{r, frequencyByConsistency, echo=FALSE, fig.height=3.5, fig.cap='The relationship between word consistency and errors at two levels of word frequency. Words were binned for frequency based on a mean-split only for purposes of visualization. Early in training the effect of consistency is most prevalent in low frequency words, and this effect is reduced late in training. Also, the overall effect of consistency is attenuated by the end of training, and the effect of frequency becomes less differentiated across levels of frequency.'}
descriptives = mono %>% 
  filter(stage %in% c('Middle', 'Late') & model == 'LSTM') %>% 
  group_by(stage) %>% 
  summarise(M = mean(freq_scaled))

mono %>% 
  filter(stage %in% c('Middle', 'Late') & model == 'LSTM') %>%
  filter(train_test == 'train') %>% 
  mutate(trainstage = case_when(stage == 'Middle' ~ 'Early in Training',
                                stage == 'Late' ~ 'Late in Training')) %>% 
  left_join(descriptives) %>% 
  mutate(freq_f = case_when(freq_scaled <= M ~ 'Low Frequency',
                            freq_scaled > M ~ 'High Frequency')) %>% 
  ggplot(aes(consistency, accuracy)) +
  facet_grid(vars(trainstage), vars(freq_f)) +
  geom_point(size = .6, color = 'firebrick') +
  geom_smooth(method = 'lm', color = 'grey32') +
  labs(x = 'Consistency (body-rime)', y = 'Mean Squared Error') +
  theme_apa()


```

These differences in performance between learning stage, frequency, and consistency are seen more precisely in a statistical model of error. Mean squared error was regressed on the interaction of learning stage, frequency, and consistency (all centered on the mean), yielding significant effects for all predictors and interactions, shown in Table XX.


```{r, stageFrequencyConsistencyModel, echo=FALSE}
descriptives = mono %>% 
  filter(stage %in% c('Middle', 'Late') & model == 'LSTM') %>%
  filter(train_test == 'train') %>% 
  group_by(stage) %>% 
  summarise(frequency_m = mean(freq_scaled),
            consistency_m = mean(consistency)) %>% 
  mutate(stage_f = case_when(stage == 'Middle' ~ -.5,
                             stage == 'Late' ~ .5))

mono %>% 
  filter(stage %in% c('Middle', 'Late') & model == 'LSTM') %>%
  filter(train_test == 'train') %>% 
  left_join(descriptives) %>% 
  mutate(frequency_c = freq_scaled-frequency_m,
         consistency_c = consistency-consistency_m) %>% 
  stats::lm(accuracy ~ stage_f*consistency_c*frequency_c, data = .) -> model

modelCI = confint(model)
modeldata = summary(model)

tabledata = data.frame(modeldata$coefficients) %>% 
  mutate(Estimate = sig_level(Estimate, Pr...t..),
         Pr...t.. = presentp(Pr...t..))

tabledata$CI = squarebracket(modelCI[,1], modelCI[,2], digits = 3)

tabledata$Predictor = c('Intercept', 'Learning stage', 'Consistency', 'Frequency', 'Learn stg x consist', 'Learn stg x freq', 'Consistency x frequency', 'Learn stg x consist x freq')

rownames(tabledata) = NULL
```


  
  
\newpage

```{r, stageFrequencyConsistencyModelRender, echo=FALSE} 

HEADER = c('*b*', '*SE*', '*t*', '*p*', '95% CI', 'Predictor')
names(tabledata) = HEADER

tabledata %>% 
  select(Predictor, everything()) %>% 
  apa_table(digits = 3, format = 'pandoc', align = 'l', landscape = T,
            note = 'Model estimated using lm() in R with confidence intervals estimated using confint(), both methods from the native R stats package [@R2021]. Bold parameter estimates indicate significant *p*-values below the alpha threshold of .05.',
            caption = 'Regression Model of the Interaction Between Learning Stage, Word (Body-Rime) Consistency, and Word Frequency on Mean Squared Error')


```


## Graded effects of consistency and frequency: replicating previous experimental findings
An additional simulation was conducted in order to replicate the graded effects of consistency and the interaction with frequency. This section reports data with the words from the @Taraban1987, @Plaut1996, and @Jared1997 word lists included during training, both of which investigated these phenomena in monosyllabic words. These additional simulations were very similar to those reported in the previous sections but with the training environments including all the words from the experiments reported in these papers, with the exception of words that were too short phonologically (less than two phonemes). This exclusion was made due to the relative few very short words in the training corpus and the possible idiosyncrasies that such words have in time-varying models of phonological processing, rather than deeper theoretical reasons. In the corpus of monosyllabic words assembled, only 19 eligible words were two phonemes or fewer, and were thus excluded (just as words that were one letter or fewer were excluded). The model reported on here is entirely capable of learning and processing such words successfully.

These experimental lists were designed in order to vary the level of orthography-to-phonology consistency using a set of categorical distinctions across consistency in order to investigate the trade-offs on reading performance as a function of consistency and frequency.This distinction is important because connectionist networks show the ability to generate phonology from orthography despite the fact that such systems do not operate by way of rules that map from segments of one perceptual modality to segments of the other, as computational models using dual-route mechanisms do. If the time-varying model proposed here operates in a similar fashion to the connectionist architectures developed using feedforward architectures then results should replicate for previous experiments with such architectures. Specifically, the graded effects of consistency on error (or loss) should be observed, in addition to their interaction with frequency.

@Taraban1987 (Experiment 1a) was a simple naming experiment where participants read aloud words that were either exceptional (words with no neighbors, like `r scaps('done')`) or regular but inconsistent (like `r scaps('catch')`). Additionally, words were either high frequency or low frequency. Participants also read a set of control words that had many neighbors. Each control was matched with a test word for the starting letter and phoneme, frequency, and bigram frequency. There were 24 words in each group (so with controls equals 24 x 4 x 2 = 192 words). The study found that exception words exhibited slower, more error prone responses than inconsistent but regular words. Low frequency words showed this difference more dramatically than high frequency words.

A similar pattern is seen in the model results. This figure shows the pattern of means across word conditions and frequency. The means were calculated as the difference between test and control words, using the data from multiple runs of the model. A larger value here indicates that the test word produced more error than its corresponding control. There are clear differences between word types at each level of frequency and across the two levels of frequency. A relevant statistical model is reported on in the section about the @Plaut1996 stimuli which largely overlap with these - a more specific statistical model for these effects won't be run here in order to avoid type I errors related to multiple tests using the same model data for several comparisons to behavioral studies. Nonetheless in these data, as we will see in subsequent sections the time varying architecture produces results similar to those seen in behavioral studies of word naming and complementary connectionist systems that model the same behavior.


```{r, tarabanFig1, echo=FALSE}
COLORS_taraban = c('Regular inconsistent'='grey86', 'Exception'='black', 'regular_inconsistent'='grey86', 'exception'='black', 'Regular consistent'='grey57')

taraban_crossval %>% 
  filter(!is.na(taraban_group)) %>% 
  filter(epoch == 27) %>% 
  group_by(taraban_group, freq_taraban, taraban_test) %>% 
  summarise(mse = mean(mse)) %>% 
  ungroup() %>% 
  mutate(condition = case_when(taraban_group == 1 ~ 'Exception',
                               taraban_group == 2 ~ 'Exception',
                               taraban_group == 3 ~ 'Regular inconsistent',
                               taraban_group == 4 ~ 'Regular inconsistent'),
         freq_taraban = case_when(freq_taraban == 'high' ~ 'High',
                                  freq_taraban == 'low' ~ 'Low')) %>% 
  group_by(condition, freq_taraban) %>% 
  arrange(desc(freq_taraban)) %>% # make sure they are ordered properly
  summarise(mse = -difference(mse)) %>%
  ungroup() %>% 
  ggplot(aes(freq_taraban, mse, fill = condition)) +
  geom_bar(stat='identity', position = position_dodge(), color = 'black') +
  scale_fill_manual(values = COLORS_taraban) +
  labs(x = 'Frequency', y = 'Mean squared error') +
  theme_apa() +
  theme(legend.title = element_blank())

```

For comparison, the means from the original study are shown in Figure XX. This pattern exhibits a something similar to what is seen in the model data in addition to a difference in the effect of frequency and word type (again these are shown as differences between test and control items). The authors of the original study reported main effects of frequency and word type. This same trend is seen in the model data: high frequency words exhibit lower errors than low frequency words, and exceptional words show more error than their more consistent counterparts.

```{r, tarabanFig2, echo=FALSE}

taraban_means %>% 
  group_by(condition, frequency) %>% 
  summarise(accuracy = -difference(accuracy)) %>% 
  mutate(condition = case_when(condition == 'exception' ~ 'Exception',
                               condition == 'regular_inconsistent' ~ 'Regular inconsistent'),
         frequency = case_when(frequency == 'high' ~ 'High',
                              frequency == 'low' ~ 'Low')) %>% 
  ggplot(aes(frequency, accuracy, fill = condition)) +
  geom_bar(stat = 'identity', position = position_dodge(), color = 'black') +
  scale_fill_manual(values = COLORS_taraban) +
  labs(x = 'Frequency', y = 'Mean squared error') +
  theme_apa() +
  theme(legend.title = element_blank())

```

If we collapse across the control items in the study and compare against the exception and regular but inconsistent items, you see a clear and complimentary pattern (this just represents the raw means rather that differences against controls, where the controls are the highest consistency "Regular consistent" condition). In fact the @Plaut1996 study uses many of these words to populate a condition by that name. They are used as controls in @Taraban1987 because they serve as an appropriate comparison across conditions where items are matched to this control set with a range of structural variables and word frequency. However, we see that when they are grouped and plotted against means in the other word and frequency conditions, they come out as low error items (when combined, the lowest error items). This is compatible with the hypothesis that words with the greatest neighborhood structure will present the lowest burden to processing; the network comes to represent this reliable structure as the result of learning across many words exhibiting such structure.


```{r, tarabanFig3, echo=FALSE}
taraban_crossval %>% 
  filter(!is.na(taraban_group)) %>% 
  filter(epoch == 27) %>% 
  mutate(condition = case_when(taraban == 'reg_consistent' ~ 'Regular consistent',
                             taraban == 'regular_control' ~ 'Regular consistent',
                             taraban == 'reg_inconsistent' ~ 'Regular inconsistent',
                             taraban == 'exception' ~ 'Exception'),
         frequency = case_when(freq_taraban == 'high' ~ 'High',
                                  freq_taraban == 'low' ~ 'Low'),
         condition = fct_relevel(condition, c('Regular consistent', 'Regular inconsistent', 'Exception'))) %>% 
  ggplot(aes(frequency, mse, fill = condition)) +
  geom_bar(stat='summary', position = position_dodge(), color = 'black') +
  scale_fill_manual(values = COLORS_taraban) +
  labs(x = 'Frequency', y = 'Mean squared error') +
  theme_apa() +
  theme(legend.title = element_blank())

```

## @Plaut1996 words
@Plaut1996 took the word lists from @Taraban1987 and extended them to include _ambiguous_ words, which are words that have approximately equal numbers of friends an enemies, therefore resting between highly consistent words and very low consistency exception words. This resulted in the same conditions as those in @Taraban1987 but labeling the controls as "regular consistent", with the new "ambiguous" words, all separated among high and low frequency sets. In Figure X you we see the cross entropy (the value used as the loss function during training) for each of the four regularity conditions from @Taraban1987 and in the high and low frequency conditions taken from performance data early on in training (27 epochs). The trend observed closely resembles that of the computational results from @Plaut1996 and follows the expected pattern based on graded effects of consistency as they interact with frequency.

```{r, figPlaut1, echo=FALSE}
# shapes:
# 0 = ambiguous (square)
# 1 = exception (circle)
# 2 = regular consistent (triangle)
# 5 = regular inconsistent (diamond)
taraban %>% 
  filter(!is.na(freq_plaut)) %>% 
  filter(epoch == 27) %>% 
  filter(plaut != 'nonword') %>%
  mutate(plaut = case_when(plaut == 'ambiguous' ~ 'Ambiguous',
                             plaut == 'exception' ~ 'Exception',
                             plaut == 'reg_consistent' ~ 'Regular consistent',
                             plaut == 'reg_inconsistent' ~ 'Regular inconsistent'),
         freq_plaut = case_when(freq_plaut == 'low' ~ 'Low',
                                  freq_plaut =='high' ~ 'High'),
         freq_plaut = fct_relevel(freq_plaut, c('Low', 'High')),
         plaut = fct_relevel(plaut, c('Exception', 'Ambiguous', 'Regular inconsistent', 'Regular consistent'))) %>% 
  group_by(freq_plaut, plaut) %>% 
  summarise(loss = mean(loss)) %>% 
  ggplot(aes(freq_plaut, loss, shape = plaut, group = plaut)) +
  geom_line() +
  geom_point(size = 3) +
  scale_shape_manual(values = c(0, 1, 2, 5)) +
  labs(x = 'Frequency', y = 'Cross entropy') +
  theme_apa() +
  theme(legend.title = element_blank())
```

In order to avoid results that rely on a single, perhaps idiosyncratic simulation, the simulation was conducted 50 times over and the same trend results when examining performance relatively early in training. The figure below shows these results averaging across runs of the model at an early stage of learning (here epoch 27).

```{r,  echo=FALSE}

taraban_crossval %>% 
  filter(!is.na(freq_plaut)) %>% 
  filter(epoch == 27) %>% 
  filter(plaut != 'nonword') %>%
  mutate(plaut = case_when(plaut == 'ambiguous' ~ 'Ambiguous',
                             plaut == 'exception' ~ 'Exception',
                             plaut == 'reg_consistent' ~ 'Regular consistent',
                             plaut == 'reg_inconsistent' ~ 'Regular inconsistent'),
         freq_plaut = case_when(freq_plaut == 'low' ~ 'Low',
                                  freq_plaut =='high' ~ 'High'),
         freq_plaut = fct_relevel(freq_plaut, c('Low', 'High')),
         plaut = fct_relevel(plaut, c('Exception', 'Ambiguous', 'Regular inconsistent', 'Regular consistent'))) %>% 
  group_by(freq_plaut, plaut) %>% 
  summarise(loss = mean(loss)) %>% 
  ggplot(aes(freq_plaut, loss, shape = plaut, group = plaut)) +
  geom_line() +
  geom_point(size = 3) +
  scale_shape_manual(values = c(0, 1, 2, 5)) +
  labs(x = 'Frequency', y = 'Cross entropy') +
  theme_apa() +
  theme(legend.title = element_blank())

```



```{r, echo=FALSE}
taraban_crossval %>% 
  filter(!is.na(freq_plaut)) %>% 
  filter(epoch == 27) %>% 
  filter(plaut != 'nonword') %>%
  mutate(plaut = case_when(plaut == 'ambiguous' ~ 'Ambiguous',
                             plaut == 'exception' ~ 'Exception',
                             plaut == 'reg_consistent' ~ 'Regular consistent',
                             plaut == 'reg_inconsistent' ~ 'Regular inconsistent'),
         freq_plaut = case_when(freq_plaut == 'low' ~ -.5,
                                  freq_plaut =='high' ~ .5),
         plaut = fct_relevel(plaut, c('Exception', 'Ambiguous', 'Regular inconsistent', 'Regular consistent')),
         plaut_num = as.numeric(plaut)) %>%
  mutate(plaut_c = plaut_num-mean(c(1, 2, 3, 4))) %>% 
  lmer(loss ~ plaut_c*freq_plaut + (1|word) + (1|run_id), data = .) -> m_plaut1

#save(m_plaut1, file = 'data/m_plaut1.Rda')

load(file = 'data/m_plaut1.Rda')



# recompute confints:
#m_plaut1_confint = confint(m_plaut1)
#save(m_plaut1_confint, file = 'data/m_plaut1_confint.Rda')

load('data/m_plaut1_confint.Rda')

#m_plaut1_summary = summary(m_plaut1)
#save(m_plaut1_summary, file = 'data/m_plaut1_summary.Rda')
load(file = 'data/m_plaut1_summary.Rda')

m_plaut1_confint

# runtime is about 25 minutes:
#m_plaut1_anova = car::Anova(m_plaut1, type = 3, test = 'F')
# note that the anova table reflects the following usage:
# Analysis of Deviance Table (Type III Wald F tests with Kenward-Roger df)
#save(m_plaut1_anova, 'data/m_plaut1_anova.Rda')
load(file = 'data/m_plaut1_anova.Rda')
m_plaut1_anova
```


Furthermore the effect is similar though not identical to those observed in @Plaut1996, as demonstrated in a statistical model of consistency condition (expressed as a linearly decreasing set of conditions in the following order: exception, ambiguous, regular inconsistent, and regular consistent) with frequency (high versus low). Using a linear mixed effects model estimated in R [@R2021] in the lme4 package [@Bates2015] with confidence intervals estimated in the stats library [@R2021] and p-values computed using the Anova() method in the car package [@Fox2019]. This model includes by item random intercepts as well as random intercepts for each run (each simulation). Here an increase in condition is, on average, associated with a `.01` decrease in loss (*b* = -.01, *SE* = .003, 95%CI = [-.01, -.005]), with the effect of frequency associated with a decrease in loss of .01 units. The interaction, unlike what was reported in @Plaut1996, is not significant (*b* = .003, *SE* = .003, *F*(1, 172) = 1.55, *p* = .22, 95%CI = [-.002, .008]). This pattern resembles the effects described earlier looking at frequency and consistency as quantitative predictors (derived from the structure of words in the corpus directly) but differs in the specification of the interaction (because they are defined across conditions rather than as observed). Words of increasing consistency demonstrate lower levels of error, and high frequency words are less error prone than lower frequency words.

```{r,  echo=FALSE, warning=FALSE}


taraban_crossval %>% 
  filter(!is.na(freq_plaut)) %>% 
  filter(epoch == 27) %>% 
  filter(plaut != 'nonword') %>%
  mutate(plaut_c = case_when(plaut == 'exception' ~ -.5,
                               plaut == 'reg_inconsistent' ~ .5,
                             plaut == 'reg_consistent' ~ .5),
         freq_plaut = case_when(freq_plaut == 'low' ~ -.5,
                                  freq_plaut =='high' ~ .5)) %>%
  lmer(loss ~ plaut_c*freq_plaut + (1|word) + (1|run_id), data = .) -> m_plaut2

summary(m_plaut2)

# runtime is about 25 minutes:
#m_plaut2_anova = car::Anova(m_plaut2, type = 3, test = 'F')
#save(m_plaut2_anova, file = 'data/m_plaut2_anova.Rmd')

load(file = 'data/m_plaut2_anova.Rmd')

```
@Plaut1996 included a number of statistical tests that aren't central to the demonstration here. Though one is worth considering both because it adds additional precision to the previous results and is important to note for theoretical reasons. In a model of the interaction between word regularity (regular versus exception - as was defined in @Plaut1996) amd frequency (low versus high), regular words are reliably produced with lower errors than exception words (*b* = -.02, *SE* = .003, *F*(1, 85) = 35.35, ). The effect of frequency maintains as in the previous model reported, as does the non-significance of the interaction term. This indicates that the LSTM network picks up more narrowly on the difference in complexity between words that can be characterized as regular (both consistent regulars and inconsistent regulars) and those that are exceptional (those that both can't be described via simple rules and those that have no neighbors).

The absence of a significant interaction between consistency and frequency here would require further experimentation, which would require analyses of loss and other relevant model behavior across training. @Plaut1996 included a range of experiments examining the variability in effects of including frequency manipulations of different kinds during training, showing that using condensed frequencies when training the network also condenses the effects related to those frequencies. It is quite possible that by expanding the frequency values used to scale gradients during training here would make the effects of frequency (whether raw or categorical) more dramatic. While this additional experimentation is outside the scope of the demonstration of the architecture here, it is worthy of future investigation in subsequent work.

In sum, the general trends relating consistency and frequency appear in data from the time-varying network architecture being proposed here. In statistical models of the experimental effects of words that vary systematically in terms of frequency and consistency, the results are mixed despite the appearance of very similar trends across the two variables from a descriptive account.

## Nonword reading
Looking at performance at the end of training, the network did quite well in pronouncing nonwords (taken from @Taraban1987; though see the footnote on @Plaut1996 p.62 for a discussion of the shortcomings of these stimuli). This is taken as a good measure of performance of models in the history of such work due to the fact that the words (should) bear a meaningful resemblance to real words and that in connectionist networks, where there are no "stored" entries of words, the ability to perform on such words evidences the ability of a rule-less system to generate reasonable pronunciations due to generalization from other similar forms.

```{r, plaut_nonwords, echo=FALSE}
taraban_testmode %>% 
  filter(plaut == 'nonword') %>%
  filter(phonemes_proportion < 1) %>% 
  select(word, phon_read)  %>% 
  filter(word != 'boad') %>% 
  pull(word) -> nonwords_pronounced_wrong

nonwords = taraban_testmode %>% 
  filter(word %in% nonwords_pronounced_wrong) %>% 
  select(Nonword = word,
         Correct = phon,
         Response = phon_read) 


apa_table(nonwords)

  
```

Of the `r length(nonwords)` from the @Taraban1987 set, `r length(nonwords_pronounced_wrong)` were pronounced incorrectly (i.e., produced something other than the target sequence of phonemes). This amounts to `r length(nonwords)/length(nonwords_pronounced_wrong)`% of words being pronounced incorrectly from this list (compare to `r 13/43`% incorrect from a similar list in @Plaut1996, but using a much simpler computational system). Like the errors on nonwords in @Plaut1996, the errors here are all reasonable attempts at a sequence of phonemes given the input orthographic pattern. These errors are shown in Table X \footnote{Note that the phonological coding for the nonword `r scaps('boad')` was incorrect and has been removed from this table}.

While performance on made up words will be discussed further in the subsequent section devoted to producing multisyllabic words, we can say a few things here about the performance on this set of words in a model of monosyllabic words. These errors, like those in @Plaut1996 are realistic in the sense that the phones produced approximate what you would expect for the orthographic input, but don't perfectly match the true sequence. For example, vowels are produced in the vowel position and consonants in the consonant positions. Additionally, the incorrect vowels and consonants produced are quite close to the proper segments in terms of their phonological similarity to the target segment. The responses for `r scaps('nowl')` involved a vowel with a postvocalic glide (`r  scaps('OW1')`), but ot the correct one. Likewise, `r scaps('dase')` and `r scaps('boose')` were incorrect because they produced incorrect voicing on the word-final fricative (either `r scaps('Z')` for `r scaps('S')` or vice versa).

All of the mistakes can be tied to the neighborhood characteristics of the words as well given that all nonwords here contain ambiguous orthography-to-phonology structure (i.e., their consistency is less than `1`). In this sense the responses can be seen as overgeneralization based on similar forms. Of the mistakes here, only `r scaps('dase')` and `r scaps('foth')` are surprising given the raw consistency counts (as body-rime calculations) of the neighborhoods involved. Most of the orthographic neighbors of `r scaps('dase')` are not associated with the rime produced here. `.67` of words with `r scaps('ase')` are associated with the rime `r scaps('EY - Z')`. Likewise, most of `r scaps('foth')`'s orthographic neighbors exhibit the lower unrounded vowel `r scaps('AA')` rather than the vowel produced by mistake (`r scaps('OW')`). This last error can be seen as an effect of an important aspect of the system, though, given that `r scaps('foth')` has a influential high frequency neighbor `r scaps('both')`.

Nonetheless, and consistent with the discussion in @Plaut1996, both humans and computational models produce "incorrect" phonological codes from orthography - though the notion of "correct" pronunciation for novel strings is not straightforward. Behavioral data have shown that reader's responses to nonwords often deviate from the expected production, notwithstanding questions about the proper production for a nonword is. For example, @Glushko1979 found that nonwords with inconsistent neighborhood structure often resulted in pronunciations that were different than the expected pronunciation based on grapheme-phoneme rules while being consistent with the pronunciation for that orthographic body in some other word (see @Plaut1996 for discussion). The issue of nonword naming will be returned to later in the discussion of a model that learns multisyllabic words.

# Glushko1979 nonwords
As an additional comparison the nonwords from @Glushko1979 were submitted for testing after training. That study was important for showing that words that appear to be pronouncable "by rule" (i.e, they are regular) that nonetheless have few if any neighbors, behave like other words that have sparse neighborhood structure, even when the word is a nonword. This was early evidence that a theory of reading that depends on rules of "phonological assembly" [@Rastle2010] and storage of exceptions isn't feasible given that nonwords can't be "retrieved" via a lexical storage procedure. Nonwords that appear to be regular (regardless of neighborhood structure) should behave similar to other regular words based on the rule-based ("assembly") account, but they do not. They behave in a way that is predictable based their neighborhood structure.

In experiments 1 and 2, @Glushko1979 organized the nonwords into two groups, which were labeled "regular" (e.g., `r scaps('bink')` and `r scaps('gobe')`) or "exception" (e.g., `r scaps('pild')` and `r scaps('tost')`).  The second experiment controlled for the possibility that real-word stimuli in the first experiment may have primed participants to read nonwords in a way that inappropriately conformed to the real words in the task, so a second naming experiment was run using exclusively nonwords. For our purposes here, words are collapsed across tasks, with accuracies reported for all nonwords across the study.

GLUSHKO NONWORD DATA HERE

# Jared (1997)
Behavioral results conveying the tradeoffs between frequency and consistency hold up for the @Jared1997 stimuli as well, with low frequency words in general being more difficult to process than high frequency words, and with inconsistent words being more difficult than consistent ones. This is a complementary study to the replications reported previously, focusing specifically on experiment 1 in @Jared1997 - a naming task. In that experiment participants read aloud sets of words (40 words to a list over two separate visits to the lab) that belonged to one of four categories. Words were either low frequency or high frequency and were consistent (all neighbors had the same rime) or inconsistent (neighbors displayed different rimes). For example, the words `r scaps('paid')`, `r scaps('said')`, and `r scaps('plaid')` are inconsistent words used in the experiment. As in other data reported here, these neighborhood characteristics were tested in the corpus of words used for these simulations and the characteristics were confirmed. Additionally, frequencies were set to high and low prior to training given the possibility that the frequencies used for gradient scaling differed from those consulted for the experiments in @Jared1997.

```{r, echo=FALSE}

jaredA_freq = taraban_crossval %>% 
  filter(epoch == 36) %>% 
  filter(!is.na(jaredA)) %>% 
  group_by(freq_jaredA) %>% 
  summarise(mse = mean(mse)) %>% 
  rename(condition = freq_jaredA)


jaredA_consistency = taraban_crossval %>% 
  filter(epoch == 36) %>% 
  filter(!is.na(jaredA)) %>% 
  group_by(jaredA) %>% 
  summarise(mse = mean(mse)) %>% 
  rename(condition = jaredA)

```

As in the behavioral data in the original paper, the model data support the tradeoffs between consistency and frequency. Taking data from 50 runs of the paradigm and using mean squared error as the metric (again early in training) ^[Note that the use of mean squared error rather than loss never affects the outcome; the effects in one always carry over to the other], low frequency words (mse = `r pull(filter(jaredA_freq, condition == 'low'), mse)`) are produced with greater error than high frequency words (mse = `r pull(filter(jaredA_freq, condition == 'high'), mse)`), and inconsistent words are higher in mse (`r pull(filter(jaredA_consistency, condition == 'inconsistent'), mse)`) than their consistent counterparts (`r pull(filter(jaredA_consistency, condition == 'consistent'), mse)`).


```{r, echo=FALSE, fig.cap=''}
COLORS_jared = c('Consistent'='grey86', 'Inconsistent'='black')

taraban_crossval %>% 
  filter(epoch == 36) %>% 
  filter(!is.na(jaredA)) %>%
  mutate(freq = case_when(freq_jaredA == 'high' ~ 'High',
                          freq_jaredA == 'low' ~ 'Low'),
         condition = case_when(jaredA == 'consistent' ~ 'Consistent',
                               jaredA == 'inconsistent' ~ 'Inconsistent'),
         condition = fct_relevel(condition, c('Inconsistent', 'Consistent'))) %>% 
  ggplot(aes(freq, loss, fill=condition)) +
  geom_bar(stat= 'summary', position = position_dodge(), color = 'black') +
  scale_fill_manual(values = COLORS_jared) +
  theme_bw() +
  labs(x = 'Frequency', y = 'Mean Squared Error') +
  theme(legend.title = element_blank())
  

```


The comparable figure reproduced from @Jared1997 is shown in Figure XX![Figure 1 from Jared (1997) with frequency on the x-axis, latency is displayed on the y-axis (with accuracy above each bar) and level of consistency shown with color across grouped bars.](img/jared1.png)


```{r, echo=FALSE}
taraban_crossval %>% 
  filter(epoch == 36) %>% 
  filter(!is.na(jaredA)) %>% 
  mutate(frequency = case_when(freq_jaredA == 'high' ~ .5, freq_jaredA == 'low' ~ -.5),
         consistency = case_when(jaredA == 'consistent' ~ .5, jaredA == 'inconsistent' ~ -.5)) %>% 
  lmer(mse ~ frequency*consistency + (1|word) + (1|run_id), data = .)  -> m_jared

#save(m_jared, file = 'data/m_jared.Rda')

#m_jared_anova = car::Anova(m_jared, type=3, test="F")
#save(m_jared_anova, file = 'data/m_jared_anova.Rmd')

#m_jared_confint = confint(m_jared)
#save(m_jared_confint, file = 'data/m_jared_confint.Rda')
load(file = 'data/m_jared.Rda')
load(file = 'data/m_jared_anova.Rda')
load(file = 'data/m_jared_confint.Rda')

x_names = c('Intercept', 'Frequency', 'Consistency', 'Frequency*Consistency')

cap = 'Mixed Effects Model Output for MSE Predicted by Consistency, Frequency, and its Interaction'

t_jared = model_to_table(m_jared, x_names, m_jared_confint, m_jared_anova, caption = cap, include_R_notes = TRUE)

t_jared

```

The effects found in the computational data are comparable but not identical to those from the original study. Using a linear mixed effects model with random slopes for words and simulations (because models were run for 50 separate train/test splits in order to avoid an idiosyncratic result from a single simulation), we see a significant effect of frequency and consistency, with no significant effect for the interaction (p-value calculated using the Anova method from the car library in R; @R2021).

When examining the effect of consistency at the level of high frequency words, as in @Jared1997 we see that high frequency consistent words show lower error than their low consistent counterparts, mirroring the earlier analagous effect in naming latency.

```{r, echo=FALSE}
taraban_crossval %>% 
  filter(epoch == 36) %>% 
  filter(!is.na(jaredA)) %>% 
  mutate(frequency = case_when(freq_jaredA == 'high' ~ 0, freq_jaredA == 'low' ~ 1),
         consistency = case_when(jaredA == 'consistent' ~ -.5, jaredA == 'inconsistent' ~ .5)) %>% 
  lmer(mse ~ frequency*consistency + (1|word) + (1|run_id), data = .) -> m_jared2

#save(m_jared2, file = 'data/m_jared2.Rda')

#m_jared2_anova = car::Anova(m_jared2)
#save(m_jared2_anova, file = 'data/m_jared2_anova.Rda')

#m_jared2_confint = confint(m_jared2)
#save(m_jared2_confint, file = 'data/m_jared2_confint.Rda')

load(file = 'data/m_jared2_anova.Rda')
load(file = 'data/m_jared2_confint.Rda')
#summary(m_jared2)
```



## Variable effects
It should be noted that effects are subjected to variability throughout training. For example, the plot below shows the interaction between consistency and frequency at several different training stages: 9, 18, 27, 36, 45, and 54, 63, and 72 epochs. The interaction looks stable across training for the most part (and is on average when collapsing across epochs), but at certain points in the development of expertise in the domain the effect may shift. For example, in epoch 54 error on consistent words is on average higher than on inconsistent words, though the trend for frequency at that point is similar to other epochs.

```{r, echo=FALSE}
COLORS_jared = c('Consistent'='grey86', 'Inconsistent'='black')


taraban_crossval %>% 
  filter(!is.na(jaredA)) %>%
  mutate(freq = case_when(freq_jaredA == 'high' ~ 'High',
                          freq_jaredA == 'low' ~ 'Low'),
         condition = case_when(jaredA == 'consistent' ~ 'Consistent',
                               jaredA == 'inconsistent' ~ 'Inconsistent'),
         condition = fct_relevel(condition, c('Inconsistent', 'Consistent'))) %>% 
  ggplot(aes(freq, mse, fill=condition)) +
  geom_bar(stat= 'summary', position = position_dodge(), color = 'black') +
  scale_fill_manual(values = COLORS_jared) +
  labs(x = 'Frequency', y = 'Mean Squared Error') +
  theme(legend.title = element_blank()) +
  facet_grid(~epoch) +
  theme_apa()

```

It is expected that the process of acquiring knowledge that is subject to constraints like those that operate on the stimuli here, you will see stochastic variability throughout learning, but with overall trends emerging when averaging across individuals or, in the case of the simulation here, across time.

As an alternative method of assessing model production, additional information about performance can be observed using the network's offline test mode. This method is dictated by a different sort of recurrent process that settles into a final state through and internally dictated mechanism rather than by the number of timesteps provided at the phonological input (see the previous section on the method for details). Summing the accuracy across phonological segments using this testing mechanism yields separate converging evidence about processing difficulty at a finer grain than that offered through mean squared error. Figure X shows the cumulative (sum) phoneme-wise distances for the @Jared1997 stimuli using production mode at the end of training, the trend of which is very similar to that seen in the previous analyses of these items.

```{r, echo=FALSE, warning=FALSE}


taraban_testmode %>% 
  filter(!is.na(jaredA)) %>% 
  mutate(freq = case_when(freq_jaredA == 'high' ~ 'High',
                          freq_jaredA == 'low' ~ 'Low'),
         condition = case_when(jaredA == 'consistent' ~ 'Consistent',
                               jaredA == 'inconsistent' ~ 'Inconsistent'),
         condition = fct_relevel(condition, c('Inconsistent', 'Consistent'))) %>% 
  ggplot(aes(freq, phonemes_sum, fill=condition)) +
  geom_bar(stat= 'summary', position = position_dodge(), color = 'black') +
  scale_fill_manual(values = COLORS_jared) +
  labs(x = 'Frequency', y = 'Sum of phoneme-wise distance') +
  theme_apa() +
  theme(legend.title = element_blank())

```



```{r, echo=FALSE, warning=FALSE}
taraban_crossval %>% 
  filter(!is.na(jaredA)) %>% 
  mutate(freq = case_when(freq_jaredA == 'high' ~ 'H',
                          freq_jaredA == 'low' ~ 'L'),
         condition = case_when(jaredA == 'consistent' ~ 'Consistent',
                               jaredA == 'inconsistent' ~ 'Inconsistent'),
         condition = fct_relevel(condition, c('Inconsistent', 'Consistent'))) %>% 
  group_by(epoch, freq, condition) %>% 
  summarise(mse = mean(mse),
            freq = first(freq),
            condition = first(condition)) %>% 
  ggplot(aes(freq, mse, fill=condition)) +
  geom_bar(stat= 'summary', position = position_dodge(), color = 'black') +
  facet_grid(~epoch) +
  scale_fill_manual(values = COLORS_jared) +
  labs(x = 'Frequency', y = 'Mean squared error') +
  theme_apa() +
  theme(legend.title = element_blank())



```


```{r, echo=FALSE, warning=FALSE}
taraban_crossval %>% 
  filter(!is.na(freq_plaut)) %>% 
  #filter(epoch %nin% c(9, 18)) %>% 
  filter(epoch == 54) %>% 
  filter(plaut != 'nonword') %>%
  mutate(plaut = case_when(plaut == 'ambiguous' ~ 'Ambiguous',
                             plaut == 'exception' ~ 'Exception',
                             plaut == 'reg_consistent' ~ 'Regular consistent',
                             plaut == 'reg_inconsistent' ~ 'Regular inconsistent'),
         freq_plaut = case_when(freq_plaut == 'low' ~ 'Low',
                                  freq_plaut =='high' ~ 'High'),
         freq_plaut = fct_relevel(freq_plaut, c('Low', 'High')),
         plaut = fct_relevel(plaut, c('Exception', 'Ambiguous', 'Regular inconsistent', 'Regular consistent'))) %>% 
  group_by(epoch, freq_plaut, plaut) %>% 
  summarise(loss = mean(loss)) %>% 
  ggplot(aes(freq_plaut, loss, shape = plaut, group = plaut)) +
  geom_line() +
  geom_point(size = 3) +
  scale_shape_manual(values = c(0, 1, 2, 5)) +
  labs(x = 'Frequency', y = 'Cross entropy') +
  facet_grid(~epoch) +
  theme_apa() +
  theme(legend.title = element_blank())

```


# Multisyllabic words
The process through which the model was trained on multisyllabic words did not differ from the one used with monosyllabic words, and the architecture is the same for both processes. All that differs is the training corpus. For this purpose, a corpus of `r nrow(multi_testmode)` words was assembled for training and testing. It is important to note that even though this model processes multisyllabic words, syllables are not explicitly marked as segments during processing. Stress is marked on vowels, and there are no perceptual markings in the representations (inputs or outputs) that divide any two syllables. Therefore the model, like humans, is required to learn about prosodic structure over sequences of sounds without feedback about what constitutes a syllable. In addition to the words already described for the multisyllabic corpus, words from @Chateau2003 were included in order to investigate experimental effects of frequency and consistency in multisyllabic words. The model was trained to 100 epochs with training and test data written out across the training process.

In general, the learning process mirrors that of monosyllabic words, with error decreasing across training, eventually setting into a stable state.

```{r, multiErrorAcrossTraining, echo=FALSE, fig.cap='Cumulative error (as MSE) across training for train and test set of multisyllabic words. Bars (barely visible) are calculated as the standard error of the mean.'}
N = length(unique(multi$word))

COLORS_tt = c('Train' = 'darkorchid', 'Test' = 'orangered3')

multi %>% 
  group_by(epoch, train_test) %>% 
  summarise(M = mean(mse),
            SD = sd(mse),
            SEM = SD/sqrt(N)) %>% 
  mutate(train_test = case_when(train_test == 'train' ~ 'Train',
                                train_test == 'test' ~ 'Test')) %>% 
  filter(epoch %in% c(9, 18, 27, 36, 45, 54, 63, 72, 81, 90)) %>% 
  ggplot(aes(epoch, M, group = train_test, color = train_test)) +
  geom_line() +
  geom_errorbar(aes(ymin=M-SEM, ymax=M+SEM), width=.1) +
  scale_color_manual(values = COLORS_tt) +
  labs(x = 'Epoch', y = 'Mean squared error', color = 'Train / Test') +
  theme_apa() +
  theme(legend.background = element_rect(color = 'black'))


```

Words with more syllables are associated with greater errors at the end of training, both using training mode (with phonological inputs provided during the calculation of loss) and in test mode.

```{r, trainmodeErrorsBySyllable, echo=FALSE}

multi %>% 
  filter(epoch == 99) %>% 
  group_by(nsyll) %>% 
  summarise(M = mean(mse),
            SD = sd(mse)) %>% 
  round(digits = 6) %>% 
  mutate(Syllables = as.character(nsyll),
         M = as.character(M),
         SD = as.character(SD)) %>% 
  select(Syllables, M, SD) %>% 
  apa_table()

  
```

The same trend comes out when looking at phonemewise errors across the word broken out by the number of syllables in each word. This measure provides a different and less graded measure of error, but useful still. Here, if the pattern that the network produces is closer to the correct phoneme than any other, it is considered correct. Once this prediction is made for the entire word, phoneme by phoneme, the calculation is made as the number of phonemes correct divided by the total phonemes in the word. Table XX shows the accuracy associated with syllable count across all words in the training corpus.

```{r, testmodeErrorsBySyllable, echo=FALSE}


multi_mistakes = multi_testmode %>% 
  filter(phonemes_proportion < 1) %>%
  select(word, phon_read, nsyll)

multi_nsylls = multi_testmode %>% 
  group_by(nsyll) %>% 
  summarise(N = n())


multi_mistakes_by_syllable = multi_mistakes %>% 
  group_by(nsyll) %>% 
  summarise(n = n()) %>% 
  left_join(multi_nsylls) %>% 
  mutate(Proportion = n/N) %>% 
  select(Syllables = nsyll, `Mistakes` = n, Proportion)


multi_testmode %>% 
  group_by(nsyll) %>% 
  summarise(M = mean(phonemes_proportion),
            SD = sd(phonemes_proportion)) %>% 
  rename(Syllables = nsyll) %>% 
  left_join(multi_mistakes_by_syllable) %>% 
  apa_table(caption = 'Descriptive Statistics of Errors for Number of Syllables in Each Word in Corpus', note = '"Mistakes" corresponds to the total number of mistakes made for words with that number of syllables, and "Proportion" expresses that number as a proportion of all words with that number of syllables. ')

multi_testmode %>% 
  filter(word %in% multi_mistakes$word) %>% 
  select(Word = word, `Pronunciation by model` = phon_read, `Correct pronciation` = phon) %>% 
  write_csv('data/multisyllabic_model_mistakes.csv')

```


## Mistakes
The types of mistakes made by the model vary, with `r nrow(multi_mistakes)` words out of `r nrow(multi_testmode)` (or `r nrow(multi_mistakes)/nrow(multi_testmode)`% of all words) pronounced incorrectly. Table XX shows a sample of the mistakes made for words of different lengths. A table of all errors can be found at the following online repository: XX^[Note that a segment coded as "XX" conveys that the segment was a tie with another phoneme and one coded as "_" means that the model produced a null phoneme (where units are rounded to the nearest whole).].


```{r, sampleMultiMistakes, echo=FALSE}
mistakes_to_spotlight = c()


mistakes_with_uncertain_phone = multi_testmode %>% 
  filter(str_detect(phon_read, 'XX')) %>% nrow()



mistakes_with_null_phone = multi_testmode %>% 
  filter(str_detect(phon_read, '_')) %>% nrow()

```


Some words are shortened with syllabic reduction, like the word `r scaps('snarled')`, which was trained with the target `r scaps('S-N-AA1-R-AH0-L-D')`, but which produced the single syllable alternative `r scaps('S-N-AA1-R-L-D')`. Other errors were related to stress pattern. For example, the word `r scaps('flatbed')` possesses the stress pattern 1-2 in the training data (where 1 is primary stress and 2 is secondary stress). The produced pattern of stress was 1-0 (i.e., the form `r scaps('F-L-AE1-T-B-EH0-D')`), which doesn't differ from the target in principle and out of context of reading connected text ^[Note that the stress codings were adopted from the CMU dictionary with little change. A vowel possessing a 0 stress should be thought of as "unstressed", and whether or not an unstressed syllable/ vowel is distinct from a syllable/ vowel with secondary stress is not a clear distinction in this case.]. Therefore the pattern produced possessed an explicit distinction between the primary stress (the first syllable) against the subordinate stress (the second syllable). That the stress pattern in this word is 1-2 is arguable. Other words in the training set that showed the 1-0 pattern with `r scaps('EH0')` include `r scaps('T-R-AW1-W-EH0-L')` and `r scaps('P-R-IH1-N-S-EH0-S')` can be thought of as having a weaker ultimate syllable (the unstressed `r scaps('EH0')`), but the extent to which the vowel is distinct from `r scaps('EH2')` (the version displaying secondary stress) is subtle, if not negligible.

A number of mistakes arise from the model either being uncertain about a phoneme (resulting in a tie among phonemes produced; `r mistakes_with_uncertain_phone` total mistakes, or `r mistakes_with_uncertain_phone/nrow(multi_mistakes)`% of mistakes) or producing a null phoneme (i.e., a segment whose nearest phoneme is the null `_` segment, consisting of all units set to zero), which was `r mistakes_with_null_phone`, or `r mistakes_with_null_phone/nrow(multi_mistakes)` of all mistakes. While a full accounting of this type of error is outside the scope of what will be described here, the error data can be found in this repository (XX). The conditions under which mistakes of this type could be avoided is a matter of future research, but it likely could be avoided through some combination of an expanded training corpus and enhanced training for complex words. Remember that training can proceed with a batch size equal to one, which likely would ameliorate such errors.

```{r, multiStressMistakes, echo=FALSE}

multi_testmode %>% 
  filter(stress < 1) %>% 
  pull(word) -> multi_stress_mistakes


multi_testmode %>% 
  filter(stress < 1) %>% 
  filter(str_detect(phon_read, 'XX') | str_detect(phon_read, '_')) %>% 
  pull(word) -> multi_stress_indeterminate_pronunciations


multi_testmode %>% 
  filter(stress < 1) %>% 
  filter(word %nin% multi_stress_indeterminate_pronunciations) %>% 
  select(word, phon, phon_read) -> tmp



```

Of the mistakes made, `r length(multi_stress_mistakes)` (or `r length(multi_stress_mistakes)/nrow(multi_testmode)`%), involved a mistake associated with the stress pattern. Some stress mistakes relate back to the fact that the model sometimes produces indeterminate phoneme segments (ties between phonemes, or the undetermined '_' segment). `r length(multi_stress_indeterminate_pronunciations)` (`r length(multi_stress_indeterminate_pronunciations)/length(multi_stress_mistakes)` of all stress errors) can be attributed to such errors. Of those remaining, some mistakes can be attributed to an improper assignment of stress despite the proper approximate contour being present (for example, the stress assigned to `r scaps('reindeer')` incorrectly supplied no stress to the second vowel but in training secondary stress was present). The remaining mistakes involved the wrong stress (e.g., a stressed second syllable present when the first syllable should have been stressed, like in `r scaps('JH-AE1-P-AH0-N')` for `r scaps('Japan')` and `r scaps('	SH-AE1-M-P-UW0-Z')` for `r scaps('shampoos')`) or the number of vowels being incorrect (e.g, `rscaps('F-AH1-N-IH0-S-T')` instead of `rscaps('F-AH1-N-IY0-AH0-S-T')` for `r scaps('funniest')`). Although, such mistakes account for `r nrow(filter(filter(multi_testmode, word %nin% multi_stress_indeterminate_pronunciations), stress <1))` words in the corpus and only `r nrow(filter(filter(multi_testmode, word %nin% multi_stress_indeterminate_pronunciations), stress <1))/length(multi_stress_mistakes)`% of total stress mistakes.


## Comparisons to other datasets
In order to validate the model using words with more than one syllable, two analyses are presented. First, a set of relevant predictors taken from @Yap2009 are examined. Then, experiments that examine the effects of consistency in multisyllabic words are reported, namely those from @Chateau2003 and @Jared1990.


## Item level predictors from the ELP [@Balota2007; @Yap2009]
Data from the English Lexicon Project were taken and merged with the items from the data from the simulations involving multisyllabic words. A set of standard variables were identified for their applicability to examining the effects of orthography-to-phonology structure on naming: word frequency, orthographic length (number of letters), phonological length (number of phonemes), number of syllables, orthographic neighborhood size, and phonological neighborhood size. The set of words common to both the ELP naming data and the data from the multisyllabic simulations conducted here were subset for the purposes of examining these word-level variables. The descriptive characteristics of the predictors are shown in Table XX.

```{r, multiWordPredictorsDescriptives, echo=FALSE}


tmp = multi %>% 
  filter(epoch == 99)

predictors =  c('Frequency', 'N syllables', 'Phon length', 'Orth length', 'Orth Lev.', 'Phon Lev.')

tabledata = summarows(tmp, c('freq', 'nsyll',  'phonlen', 'orthlen', 'orth_lev', 'phon_lev'), newnames = predictors, na.rm = T, manuscript = T, notes = 'Frequency values provided from HAL, which is one of the frequency sources native to the ELP data. Levenshtein distances (orth lev. and phon lev.) were taken from the ELP. Levenshtein distance is calculated as the minimum number of character changes in the sequence that are needed in order to generate a new word (orthographic form for OLD and phonological form for PLD) from the existing one.')


elp_model_words = multi %>% 
  filter(epoch == 99) %>% 
  filter(!is.na(elp_rt)) %>% 
  pull(word)


tabledata
```

In order to obtain an estimate of the relationship between word variables and a relevant performance measure for behavior (from ELP) and the computational model the data were subset to include all words present in both the model corpus and the data from the ELP. This yielded data for `r length(elp_model_words)` words in the model corpus (or `r length(elp_model_words)/length(unique(multi$word))`% of words included in the training pool). These variables were each correlated with the relevant outcome variable from the model and in the ELP. For the model data the mean squared error was used, and for the ELP, the item-level RTs.

```{r, corsMultiELP, echo=FALSE}

EPOCH = 45

multi %>% 
  filter(!is.na(elp_rt)) %>% 
  filter(epoch == EPOCH) -> tmp

corvars = c('freq', 'nsyll', 'phonlen', 'orthlen', 'orth_lev', 'phon_lev')
header = c('Predictor', '*r* (Model)', '*r* (ELP)')
tabledata = data.frame(matrix(nrow = length(predictors), ncol = length(header)))
names(tabledata) = header
tabledata$Predictor = predictors

tabledata$`*r* (Model)` = sig_level(cortests(tmp, 'mse', corvars, 'estimate'), cortests(tmp, 'mse', corvars, 'p.value'))
tabledata$`*r* (ELP)` = sig_level(cortests(tmp, 'elp_rt', corvars, 'estimate'), cortests(tmp, 'elp_rt', corvars, 'p.value'))


apa_table(tabledata, caption = 'Correlations between Word Variables and Processing Difficulty (MSE for Model and RT for Behavioral Data)', note = "Correlations provided as Pearson's *r*. Bolded coefficients are significant at *p* < .001.", digits = 3, format = 'pandoc', align = 'l', landscape = T)

```

The bivariate relationships presented above, and their direction, are all expected and they are approximately consistent across the model and behavioral data. An increase in frequency is associated with a decrease in processing difficulty (error in the model, latency in humans). The remaining word variables are all positively correlated with processing difficulty: number of syllables, number of phonemes, number of letters, orthographic Levenshtein distance, and phonological Levenshtein distance.

Similar to the results of the model of monosyllabic words, the more important test of the model's performance concerns the effect of syllabic characteristics on naming in experiments that examine such characteristics in a more controlled manner.

There is a debate in the word recognition literature about whether something like _syllabic processing_ is a necessary component of word recognition with, of course, particular attention paid to longer (multisyllabic) words in determining this level of representation in the reading system. @Jared1990 posited that the processing of multisyllabic words can't be reduced to a syllable-by-syllable process. Their data from Experiment 4 show that when words are delivered one syllable at a time, the magnitude of the effect of inconsistency on naming increases, demonstrating that the context (across syllable boundaries) facilitates their processing. This is expected in a theory where the processing of syllables is simply part of a broader system that associates speech from print by exploiting the statistical (contextual) dependencies in orthography (i.e., a connectionist theory). If syllables were "assembled", you would expect to see left-to-right syllabic processing without regard to the (orthographic context of) the subsequent syllable in experimentation. @Chateau2003 extended the work in @Jared1990 with experimental results that showed that orthographic segments that extended across traditionally understood orthographic "syllable boundaries"` (i.e., intervocalic prosodic contours characterized by a dramatic reduction and shift in amplitude).

Taking error data from the end of training (using testmode data) we see a similar pattern of means, shown in Figure XX.

```{r, jared1990a, echo=FALSE}

multi_testmode %>%
  filter(!is.na(js1990_rt)) %>%
  mutate(condition = case_when(js1990_condition == 'reg_inconsistent' ~ 'Inconsistent',
                               js1990_condition == 'exception' ~ 'Inconsistent',
                               js1990_condition == 'regular' ~ 'Consistent'),
         syllable = case_when(js1990_syll == 'first' ~ 'First',
                              js1990_syll == 'last' ~ 'Last')) %>% 
  group_by(condition, syllable) %>%
  summarise(M = mean(phonemes_sum),
            SD = sd(phonemes_sum),
            SEM = SD/sqrt(N),
            condition = first(condition), 
            syllable = first(syllable)) %>% 
  ggplot(aes(syllable, M, fill = condition)) +
  geom_bar(stat = 'summary', position = position_dodge(), color = 'black') +
  geom_errorbar(width = .2, color = 'grey25', aes(ymin = M-SEM, ymax = M+SEM), position = position_dodge(.9)) +
  scale_fill_manual(values = COLORS_jared) +
  labs(x = 'Syllabic position', y = 'Phonemewise distance (avg)') +
  theme_apa() +
  theme(legend.title = element_blank())

```


## Chateau & Jared (2003)
The experiments in @Chateau2003 sought to investigate consistency and frequency effects in naming tasks with word-level manipulations. Extending the consistency constructs used in @Jared1990 and @Taft1992. The idea of consistency articulated in this work concerns a portion of the printed word that the authors call the "basic orthographic syllabic structure" (or "BOSS" for short). The idea behind this unit of representation is that it is the portion of the printed form that includes the first orthographic vowel plus all the orthographic consonants that lead up to the second orthographic vowel of the word, provided those orthographic consonants occur in word-ending positions in other (monosyllabic) words in English^[This terminology is based in the description of the "basis orthographic syllabic structure" in @Taft1992. It should be noted that the concepts "orthographic vowel" and "orthographic consonant" are problematic in that it assumes some fundamental homology between sounds and letters that runs against the ideas behind the connectionist theory of learning presented here. These terms are used for convenience.]. The orthographic "body" they define is the sequence of letters that span from the first orthographic vowel to last orthographic consonant defined in this way. Take the word `r scaps('blemish')`. Using this heuristic, the basic orthographic syllabic structure would be defined as `r scaps('blem')`, and its body would be `r scaps('em')`. Note that, importantly, this definition avoids the issue of where the "syllable boundary" is with respect to the orthography of the word and thus allows for a calculation of the neighborhood statistic (consistency) over such a unit with respect to the phonemes associated with the orthographic form because we can assume that such an orthographic sequence would be pronounced `r scaps('B-L-EH-M')``, despite it not being a word one would regularly encounter.

It should be noted that there is a theoretical assumption that underlies this orthographic segment as described in @Taft1992 that runs contrary to ideas about learning espoused in this dissertation. @Taft1992 articulates ideas about syllabic structure in orthography, which doesn't square with the theoretical assumption that orthographic units aren't processed in syllabic segments. It also assumes that orthography contains homologous structure to that of spoken language syllabic structure. The computational model developed here and the theory of cross-modal perceptual processing conveys a different assumption: that printed words are processed in a continuous fashion (over some grain size) and not syllable by syllable, just like spoken language. The ideas about the syllabic structure of orthography in @Taft1992 are indeed contested [@Chateau2003; @Jared1990], and the empirical findings about the nature of the basic orthographic syllabic structure have been challenged experimentally [@Seidenberg1987]. More generally, the extent to which syllables (like other subcomponents of words) function independently in as perceptual subcomponents has been debated as well [@Seidenberg1987].

Nonetheless, the experiments in @Chateau2003 are useful because their stimuli account for the fact that orthographic patterns can be associated with spoken language segments at a variety of orthographic grain-sizes. They found that consistency measures across a variety of grain sizes impacted naming latencies, including at levels of orthographic grain that suggest that printed words aren't decomposed syllable by syllable during reading (similar to the finding of @Jared1990). Like @Jared1990, they show that processing difficulty is moderated by word frequency, but show that even high frequency multisyllabic words show an effect of neighborhood consistency. This finding is consistent with the analogous study of monosyllabic words [@Jared1997].

The patterns of means in experiments 3 and 4 in @Chateau2003 are seen in the model data here. Figure XX shows the data from the two experiments on top and the corresponding data from the computational model on bottom using error data during training as the outcome variable.

![Model (top) and analagous behavioral (bottom) data from experiments 3 and 4 of Chateau & Jared (2003). Errorbars are given as standard error of the mean.]("img/chateau_figure.png")



## Variable effects over time
It should be noted again that effects of expected word-level characteristics on error shift over time in the computational model's performance. This variability isn't typically reported in analogous computation work related to reading development, but it bears mention here. Figure XX shows the patterns of means for the interaction between consistency and frequency for a range of timepoints across training using the @Chateau2003 experimental stimuli (their Experiment 3).

```{r, cj2003AcrossTraining, echo=FALSE, fig.cap='The interaction between frequency and consistency for a range of timepoints across training. Mean squared error has been standardized within each epoch in order to put barplots on a common scale (i.e., in terms of standard deviations of MSE).' }
N = multi %>% filter(epoch == 45) %>% filter(!is.na(chateauB_consistency)) %>% nrow()

COLORS_chateau = c('High' = 'grey86', 'Low' = 'black')

tmp = multi %>% 
  filter(!is.na(chateauB_consistency)) %>% 
  mutate(consistency = case_when(chateauB_consistency == 'low' ~ -.5,
                                 chateauB_consistency == 'high' ~ .5),
         frequency = case_when(chateauB_frequency == 'low' ~ -.5,
                               chateauB_frequency == 'high' ~ .5)) %>% 
  group_by(epoch) %>% 
  summarise(SD = sd(mse),
            M = mean(mse))


multi %>% 
  left_join(tmp) %>% 
  filter(!is.na(chateauB_consistency)) %>% 
  filter(epoch %in% c(45, 54, 63, 72, 81, 90, 99)) %>% 
  mutate(consistency = case_when(chateauB_consistency == 'high' ~ 'High',
                                 chateauB_consistency == 'low' ~ 'Low'),
         frequency = case_when(chateauB_frequency == 'high' ~ 'H',
                               chateauB_frequency == 'low' ~ 'L')) %>% 
  group_by(epoch, word) %>% 
  summarise(Z = (mse-M)/SD,
            consistency = first(consistency),
            frequency = first(frequency)) %>%
  ungroup() %>% 
  ggplot(aes(frequency, Z, fill = consistency)) +
  geom_bar(stat = 'summary', position = position_dodge()) +
  facet_grid(~epoch) +
  scale_fill_manual(values = COLORS_chateau) +
  labs(x = 'Frequency', y = 'Mean squared error', fill = 'Consistency') +
  theme_apa()

```

However, using the standardized outcome variable (mean squared error here) within each training cycle and averaging across training epochs you see the expected pattern emerge (Figure XX). This can be thought of as averaging across a (nonindependent) sample of model states to observe the relationship between consistency (defined in a certain way here for multisyllabic words) and frequency on error during training. This is the expected trend, but future work could be done to capture more precise characteristics of these dynamics especially as they relate to the behavior of cognitive computational systems like this one and the analogous behavior in humans.

```{r, cj2003AveragedAcrossTraining, echo=FALSE, fig.cap='The interaction between frequency and consistency', fig.height=3, fig.width=5}


tmp = multi %>% 
  filter(!is.na(chateauB_consistency)) %>%
  group_by(epoch) %>% 
  summarise(SD = sd(mse),
            M = mean(mse))



multi %>% 
  left_join(tmp) %>% 
  filter(!is.na(chateauB_consistency)) %>% 
  mutate(consistency = case_when(chateauB_consistency == 'low' ~ 'Low',
                                 chateauB_consistency == 'high' ~ 'High'),
         frequency = case_when(chateauB_frequency == 'low' ~ 'Low',
                               chateauB_frequency == 'high' ~ 'High')) %>% 
  filter(epoch %in% c(45, 54, 63, 72, 81, 90)) %>% 
  group_by(epoch, word) %>% 
  summarise(Z = (mse-M)/SD,
            consistency = first(consistency),
            frequency = first(frequency)) %>%
  ungroup() %>% 
  group_by(frequency, consistency) %>% 
  summarise(Z = mean(Z),
            frequency = first(frequency), 
            consistency = first(consistency)) %>% 
  ggplot(aes(factor(frequency), Z, color = factor(consistency), group = consistency)) +
  geom_point() +
  geom_line() +
  scale_color_manual(values = COLORS_chateau) +
  labs(x = 'Frequency', y = 'MSE (standardized)', color = 'Consistency') +
  theme_apa()

```

\newpage
