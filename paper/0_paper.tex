% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  american,
  man,floatsintext]{apa6}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
  \usepackage{amssymb}
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Time for time: In search of a veridical model of time-varying processes in word reading},
  pdfauthor={Matt Cooper Borkenhagen1},
  pdflang={en-US},
  pdfkeywords={reading, language development},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\begin{center}\begin{threeparttable}}
%   {\end{threeparttable}\end{center}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\begin{center}\begin{ThreePartTable}}{\end{ThreePartTable}\end{center}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% Overwrite redefinition of paragraph and subparagraph by the default LaTeX template
% See https://github.com/crsh/papaja/issues/292
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\itshape\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

% \usepackage{etoolbox}
\makeatletter
\patchcmd{\HyOrg@maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\HyOrg@maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother
\shorttitle{FALSE}
\keywords{reading, language development\newline\indent Word count: XXX}
\usepackage{csquotes}
\usepackage[titles]{tocloft}
\cftpagenumbersoff{table}
\renewcommand{\cfttabpresnum}{\itshape\tablename\enspace}
\renewcommand{\cfttabaftersnum}{.\space}
\setlength{\cfttabindent}{0pt}
\setlength{\cftafterloftitleskip}{0pt}
\settowidth{\cfttabnumwidth}{Table 10.\qquad}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\PassOptionsToPackage{x11names}{xcolor}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage[normalem]{ulem}
\ifxetex
  % Load polyglossia as late as possible: uses bidi with RTL langages (e.g. Hebrew, Arabic)
  \usepackage{polyglossia}
  \setmainlanguage[variant=american]{english}
\else
  \usepackage[shorthands=off,main=american]{babel}
\fi
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
  % set entry spacing
  \ifnum #2 > 0
  \setlength{\parskip}{#2\baselineskip}
  \fi
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\title{Time for time: In search of a veridical model of time-varying processes in word reading}
\author{Matt Cooper Borkenhagen\textsuperscript{1}}
\date{}


\authornote{

Correspondence concerning this article should be addressed to Matt Cooper Borkenhagen, 1202 West Johnson Street, Madison, WI 53705. E-mail: \href{mailto:cooperborken@wisc.edu}{\nolinkurl{cooperborken@wisc.edu}}

}

\affiliation{\vspace{0.5cm}\textsuperscript{1} Department of Psychology, University of Wisconsin, Madison}

\abstract{
Dissertation
}



\begin{document}
\maketitle

\begin{verbatim}
## Warning in library(package, lib.loc = lib.loc, character.only = TRUE,
## logical.return = TRUE, : there is no package called 'tidytext'
\end{verbatim}

\begin{verbatim}
## [[1]]
## [1] TRUE
## 
## [[2]]
## [1] FALSE
## 
## [[3]]
## [1] TRUE
## 
## [[4]]
## [1] TRUE
## 
## [[5]]
## [1] TRUE
## 
## [[6]]
## [1] TRUE
\end{verbatim}

\hypertarget{chapter-1-introduction}{%
\section{Chapter 1: Introduction}\label{chapter-1-introduction}}

\hypertarget{overview}{%
\subsection{Overview}\label{overview}}

Words are read by virtue of perceptual and cognitive processes that are distributed across space and time. As a reader engages visually with a word printed on the page, the reader's eyes move by fixation in semi-systematic ways across portions of printed word, tapping her or his knowledge of speech, print, and meaning. Historically, cognitive accounts of word reading have been focused on the simplest of words of English - those with one syllable (perry2010, rastle2000jml, sibley2010, yap2009), especially in the connectionist learning literature (plaut1996, seidenberg1989, sibley2010). This focus results from two primary factors. First, any model of word reading must at the very least account for the development of knowledge of the simplest words in the language given the child develops the ability to read these words first (ref.). Second, accounting for words that are longer than one syllable requires the specification of a learning architecture that accounts for time-varying aspects of learning that must be happening over longer words. This includes but is not limited to variability in prosodic information involved in pronunciation over different syllables (especially of vowels and their associated patterns of stress).

This dissertation reports on a model of word reading (that associates print to speech) that can accommodate words that are not bounded by orthographic or phonological length - that is, of words that are not limited to a single syllable. This is a significant advance from previous models in that the types of words it can recognize (printed and spoken) resemble those that are naturalistic in terms of the word environments that children (and adults) learn within.

The remaining chapters of this dissertation are organized in the following way. First, in the remaining portion of Chapter 1, I will introduce the central issues in word reading as they relate to the type of sequential processing that is essential to the theory developed in the dissertation. In Chapter 2, a history of models of word recognition will be provided. Chapter 3 will develop the architecture and corresponding theory of naming that represents the core topic of the dissertation. Chapter 4 will provide several experiments using the architecture described in Chapter 3, forming the behavioral support validating the architecture. Chapter 5 will be the conclusion, which will discuss the theory in light of its limitations and future directions.

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Language comes and goes in a time-contingent fashion. The language that we see and hear is structured by sequential statistics that are processed through processes that happen across time. Among other important perceptual and cognitive aspects of language processing, letters are processed visually according to patterns of fixation that occur in a temporally governed manner, auditory processes take place relative to sound patterns that appear in a fast fading way over time. As a result, our theoretical models should account for the fact that such processing takes place in a sequentially arranged, time-contingent manner. This is especially true of reading given that the activity involves several such processes, each of which is distributed across time.

Prevailing models of word reading have dealt with temporal aspects of processing in three ways: (1) through the use of networks that have a continuous recurrent component to their architecture (harm1999, harm2004), (2) implicit implementations with the use of feedforward networks that possess a component of the architecture dedicated to stress (perry2010), or (3) using simple recurrent networks (sibley2010). How time factors into word reading is typically discussed in the literature as concerning the processing of syllables: models of word reading can either deal with polysyllabic words or not. This distinction makes sense at one level, given that syllables are an important unit of speech. They carry information about vowel stress, and are an organizational category of language, contributing to the understanding of how consonants and vowels cluster within and across words. However, sequential statistics relevant to long words go beyond structure related to syllables. For example, orthotactic patterns exist as a matter of linguistic structure not strictly tied to syllabic structure.

In a separate research tradition, several architectures have been proposed in the engineering literature in order to simulate the relationship between text and speech for technological and commercial ends.

\hypertarget{chapter-2-a-brief-history-of-models-of-word-reading}{%
\section{Chapter 2: A Brief History of Models of Word Reading}\label{chapter-2-a-brief-history-of-models-of-word-reading}}

\hypertarget{representational-units}{%
\subsection{Representational units}\label{representational-units}}

\hypertarget{models-of-multisyllabic-word-reading}{%
\subsection{Models of multisyllabic word reading}\label{models-of-multisyllabic-word-reading}}

The CDP++ model from Perry et al.~(2010) posits a complex architecture with subcompnents that capture different aspects of the theory from which the computational architecture is based. Said differently, their system lacks architectural homogeneity, a hallmark of connectionist learning systems. Some assumptions of the system violate primitives of learning seen in naturalistic environments. For example, CDP++ requires the prespecification of a graphemic level of representation, based on the assumption that letter sequences map cleanly onto phonemic sequences, and that such representational units contribute to low level operations within the reading system. Graphemes are identified on the input layer based on their frequency, This violates an important aspect of development of word reading knowledge in that such representations (i.e., the ways that print sequences relate to auditory sequences) are learned and do not preexist in the learner once knowledge about print begins to develop.

In a connectionist architecture something like a grapheme may well exist, but it is an emergent aspect of the knowledge of the system that develops as the result of the exposure of the learning architecture to the environment.

The selection of representational units is a critical issue of theoretical interest in the development of a learning architecture that learns to read words. A fundamental question will always be: how do such representational units arise in learning? This issue plays even larger in models of word reading that deal in long words given that the longer the words are the more representational units there are to account for. In symbolic approaches this has been taken up historically as an issue of positing rules of identification, as in Rastle and Coltheart (2000). These rules take the form of complex verbal descriptions that specify structured labels for representational units based on the presence or absence of structural elements in a printed or spoken wordform.

\hypertarget{statistical-learning}{%
\subsection{Statistical learning}\label{statistical-learning}}

\hypertarget{survey-of-previous-architectures}{%
\section{Survey of previous architectures}\label{survey-of-previous-architectures}}

\hypertarget{simple-recurrent-networks}{%
\subsection{Simple recurrent networks}\label{simple-recurrent-networks}}

The most comprehensive application of an SRN to date is Sibley et al.~(2010), which used a two-component architecture for orthography and phonology, each with an SRN operating under the hood.

The most recent of these architectures can be found in (\textbf{Luthra2020?}), which they refer to as VOISeR. This model of orthography-to-phonology processing takes in an orthographic sequence (a spatially represented)

\hypertarget{serial-effects-in-naming}{%
\subsection{``Serial'' effects in naming}\label{serial-effects-in-naming}}

The evidence most commonly taken to suggest that there is some type of serial processing operating during naming, particularly for words that contain ``irregular'' pronunciations, concerns the timecourse of naming processes for such words.

Such findings are typically identified as evidence for a dual-route process operating during reading. The dual-route theory predicts that

\hypertarget{strategic-control}{%
\subsection{Strategic control}\label{strategic-control}}

There have been a number of studies concerning so-called ``extra-stimulus'' factors that concern processes in naming (see (\textbf{kello2003?}) for a review), which have been described broadly as strategies for cognitive control in word naming. The most important of these for the present work has to do with the time-course of processing printed words.

In behavioral research on aspects of control in word reading, the method of control is typically manipulated by providing special instructions to the participant, or constructing a paradigm to elicit a control-type mechanism when processing a visually presented word. Most research on this topic and using experimental paradigms along these lines has concerned short, or monosyllabic words. This isn't surprising given the rich history of such words in theories of word recognition (discussed elsewhere here), and that characteristics of the stimuli have driven theorizing and experimentation; monosyllabic words offer natural constraints to theory building that are useful to the endeavor, and provide simplifying case-studies of most critical characteristics observed in words more broadly (see (\textbf{yap2009?}) for discussion).

\hypertarget{behavioral-data}{%
\section{Behavioral data}\label{behavioral-data}}

There are a number of possibilities for the visual span over which visual information is passed in the process of initiating phonology in word reading.

Earlier connectionist models avoided this issue in favor of simplified feedforward networks that typically passed visual information to phonology in a single temporal step. Alternative implementations using recurrent backpropagation through time were also used (ref.) These networks function with a continuous, recurrent component, over temporal intervals (``ticks'' in that literature) where the input pattern (in a feedforward network) is allowed to stay clamped to the input layer, allowing for gradual buildup of the input signal over some time period. During training, error signal is backpropagated over timesteps where inputs are gradually being increased up to their eventual full activation state. However, architectures with recurrent backpropagation along these lines are simulating the timecourse of activation in a way that is different than networks that take time-distributed input patterns, like LSTM architectures and, more specifically, the architecture laid out in the current work. Rather than simulating the ways that sequentially-arranged portions of perceptual information play in cross-modal mappings (where the perceptual information unfolds in a time-distributed manner), these earlier implementations of continuous processes that operate over some time were developed in order to understand the types of disordered behavior seen, for example, in patients with brain injuries that impact lexical or semantic processing (Plaut, McClelland, Seidenberg, and Patterson (1996), Rogers et al. (2004)).

\hypertarget{chapter-3-architecture-of-a-computational-learner}{%
\section{Chapter 3: Architecture of a computational learner}\label{chapter-3-architecture-of-a-computational-learner}}

\hypertarget{words-used-for-training-and-testing}{%
\section{Words used for training and testing}\label{words-used-for-training-and-testing}}

The perceptual span within which letters can be clearly identified on a single fixation is approximately eight letters (ref.). For this reason, words of eight letters or less were compiled from the previously discussed sources for the purposes of training and testing the models developed here. It is important to note, however, that the model is capable to processing any word regardless of its length. A demonstration of this capacity is included in the results. This is important given that while a reader - a child, for example - may only have learned words of shorter lengths, the reader nonetheless has the capacity to attempt to read words of longer lengths. This is one of many differences between the time distributed computational process proposed here and other connectionist models proposed elsewhere. Feedforward systems require pre-specification about word length that prohibit generalization to other forms that fall outside the window of what has been specified.

Words were aggregated from a corpus of 250 commonly read children's books for children five years-old and younger in the United States (\textbf{lewis2021?}). This corpus was used given that a word's presence in such a database indicates that a child is likely to be exposed to it early in literacy development, at least in the US. A word was included for training if it was XX letters or less, XX phonemes or less, and XX syllables or less. These constraints were included for the training set in order to avoid length effects that were outside the focus of the analyses presented here. However, the capacity of the model to accommodate longer words than those included for training is demonstrated. Also, training can happen over words (or orthographic/ phonological sequences) of unlimited length, this length was just limited in order to focus the learning process on reasonably lengthed words from this realistic database of child-oriented language.

Practically speaking, this has the effect of including XX\% of the total words from the database, or XX of XX total words from the children's corpus from (\textbf{lewis2021?}). Figure XX depicts this inclusion criteria visually.

A number of outlier words were removed from the set due to their idiosyncratic characteristics. Words removed if they were abbreviations (``Nov'') or initialisms (``bbq''), which constituted XX\% (XX total words) from the child language source data. Additionally, children's books contain language that is quite different from words commonly read in other genres. These words include printed expressions of motherese (``aaaaa''), sing-song (``yipiyuk''), or forms of onomatopoeia (``zoooooom,'' ``wooshee'') that aren't well-suited for modeling the type of word reading attempted here. Single-letter words were removed as well, given that there are relatively few of these words (even though the architecture is equipped to handle such words).

Once collected, all words were transcribed using the CMU pronunciation dictionary implemented in Python with the Natural Language Toolkit (nltk) library (ref.). In order to increase the training set size, an in an effort to do so in a realistic manner, morphological variants were added to the corpus for words that possessed common variants. Sometimes, this applied to words that only appeared in their morphologically complex form in the children's book corpus, like ``adults'' (where the bare form ``adult'' was added), on other times this involved adding a morphologically complex form to the set where the simplex form was present in the children's book corpus (e.g., ``buyers'' for the word ``buyer'').

\hypertarget{architecture}{%
\section{Architecture}\label{architecture}}

In this section, an initial high-level description of the computational learner is provided, and is followed by a more technical description of how the architecture operates functionally and mathematically.

The architecture takes in a time distributed sequence of letters (one timestep per letter) and passes the representation of that sequence to the phonological portion of the network. This orthographic representation is composed of two separate vectors (the last hidden state of the orthographic pattern learned along with the \emph{cell state}, described in greater detail later), and serves as the initial memory state for learning about the time-distributed phonological sequence corresponding to the visual pattern learned. This should be thought of as the orthographic \emph{context} within which a phonological word is learned. Importantly, while the nature of the temporal dynamics around how this representation is passed has the potential to be manipulated, in the experiments reported here the representation is built up over an entire orthographic input pattern before being passed to phonology. While the timecourse of how visual information activates phonological information is unclear based on behavioral research, the information flow in this architecture approximates what we believe to be taking place at least in shorter words, but is used during all learning in the reported experiments.

Once the phonological network receives its initial state from orthography, it begins processing the corresponding phonological pattern to the orthographic input it has just taken in (i.e., a sequence of phonemes representing the spoken form of a word). The time-varying way in which phonology is processed is very similar to that of orthography. In training trials that take in a phonological input along with the orthographic input (a teaching signal where the visual and auditory inputs are provided on the same learning trial), the phonological input is represented with one phoneme per timestep. The sequence of phonemes builds up a representation and is mapped to the same phonological sequence at the output, having also integrated the orthographic representation passed as the initial cell state of the phonological LSTM layer.

Note that phonemes are used for convenience; future extensions of the architecture proposed here should experiment with more sophisticated representations of spoken language. This could include, for example, higher-definition raw digitized representations of spoken language. This architecture would allow for such representations of speech despite phonemes being used for simplicity here, and such an extension would be vaulable given the known role that learning about print has on spoken language knowledge {[}ref.{]}.

A word's the phonological input and output patterns differ in one important way. The phonological input pattern contains a special representation at the beginning of the word (i.e., as the first timestep) and the corresponding output pattern contains an end of word representation as the final timestep (but not the beginning-of-word representation). For example, the word \emph{breath}, would correspond to the sequence of phonemes \texttt{B\ -\ R\ -\ EH\ -\ TH} (see Appendix XX for the CMU pronunciation dictionary representations used). Though, the input pattern for this spoken word would be \texttt{\#\ -\ B\ -\ R\ -\ EH\ -\ TH}, with an output pattern of \texttt{B\ -\ R\ -\ EH\ -\ TH\ -\ \%}. The word initial representation (\texttt{\#} in the example) allows the phonological portion of the network to be trained with knowledge of when a word should be initiated during the production of the spoken wordform corresponding the orthographic input provided, and when the production of the phonological form should be terminated (i.e., \texttt{\%}, when the production should stop). It also causes the time-varying phonological input pattern and and its corresponding output pattern to be offset by one segment.

\hypertarget{orthographic-representations}{%
\subsection{Orthographic representations}\label{orthographic-representations}}

A training trial's orthographic input is always straightforward. The input pattern contains a representation for each letter of the word, with the representation for that letter being one timestep. As a result the orthographic LSTM operates over some number of timesteps, defined by the number of letters in the word. Each letter is defined as a binary vector with the ``hot node'' corresponding to the sequential index of that letter in the alphabet, where the vector is 26 units long. So, the vector for the letter \texttt{a} is 26 units long, with the first unit set to \texttt{1} and the 25 units after it set to \texttt{0}. No centering or justification is used because the LSTM functions as a loop, with the loop running over \emph{n} iterations, where \emph{n} is defined by the number of letters in the word\footnote{Another relevant detail here concerns the batched nature of training, which is discussed in the section on batch learning}.

\hypertarget{phonological-representations}{%
\subsection{Phonological representations}\label{phonological-representations}}

The phonological patterns are distributed representations over articulatory features common to phonetic descriptions found in the linguistic literature on the topic and similar to those used in previous connectionist models of word recognition (e.g., Harm \& Seidenberg, 2004; Sibley, Kello, \& Seidenberg, 2010). The features and their corresponding phonemes are shown in Appendix XX. In order to account for phonological information relevant to words with more than one syllable, features for primary stress and for secondary stress were included in the the set of representations. Only vowels were eligible for a stress marking, where a given vowel could be designated as primary stress (with the primary stress unit set to \texttt{1}), secondary stress (with the secondary stress unit set to \texttt{1}), or no stress (with both stress units set to \texttt{0}). The distinction only applies in situations where primary and secondary stress are both present in the word, where in most cases where stress is present the distinction between primary stress and no stress is enough to mark the prosodic contour of the word (as in most 2-syllable words). Stress encodings were adopted from the CMU pronouncing dictionary. To illustrate, take the word \emph{squirmy}. The phonological wordform is defined as \texttt{S\ -\ K\ -\ W\ -\ ER1\ -\ M\ -\ IY0}, with the vowel marked with \texttt{1} receiving primary stress, and the vowel marked with \texttt{0} recieving no stress. For comparison, take the 3-syllable word \emph{understand}. The phonological word in this case was defined as \texttt{AH2\ -\ N\ -\ D\ -\ ER0\ -\ S\ -\ T\ -\ AE1\ -\ N\ -\ D}, with a distinction between primary stress (\texttt{AH2}), secondary stress (\texttt{AE1}), and no stress (\texttt{ER0}).

Two additional units were present on each phonological representation in order to represent the start-of-word (i.e., the ``start producing'') and end-of-word (i.e., the ``stop producing'') segments. In each case, when that segment was used, the representation consisted of a vector in which every unit was off except for that critical unit (i.e., the one representing the ``start producing'' or ``stop producing'' feature), which was on (set to \texttt{1}). Training phonological sequences with the start-of-word segment allows for an offline production process, separate from training, where the phonological portion of the network can freely unroll a phonological sequence until it reaches the end of the word, which is marked by the corresponding end-of-word representation. Learning about these terminal points accumulates through training, with the segment occupying a timestep (either the first or last in the phonological wordform) just like any other phoneme. These elements of the training patterns are necessary given that the time-varying type of learning being modeled here benefits from explicit training on the boundaries of the phonological wordform, and is essential for being able to produce a spoken form of the printed word in ``production'' mode, where a spoken form unfolds over timesteps when provided only an orthographic pattern (the ``context'' represented by the cell state and hidden state generated by the orthographic LSTM which is passed to phonology). The production mode of the process is describe more fully belowXX.

\hypertarget{possibilities-of-different-inputs-to-be-present-during-training}{%
\subsection{Possibilities of different inputs to be present during training}\label{possibilities-of-different-inputs-to-be-present-during-training}}

The training trial process described above corresponds to a teaching signal where the learner is both hearing and and seeing the visual word while receiving feedback about the word that should be produced given the input. This is an important part of any cross modal form of learning; learning is enhanced by experiences in which the information signal for both modalities are present (ref.). However, learning to read (as in other forms of learning) also (in fact, most often) involves the production of spoken language from the visual signal alone.\footnote{Note that the term ``production'' is used throughout this work to mean the generation of a spoken form from some input}

Not surprisingly, a learner trained on simultaneously provided orthographic and phonological input (with an error signal back propagated to orthographic and phonological layers from a phonological output layer) will not become independent in its ability to produce phonology from orthography alone. In such a case, the network comes to rely to heavily on the phonological input given its identity with the word's phonological output (i.e., these mappings are much easier and reliable than those between print and speech). This is what we would expect also for children learning to read. If, during learning, the child isn't required to produce speech from print alone, and if she or he can rely on the paired spoken language input, the task becomes one of repetition rather than the production of speech from print (alone). As a result, an alternative training process is required in a time-varying system like the one proposed here in order to avoid this problem.

Given the dual nature of the input system in the architecture, learning can take place by providing inputs on either input layer, or both. The orthographic part of the network is connected to phonology through the passing of state vectors rather than the traditional information flow associated with feedforward and other canonical model architectures. Feedforward networks implement spatially defined inputs and outputs, and when an architecture implements temporal dynamics, it is typically with respect to these spatially defined inputs and outputs. In an LSTM, other network time-varying input and output sequences, training that involves mapping an orthographic input directly to a phonological output (without an explicit phonological input) requires an alternative specification of the inputs on phonology in order to initiate production of a spoken word from a print input.

When learning to produce phonological output from orthography without phonological input present, the phonological input layer receives an empty sequence, but with the start segment (\texttt{\#}) provided as the first timestep (for notation purposes, the symbol \texttt{\#} is used to denote this segment even though the representation itself is a binary vector, as described previously). The timesteps following the start segment are then empty phonological representations where all units are off (\texttt{0}), where the input proceeds through as many phonemes (timesteps) as there are present on the target output pattern. The result is that the empty input and the target output pattern have the same dimensions, which is always the number of phonemes plus one (to accommodate the terminal segments, either the start segment on input or the end input on output).

The timespan of the phonological input sequence when training in this way is determined by setting some set number of empty timesteps as the null phonological input. This serves as the temporal signal that allows the phonological LSTM to function given its temporal nature. In the architecture used here, the number of empty timesteps is defined as the number of phonemes in the word. For example, in such a training trial using the word \emph{breath}, the phonological input pattern would be \texttt{\#\ -\ \_\ -\ \_\ -\ \_\ -\ \_}, with the same output pattern as before, \texttt{B\ -\ R\ -\ EH\ -\ TH\ -\ \%}. As in other training trials, even when provided with an empty phonological input, the produced phonological output is compared to the target phonological pattern, with error backpropagated to weights within the phonological and orthographic portions of the network. The output pattern is generated by virtue of combining the learned weights in the phonological LSTM, the state vectors (context) passed from the orthographic LSTM, and the temporal dynamics afforded by the null phonological input.

This specification relates to the fact that the network uses no justification (i.e., ``padding''), and that a temporal signal is required to drive the dynamics of the time-varying learning over parameters of the LSTM layer. An alternative method could be used that provided more timesteps than those provided strictly by the length of the phonological form itself (e.g., defined by the longest phonological form in the entire training corpus), but such a process requires padding on the output layer given that the time distribution of the input would then be greater than the number of phonemes in the word (plus the word-edge representation). In fact, a training process like this is adopted in most other machine translation endeavors taken up in the engineering and NLP literatures (see ref. and ref. for examples), but is avoided here due to the fact that ``justification'' of inputs and outputs deviates from the sort of veridical representations this work takes as fundamental to developing a more psychologically plausible architecture of time-distributed word recognition processes. An alternative training method is possible, where null segments (after the start of word segment) are passed to the phonological input one at a time, with a corresponding phonological being generated one segment at a time (rather than the gradual build up of input signal over an entire null input). In such a case the spoken form would need to be terminated somehow, presumably initiated once the end of the phonological sequence is reached as indicated by the computation of error with respect to the target pattern. The implementation here avoids this by

Alternatively, the phonological portion of the network can be trained independent of orthography. This can happen in an analogous way to the method described previously, but with empty orthographic inputs. It can also happen by training the phonological LSTM completely independently from the orthographic LSTM, as in during pre-training when weights are developed to approximate the type of early phonological learning that occurs in children prior to the onset of learning about print. In this case the orthographic portion of the network is simply removed from training alltogether. For our purposes here, the latter is always used, as in pre-training. When this occurs, the phonological network is used, essentially, as an autoencoder that maps speech to speech (except that the terminal segments are used). This process is described more fully in the section on pre-trainingXX. Nonetheless, the technical difference between these two implementations isn't important or psychologically interesting. The method adopted here is computationally straightforward and fits the cognitive model of learning espooused in this work.

\hypertarget{phonological-network}{%
\subsection{Phonological network}\label{phonological-network}}

The phonological portion of the network, while being influenced by orthography, operates as a subnetwork that functions in a similar way to an autoencoder, mapping phonological inputs to phonological outputs in the process of developing deep knowledge about spoken language. The primacy of phonology and phonological learning in reading is not controversial. There are debates about the processes at play in how spoken language units are assembled during reading as a function of visual processes (rastle2010).

\hypertarget{production-testing-process}{%
\subsection{Production (testing) process}\label{production-testing-process}}

A different method is used for the purposes of testing the network's ability to produce a phonological form when provided an orthographic input pattern, though the different processes aren't fundamentally different in terms of the information processing that takes place. They only differ procedurally. On production (what we could also think of as ``testing'') trials, an orthographic input pattern is provided, and its activation state builds up over the entire time-varying pattern (for our purposes here this is an entire word). This input process is identical to the one that occurs during training, where each letter is provided as a timestep, and the activation state of the layer is provided as a cell state and hidden state (of the final timestep).

After the orthographic pattern is passed to phonology, the \texttt{\#} segment is provided as a single timestep to the phonological LSTM, indicating that production should take place. From there, information propagates forward through the phonological LSTM as it would on any other trial, producing dynamics over its internal state as it usually does, though only for the single timestep. An output pattern is produced and recorded as the first segment of the word, with the intention of building on that segment until the model is done producing the word. Given that the network never perfectly produces a phoneme (in a unit-wise sense), in order to approximate the speech sound produced the nearest phoneme to the one produced is calculated and recorded for the trial using an L2 norm. This calculation is similar to many other implementations of similar phonological production processes from the connectionist literature (ref.). After the speech sound is recorded, it is passed back to the input in order to determine the next speech sound in the word. Because the state of the network is maintained from the prior timestep, which was initiated by the representation of the printed word passed from the orthographic layer, the effect of this process is essentially, unfolding the phonological wordform one segment at a time. When subsequent phonemes are produced, they are added to those generated previously, constituting a produced word that unfolds over continuous time. The production process ends for a given word when the network produces the stop segment (\texttt{\%}), the boundary which has been learned during training. Alternatively, a limit to the total number of phonemes produced can be set, where the production ends and the experimenter is able to observe what is producedXX.

At this point, the reasoning behind having separate training and testing trials should be more clear. When assessing the ability of the network to produce a complete word, this process allows the phonological layers of the network to settle on a state for each segment of the production, while also being independent in its ability to complete the process of producing a spoken form of the word over continuous time. In this mode, the model is unconstrained with respect to how long it engages in production. This aspect of the process is important given the temporal nature of spoken language production and the challenges this time-varying process presents in terms of computational modeling. A temporal signal needs to drive the process if this aspect of the action is to be simulated, and the dynamical but independent nature of the production process as described here allows this to take place in a realistic manner in the sense that when we read a word we do so until the utterance terminates. This is different than other computational implementations of reading models where, typically the phonological output is of a \emph{fixed} dimensionality, which assumes that the named word always encodes something about the phonological length of all words it has ever learned (e.g., in spatial models that produced a vowel-centered phonological output padded by \texttt{\_} on the left and right side to accomodate the longest trained word).

As an additional point of comparison, other computational models of reading have also used different training and testing processes. For example, the CPD++ model in Perry, Ziegler, and Zorzi (2010) used different procedures, which they termed ``training'' mode and ``running'' mode. In their case, the distinction was quite dramatic, involving rather different mechanisms for learning and producing. For example, training involved learning over pre-specified, structured grapheme-phoneme pairs, among other structured experience to support the development of knowledge in the system. The authors engineered this process as an extension from well-establish phonics instructional methods in education (i.e., that use grapheme-phoneme correspondences), rather than by some more fundamental learning mechanism. In some cases, entirely different learning mechanisms were introduced in order to learn common tricky orthographic wordforms, like those that show a vowel-consonant-e pattern (e.g., \emph{bite}). In other cases, additional ``backup'' strategies were developed in order to assign letters in the orthographic pattern to slots in the syllabic template operating in the system (see p.~122 for discussion of the nonword example \emph{fanj}). By contrast, testing involved a separate process of aligning an orthographic input to build a hypothetical phoneme string based on the learned grapheme-phoneme associations. In the proposed architecture here, the difference between training and testing process isn't a mechanistic one, rather simply implements a different (but highly related) temporal process for the purposes of generating a response.

\hypertarget{anatomy-of-an-lstm}{%
\subsection{Anatomy of an LSTM}\label{anatomy-of-an-lstm}}

Recurrent neural networks have been influential in the evolution of cognitive science in previous decades (Elman, 1990). Cognition is subject to time-sensitive processes, and thus it is important to understand how to represent time in the cognitive models we use to understand human behavior. This is related to the core assumtion underlying this dissertation, namely that models of reading should account for temporal processes relevant to reading behavior.

Recurrent neural networks received a considerable boost in their sophistication with the development of the long short term memory unit, and since their development the architecture has gained traction in cognitive science {[}ref.{]}. The LSTM was developed in order to account for a computationally important problem: how can representations of experiences from the past be maintained in systems that learn over long, temporally-distributed sequences of events. This problem most clearly manifests in understanding the gradients used for weight updates in simple recurrent networks (SRNs) (Elman, 1990). In SRNs, knowledge is typically updated after the system receives a input in the form of a single event (timestep) of a temporally distributed sequence of events. This might be a sequence of words, phonemes, visual events, etc. However, for events that maintain a distal temporal relationship (i.e., long-distance dependencies), the gradient is not maintained, at least directly. This is commonly described as the \emph{vanishing} gradient problem, where error signals over long temporal distances become increasingly dissociated. This problem becomes particularly clear in the application of such architectures to language processing. For example, anaphoric reference often requires an unambiguous association between a nominal referent and its distal pronoun, as in the sentence " \emph{Sam} looked at the monkey in the cage and as though the expectation was for thousands and thousands of years of evolution to be undone, motioned for an object sitting on the ground to be passed over to \emph{her}."

A related problem can also occur when the error signal across proximal timesteps is too large (rather than too weak as before). This has been labeled the \emph{exploding} gradient problem. Given that SRNs function as a many layer feedforward network, where the depth of the network corresponds to the number of timesteps over which inputs will vary for an example. When correlational structure is distal across timesteps, the error signal becomes to weak, and the gradients vanish. When the correlational structure across timesteps is too strong, the gradients become very strong due to their high correlation, leading to volatile behavior as weights are updated in the system.

LSTM networks, a single instance of such a network we will call a \emph{block}, solve this problem by governing the extent to which information received at a previous point in time is relevant at the current moment in time. This is achieved by maintaining memory about the input pattern across timesteps in a representational state called the \emph{cell state}. Information about the current input (timestep) may to flow into it on an attenuated basis using \emph{gates}, which are themselves neural networks with trainable parameters that operate within the larger neural network structure. There are several gates, and they will be described based on what they contribute to the process. In essence though, they allow the representational state to be altered such that information is either dropped, added, or passed off to future timesteps.

At any particular timestep, for sake of description let's imagine the first in a sequence for a example (e.g., a word), the input vector is provided to the network. For our purposes, we can think of the example to be the printed word, \emph{breath}, so the first timestep (if letters are designated as the temporal segment) would be the letter \emph{b}. Unless one is provided, there is no existing representational state for that example because this is the first timestep. In the case of our architecture, the vector representing this letter consists of as many units as there are letters in the alphabet, with the \(i\)th unit set to \texttt{1} and all others set to \texttt{0}, where \(i\) is the sequential index of where that letter occurs in the alphabet (e.g., the letter \emph{b} would have the \(2\)nd unit on). Upon input, the vector is combined (via concatenation) with the hidden representation of the timestep prior. In the case of the first segment (timestep) of the example (orthographic wordform), this hidden representation is a vector of units all with the value \texttt{0}. The length of the hidden representation is as long as there are units in the input vector, which will become more clear when we understand how hidden representations for subsequent timesteps are formed. Once these two vectors are concatenated the resulting vector is passed to each of the four gates within the LSTM block.

Each gate functions similarly internally but is combined with the maintained representational state of the LSTM block in different ways to produce different memory effects based on the particular gate's intended function. The process described here happens within any single timestep before passing the state to the next timestep (timesteps iterate within a loop defined by the total number of timesteps in the example). We can think of the separate gating processes as happening in a sequential way, which can aid in understanding how they function relative to one another, though some of the computation happens in parallel which is made feasible computationally using current libraries and hardware designed for fast, efficient, parallel computation with neural networks such as LSTMs.

The concatenated input is passed to a gate whose job it is to attenuate the cell state in a way that diminishes information that isn't relevant to the ongoing memory of the system. Of course, early in the sequence, the cell state hasn't built up information about the temporal dependencies present in the sequence, so this attenuation is minimal. The gate is a neural network with sigmoidal activation function applied upon output, thereby passing values between zero and one to be combined with the cell state before the signal is moved along to the next gating routine. The output of the forget gate is combined with the cell state using multiplication. This has the effect of maintaining activation states over units of the cell state when the highest value possible is passed through the forget gate (\texttt{1}), and minimizing the values for the lower end of the scale of values passed through (when \texttt{0}). Once combined, the cell state is passed to the next gating functions. The formula for the forget gate portion of the LSTM is shown in XX.
\[
f_t = \sigma(W_fx_t + U_fh_{t-1} + b_f)
\]
Here, \(W\) and \(U\) represent the learned weights of the gate \(f\) at a particular timestep \(t\). A given input representation at time \(t\) is shown as \(x_t\), and the representation passed from the previous timestep is \(h_{t-1}\). In prose, the latter is referred to as a \emph{hidden representation} and the former as an \emph{input representation}, though the equation makes clear the symmetry between these two vectors given that they function in very similar ways with respect to any given gate (here, the forget gate but the symmetry is clear in other gates as well). The element \(b_f\) is a bias unit included in the gate. The output is passed through a sigmoidal transfer function, \(\sigma\), before it combines with the cell state.

The concatenated input (plus hidden) vector is also passed to a set of gates called the \emph{input gate} and \emph{cell gate}\footnote{The name ``input gate'' is conventional and used consistently throughout the literature, however the name of this other gate, which has been termed the ``cell gate'' here is not. We will call it this for the time being, however note that the name is somewhat misleading. Its relationship to the cell state of the LSTM is no more privileged than other gates within the LSTM block. The term ``gate'' is used here for both (in fact all) gates in order to underscore their architectural similarity, though the terminology may differ slightly from descriptions elsewhere.}, whose activity is coordinated and thus described together. This coordination occurs with respect to their outputs and how they are combined before they are passed to the cell state; their internal functioning is independent of each other.

The gates each receive the concatenated hidden and input vectors, just like the forget gate. This longer vector is passed to a neural network with trainable parameters, and outputs a signal. The output is summed and passed through either a sigmoidal transfer function in the case of the input gate or a hyperbolic tangent function (\(tanh\)) in the case of the cell gate, these are shown in XX and XX. For notation, we will use the symbol \(\Im\) to represent the input gate and the symbol \(\tilde{c}\) is used to represent the cell gate.
\[
\Im_t = \sigma(W_{\Im}x_t + U_{\Im}h_{t-1} + b_{\Im})
\]
\[
\tilde{c}_t = \tanh(W_{\tilde{c}}x_t + U_{\tilde{c}}h_{t-1} + b_{\tilde{c}})
\]
Their outputs are then combined via multiplication before being added to the cell state, and at this point the cell state has already been attenuated by the output of the forget gate, \(f_t\). This whole process is represented in XX and defined in terms of the cell state \(c\). This equation accounts for the final cell state at the end of the current timestep, and is passed on to the next timestep as its cell state to be further updated.
\[
c_t = f_t \cdot c_{t-1} + \Im_t \cdot \tilde{c}_t
\]
The last gate involved in the function of the block is the \emph{output gate}, referenced with the notation \(o\). Its job is to determine what activation gets passed on to the next timestep in the form of a hidden representation. The function of this gate helps to make clear why this vector is termed the \emph{hidden representation}: it is the emergent representation of the current timestep (segment) that combines information from the input, the previous hidden representation, and the cell state of the block. It essentially combines these different, related information sources (external perceptual information and internal memory representations) in the form of a single vector that represents information about that timestep for the next timestep. This vector is different than the cell state itself, which is characterized elsewhere here.

The output gate internally functions identically to both the forget gate and the input gate\footnote{These three gates only differ internally from the fourth, the cell gate, because of the transfer function used (i.e., \(\sigma\) instead of \(\tanh\)).}, and its formula is shown in XX.
\[
o_t = \sigma(W_ox_t + U_oh_{t-1} + b_o)
\]
This gate receives a concatenated input (\(h_{t-1}\) with \(x_t\)), which are passed through trained weights. The output is then summed and a sigmoidal transfer function is applied. Importantly, the output signal is used differently than other gates. The vector is multiplied against a copy of the cell state (a copy which has subsequently been transformed using a \(\tanh\) function). This is shown in XX, where \(h_t\) in the previous description should be thought of as the hidden state that the current timestep produces for the next timestep.
\[
h_t = o_t \cdot \sigma_h(c_t)
\]

This operation allows for the newly created hidden representation to be of the appropriate length, given that the output of the operation is determined by the dimensions of the cell state (i.e., the hidden state dimensions are the same as the cell state dimensions). The output of this set of operations is then passed on as the hidden representation used in the next timestep.

This process is repeated across as many timesteps as occur in the input. As a result, when an example (e.g., an orthographic wordform) terminates, what's left is the cell state and a hidden state. The cell state is in its final state for a given timestep once it has been combined via addition with the (combined) output of the input and cell gates. The hidden representation is ready for use on the subsequent timestep once it has been combined via multiplication with the \(\tanh\) transformed cell state as described in the above section on the output gate and shown in XX. As a result, any LSTM layer has the capacity to pass the cell state as well as all hidden states from each timestep of the example if necessary or useful (not just the hidden state of the final timestep). This distinction is important in terms of the representational capacity of the layer The cell state should be seen as a memory representation of the example as a whole, whereas the hidden state of the last timestep is more narrow in its representational capacity. The information contained in this vector is more relevant to the final timestep itself, though has been affected directly (via a \(\tanh\) scaling operation) by the cell state, and so contains a trace of information relevant to the entire word.

At this point a few important aspects of the cognitive nature of such a system can be foregrounded in computational terms. Statistical learning systems, like the artificial neural network described here, should account for the statistical dependencies of the percepts they experience. Feedforward networks accomplish this by taking in information on an input layer (like orthography) and updating a set of trained weights with respect to error calculated on the output layer. Dependencies are encoded in the weights given the degree of covariation between the input pattern and its corresponding output pattern, taking into account this covariation across all learning trials. This is a spatial mapping given that the covariation, which is captured by the dependencies present within a given learning trial, happens without time distribution on the input and output. The LSTM performs a spatial mapping for a single timestep, but also captures dependencies across timesteps, capturing the dynamics of segments across the training example.

The dynamics specifically play out with respect to the two outputs served up by the block: the cell state and the hidden state. While these two vectors will always bear a relation to one another, most specifically in terms of the combination that occurs on the output gate (shown in XX), they are computationally and functionally distinct. The cell state serves as the memory state that captures statistical dependencies across all timesteps of a learning trial (letters or phonemes across words) -- a distal relationship -- and the hidden state captures the proximal analogue: the emergent representation of the current segment with respect to the cell state.

Without gating mechanisms that allow information to flow into the long term memory of the example, these long term dependencies can't be captured. Likewise, without the ability to combine information about the long term dependencies with information being processed at a given timestep (i.e., the combining function of the output gate and the cell state that creates the hidden state \(h_t\)), proximal information about the segment can't be transferred from one segment to the next. So, the two internal ``state'' elements of an LSTM have the capacity to capture something long-term (the cell state) and short term (the hidden state) across a training trial.

This relates directly to aspects of human behavior in important ways. For example, the debate about the viability of connectionist models of word reading are sometimes pitched in terms of a model's capacity (or lack thereof) to account directly for segmental phenomena found in experimental work with humans (Kawamoto, Liu, \& Kello, 2015). The temporal model developed here is capable of addressing such issues given its ability to simulate phenomena related to segments due to the fact that segments are part of the temporal phenomena related to speech and print. Introspection about different representational grain sizes in the perceptual inputs of the learning system should be possible in a system that is concerned with learning that involves segments, and this is the case for the architecture being proposed. This capacity deviates sharply from other models of word reading given the limitations of purely spatial learning systems, and the ways in which representational schemes for letters and sounds further exacerbate the capacity to introspect about segmental properties of the learning they involve. For example, the LSTM architecture captures more closely that the learner has representations for ``sublexical'' units possessed by any given input or output, and that the representational capacity can be observed by observing the model respond to that sublexical unit directly. For example, individual letters and letter sequences have pronunciations, which can be directly observed by setting up a testing trial using the network. This isn't possible in feedforward networks due to the abstract ways in which segments are specified and processed in those systems. This virtue of the architecture will be returned to in the results when we look at the dynamics of a training trial and the types of targeted phenomena the network can simulate for segments of language.

\hypertarget{representational-virtues}{%
\subsection{Representational virtues}\label{representational-virtues}}

Readers can process, correctly or not, letter strings of any length. Any model architecture of producing phonology from orthography should be able to account for this fact. Also, this should include the ability to read any word, regardless of the length of its spoken form. There should be a direct relationship between the length of a word's printed or spoken form and its representation used by any computational architecture that consumes it. This relationship is maintained in the current architecture in a very direct way. The representation of the printed form of a word is generated letter-for-letter, where the orthographic representation is as long as there are letters in the printed form. The phonological representation is formed in a similarly veridical way. A spoken form is defined as a sequence of phonemes, which is a well-established level of linguistic representation for the spoken form of words. The output representation for a word is build phoneme-by-phoneme from the word's phonemic form, where each phoneme is defined as a distributed pattern over phonemic features, as described elsewhere here. Therefore, the length of the spoken form of the word is defined by the number of phonemes in the word, and the complexity of the representation can be estimated by the number of features on over an individual or string of phonemes. This representational scheme is intended to represent the structure of the language in a way that is veridical, or at least more veridical than other implementations of similar models.

\hypertarget{justifications-for-justification}{%
\subsubsection{Justifications for justification}\label{justifications-for-justification}}

A number of alternative schemes have been entertained in other models of word naming. The most common approach involves specifying the orthographic and phonological representations using vowel centered representations (Chang \& Monaghan, 2019; Cox, Borkenhagen, \& Seidenberg, 2019). For models of reading that deal in monosyllabic words this approach captures something appropriate about vision, given that an entire printed word can be taken in on a single fixation if the word is short. Granting this assumption, learning that the centered pattern \texttt{\_\_breath\_\_} maps onto \texttt{\_\ -\ \_\ -\ B\ -\ R\ -\ EH\ -\ TH\ -\ \_\ -\ \_} is a defensible simplifying assumption.

However, procedures like that require justification of some kind, typically in the center with both left and right pads, introduce a computational problem: perceptual segments come to occupy slots in their representational domains. For example, the letter \emph{b} from \texttt{\_\_breath\_\_} comes to be represented in a different way than that same letter in other positions (e.g., \emph{cob}, or \emph{curb}). Some computational architectures have tried had to work around this issue one way or another, usually by abstracting away from positions to higher order linguistic units. For example, Plaut, McClelland, Seidenberg, and Patterson (1996) represented input and output patterns based on the syllabic structure which a given segment is commonly associated. Their method was developed in the wake of other procedures designed to encode just enough of the relative structure of segments in words to be suitable for modeling monosyllabic words. For example, see the discussion in Seidenberg and McClelland (1989) and their use of orthographic and phonological ``triples'' (\textbf{Wikelgren1969?}) to work around this problem, which is thought to be a plausible alternative given that such representations build in contextual information that helps the model abstract away fro position-specific representations in the input and output layers.

Positional information in other proposed models deviates from this assumption.

Issues around positional representations and dispersion All previous connectionist models of word recognition have involved spatial (and in some cases over temporally varying segments) justification of one kind or another, usually implemented as empty slots to the right of an orthographic or phonological pattern (e.g., "breath\_\_\_\_"). When present, this feature of an architecture (and the patterns that are used to train it) represents a deep limitation in its psychological plausibility. While including justification in some form is understandable as an aspect of the engineering problem that computational modelers face in modeling visual (or other) word recognition processes, they assume that the patterns that are used in orienting words, left or right, on their input and/ or output layers (however the justification might be implemented) are an aspect of the learning process, though

\hypertarget{possibilities-in-the-timecourse-of-visual-and-phonological-processing}{%
\subsection{Possibilities in the timecourse of visual and phonological processing}\label{possibilities-in-the-timecourse-of-visual-and-phonological-processing}}

The timecourse of phonological activation of orthographic input is a debated topic, but little is known about how such processes operate for long words. This issue has primarily been dealt with in the literature using hybrid computational architectures, containing both connectionist and structured, symbolic components of the computational system Perry, Zorzi, \& Ziegler (2019). Though some purely connectionist architetures have been proposed (Sibley, Kello, \& Seidenberg, 2010). While outside the scope of previous modeling and theorizing, it is likely that a dynamic process where orthographic inputs propagate information to activate phonology, initiating a response, which is modulated by subsequent visual input. Consider, for example, reading the word \emph{architecture}. Phonological activation could well be taking place over the initial visual portion of the word, such that the \emph{\ldots ture} portion isn't processed until after the phonology of the root (\texttt{AA\ -\ R\ -\ K\ -\ AH\ -\ T\ -\ EH\ -\ K\ -\ T}) is initiated. While the dynamics of such processes are interesting and important for developing a model of word reading that accounts for how long words are read, such cases are outside of what is dealt with here. A simpler corpus has been developed for the purposes of this work, built on with assumptions based on perceptual span (some results are provided that demonstrate the ability of the model to produce output for words longer than this span, demonstrating the broader capacity of the model and architecture). A virtue of the architecture developed in this dissertation is that such dynamics are possible given the nature of the computational learner developed, though these dynamics are not accounted for in service of accounting for simpler and more fundamental processes in word reading.

\hypertarget{segments}{%
\subsection{Segments}\label{segments}}

\hypertarget{recurrence}{%
\subsection{Recurrence}\label{recurrence}}

\hypertarget{chapter-4-experimentation}{%
\section{Chapter 4: Experimentation}\label{chapter-4-experimentation}}

The purpose of the model developed here is to read printed words aloud. A number of results are presented based on results from behavioral research and similarly oriented models of word reading from elsewhere in the literature.

Results of the model's ability to correctly produce a spoken form of a word can be based on a number of different measures arising from the same computational system. The most common measure of the model's ability is an accuracy score, which itself can be structured in a few different ways - similar to that of a human reader. Given that the goal of the model is to take a sequence of letters and recode them in the form of a sequence of speech sounds, individual sounds can be inspected for their accuracy along with the entire sequence of sounds - that is, the entire word. Both the phoneme-wise and the word-wise accuracy metrics are used in reporting, depending on the purpose of the result shown. They are of course correlated given that a word that is produced correctly is one in which all the phonemes are produced correctly (or at least each phoneme produced is closer to the correct phoneme than all other possible phonemes).

A limited corpus of words has been selected for training purposes here, though the model is capable of attempting to produce phonology from any sequence of letters provided on the input - a feature we see as a virtue of the implementation given that humans are also capable of this.

\hypertarget{training-details}{%
\subsection{Training details}\label{training-details}}

\hypertarget{testing-trials}{%
\subsection{Testing trials}\label{testing-trials}}

The model is tested in a way that follows a different procedure than that used in training, allowing for more flexible and deeper introspection about the dynamics at play in the system when operating. When a word is submitted for testing (or ``production'') an orthographic pattern is introduced to the the orthographic input - the model's visual input system. This pattern is structured just like that of the orthographic input during training: it is a two dimensional array with as many letter representations along the first axis as there are letters in the word. This time-varying input pattern is input to the orthographic input, and its representation decomposed into its two critical states for use in phonology: the cell state and the final hidden state. Once these vectors are achieved for the orthographic form of the word, that portion of the model rests and is unchanged for the remainder of the trial.

The states of the orthographic layer are passed to phonology to generate the phonological code output. This process is initiated by the input of the start of word segment (\texttt{\#}), the representation of which was described earlier. The fact that the model has been trained with an explicit start of word marker enables the phonological layer to produce a next phoneme given the current state of the layer and its input pattern. At the start of production this context is provided completely from the orthographic layer. That is, the role of the phonological portion of the model is to produce a phonological code given the orthographic context.

Once the first phonemic pattern is produced it is saved for subsequent phonemes to be appended to it in order to form a full sequence of phonemes (a production). While the weights in the network are frozen at this stage, the state of the phonological LSTM is updated at each timestep following the same processing flow as described in the architecture section. Once a phoneme is output at each timestep, it is fed back to the input of the phonological layer to support subsequent productions. This differs in process from training trials but is the same conceptually given that in training, the entire phonological sequence is processed one segment at a time without the feedback process from the output layer back to the input. Because the output for a given phoneme is never specified as a perfect veridical binary pattern, any phoneme output is compared to all possible phoneme outputs using the L2 norm and the nearest phoneme is selected and recorded as the produced phoneme. The production trial progresses until the phonological output layer produces the ``stop producing'' segment (\texttt{\%}). At this point the word produced is recorded for analysis.

\hypertarget{dispersion}{%
\subsection{Dispersion}\label{dispersion}}

Sequential approaches to orthography-to-phonology conversion solve this problem by processing discrete speech segments from discrete print segments in a left-to-right, segment-by-segment manner. These solutions always involve some form of justification (sometimes called ``alignment'' in the literature), though the alignment procedure differs across sequential implementations. For example, (\textbf{Sejnowski1987?}) provided orthographic input strings as sequences of letters and mapped them to equal-length sequences of phonemes. This allowed their network to implement a kind of temporal processing over visual and its corresponding spoken pattern, but required architectural specifications that deviate from assumptions about processing in other connectionist networks. Their procedure required inserting null elements in slots where a phoneme wasn't present for the corresponding input letter (or grapheme) given that the input and output sequences were always equal length. So, for the input pattern ``late'' the corresponding output would be \texttt{L\ -\ EY\ -\ T\ -\ \_\_}, with the final phoneme segment \texttt{\_\_} being the null one given the word-final silent ``e'' in ``late.'' This avoids the type of dispersion found in feedforward networks, where knowledge about letters and their corresponding sounds is localized to weighted connections associated with specific slots given the way that input and output patterns are specified during learning, but introduces a different type of knowledge localization that is undesirable: that letters on the input layer become associated too narrowly with phonemic segments on the output given the segment-by-segment processing mechanism (i.e., they lose the context sensitive nature of processing due to the segment-by-segment way that print and speech are associated in the network).

Dual-route models use an assembly method, where a letter or letter segment (e.g., grapheme) is associated with a phonemic segment by rule. Processing speech from print involves assembling the phonology from the pre-specified letter-sound rules, which are derived via an analytic technique that resides outside the scope of the computational architecture itself, specified by the experimenter. The assembly process happens alongside a lexical lookup procedure, which has its own separate timecourse. The relative timecourse of these processes determine whether a word is produced via assembly rules or as a structured lexical object. This is the extreme end of the spectrum of methods that result in knowledge dispersion. Here knowledge about the relationship between visual and spoken elements is dispersed across the symbolic rules that operate over the crossmodal assembly that takes place during learning and performance. This form of knowldge dispersion is different implementationally but related conceptually in that what is known about similar elements in the domain becomes dissociated based on the structure of the architecture, which of course has implications on the learning that takes place given that architecture.

Other approaches handle input strings with the use of slots, but in a different way than in (\textbf{Sejnowski1987?}). Commonly in connectionist feedforward implementations input patterns are specified in a vowel-centered arrangement (see Plaut, McClelland, Seidenberg, and Patterson (1996) for a discussion). A similar solution exists in hybrid connectionist dual route architectures like Perry, Ziegler, and Zorzi (2010). Here the input patterns are fit into a template of sorts, specified by the limits of the structure of the training set.

These issues have not been solved by engineering solutions to text-to-speech technologies either.

\hypertarget{benchmarks}{%
\subsection{Benchmarks}\label{benchmarks}}

\emph{jared1990: latencies increase with number of syllables, but is moderated by frequency (Experiment)
}jared1990: exceptional and regular inconsistent words exhibit longer latencies as compared to regular words, but only for low frequency words (see yap2009, top right of p.~503 for discussion of the effect)
*position of irregularity effects

kawamoto1998 showed that words with less predictable pronunciations of the vowel exhibit longer latencies for the consonant in the onset, rather than simply longer overall naming latency of the entire word. This is due to the fact that the vowel pronunciations that are less predictable require more processing time, and this processing manifests in the system dwelling on pre-vocalic consonants, rather than being distributed throughout the naming process for the entire word. Their studies were performed using monosyllabic stimuli that were quite simple, though well established in the word reading literature related to naming and related phenomena.

\hypertarget{canonical-naming-effects}{%
\subsection{Canonical naming effects}\label{canonical-naming-effects}}

\hypertarget{frequency}{%
\subsubsection{Frequency}\label{frequency}}

\hypertarget{regularity}{%
\subsubsection{Regularity}\label{regularity}}

\hypertarget{frequency-by-regularity}{%
\subsubsection{Frequency by Regularity}\label{frequency-by-regularity}}

\hypertarget{latency}{%
\subsection{Latency}\label{latency}}

Given that computational processing times on a GPU haven't been validated as an analogous method for naming latency in humans, similar to Sibley et al.~(2010) a measure of model confidence for processing an orthographic word was designed to measure the latency for reading that word.

\hypertarget{overall-results}{%
\subsection{Overall results}\label{overall-results}}

*Provide an overall summary of results here before going into the piecemeal results. See Seidenberg \& McClelland (1989) pages 531-532 for examples

\hypertarget{chapter-5-general-discussion}{%
\section{Chapter 5: General Discussion}\label{chapter-5-general-discussion}}

\hypertarget{future-work}{%
\subsection{Future work}\label{future-work}}

This model architecture could be extended easily, in some cases trivially, to account for representational assumptions that would more realistically approximate spoken language.

Note that an architecture of this kind can be easily extended to include both a phonological and orthographic output layer despite the more simple version of the architecture adopted for this work. Such an extension would be important to pursue given the role of writing (action) in reading development; the two activities are almost always unavoidably linked in development.

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\hypertarget{ref-Chang2019}{}%
Chang, Y.-N., \& Monaghan, P. (2019). {Quantity and Diversity of Preliteracy Language Exposure Both Affect Literacy Development: Evidence from a Computational Model of Reading}. \emph{Scientific Studies of Reading}, \emph{23}(3), 235--253. \url{https://doi.org/10.1080/10888438.2018.1529177}

\leavevmode\hypertarget{ref-Cox2019}{}%
Cox, C. R., Borkenhagen, M. C., \& Seidenberg, M. S. (2019). \emph{{Efficiency of learning in experience-limited domains: Generalization beyond the WUG test}}. 1566--1571.

\leavevmode\hypertarget{ref-Elman1990}{}%
Elman, J. L. (1990). {Finding Structure in Time}. \emph{Cognitive Science}, \emph{14}(2), 179--211. \url{https://doi.org/10.1207/s15516709cog1402_1}

\leavevmode\hypertarget{ref-Harm2004}{}%
Harm, M. W., \& Seidenberg, M. S. (2004). {Computing the meanings of words in reading: Cooperative division of labor between visual and phonological processes}. \emph{Psychological Review}, \emph{111}(3), 662--720. \url{https://doi.org/10.1037/0033-295X.111.3.662}

\leavevmode\hypertarget{ref-Kawamoto2015}{}%
Kawamoto, A. H., Liu, Q., \& Kello, C. T. (2015). {The segment as the minimal planning unit in speech production and reading aloud: Evidence and implications}. \emph{Frontiers in Psychology}, \emph{6}(September), 1--6. \url{https://doi.org/10.3389/fpsyg.2015.01457}

\leavevmode\hypertarget{ref-Perry2010}{}%
Perry, C., Ziegler, J. C., \& Zorzi, M. (2010). {Beyond single syllables: Large-scale modeling of reading aloud with the Connectionist Dual Process (CDP++) model}. \emph{Cognitive Psychology}, \emph{61}(2), 106--151. \url{https://doi.org/10.1016/j.cogpsych.2010.04.001}

\leavevmode\hypertarget{ref-Perry2019}{}%
Perry, C., Zorzi, M., \& Ziegler, J. C. (2019). {Understanding Dyslexia Through Personalized Large-Scale Computational Models}. \emph{Psychological Science}, \emph{30}(3), 386--395. \url{https://doi.org/10.1177/0956797618823540}

\leavevmode\hypertarget{ref-Plaut1996}{}%
Plaut, D. C., McClelland, J. L., Seidenberg, M. S., \& Patterson, K. (1996). {Understanding Normal and Impaired Word Reading: Computational Principles in Quasi-Regular Domains}. \emph{Psychological Review}, \emph{103}(1), 56--115. \url{https://doi.org/10.1037/0033-295X.103.1.56}

\leavevmode\hypertarget{ref-Rogers2004}{}%
Rogers, T. T., Lambon Ralph, M. A., Garrard, P., Bozeat, S., McClelland, J. L., Hodges, J. R., \& Patterson, K. (2004). {Structure and Deterioration of Semantic Memory: A Neuropsychological and Computational Investigation}. \emph{Psychological Review}, \emph{111}(1), 205--235. \url{https://doi.org/10.1037/0033-295X.111.1.205}

\leavevmode\hypertarget{ref-Seidenberg1989}{}%
Seidenberg, M. S., \& McClelland, J. L. (1989). {A Distributed, Developmental Model of Word Recognition and Naming}. \emph{Psychological Review}, \emph{96}(4), 523--568. \url{https://doi.org/10.1037//0033-295X.96.4.523}

\leavevmode\hypertarget{ref-Sibley2010}{}%
Sibley, D. E., Kello, C. T., \& Seidenberg, M. S. (2010). {Learning Orthographic and Phonological Representations in Models of Monosyllabic and Bisyllabic Naming}. \emph{European Journal of Cognitive Psychology}, \emph{22}(5), 650--668. \url{https://doi.org/10.1080/09541440903080583}

\end{CSLReferences}


\clearpage
\renewcommand{\listtablename}{Table captions}


\end{document}
