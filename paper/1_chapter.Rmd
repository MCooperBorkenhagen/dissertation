\newpage
# Chapter 1: Introduction
Language comes and goes in a time-contingent fashion. This is true for a variety of ways that we experience language, including reading. Words are read by virtue of perceptual and cognitive processes that are distributed across time and space. As a reader engages visually with a word printed on the page, the reader's eyes move across portions of printed word, tapping her or his knowledge of speech, print, and other domains of knowledge relevant to processing text. These aspects of experience form the sequential statistics of our knowledge relevant to a variety of linguistic tasks. As a result, our theoretical models of reading and language processing more generally -- computational and otherwise --  should account for the fact that such processing takes place in a sequentially oriented, time-distributed manner.

Historically, cognitive accounts of word reading have been largely focused on the simplest of words of English - those with one syllable [@Perry2010; @Rastle2000; @Sibley2010; @Yap2009]. This focus affords a simplified account of the visual processes involved in learning how print and speech relate. Relatedly, our established theories of learning in the domain have focused on the computations that occur when processing single-syllable words, and have formalized the processes and mechanisms in computational models [@Plaut2005]. Computational models are an important tool used to test hypotheses about learning and performance, and formalize theories of how reading development takes place including, but not limited to, specifying the representations and computations involved in skilled reading [@Plaut2005; @Rastle2016; @Seidenberg2022].

These accounts and their computational implementation come from connectionist theories of learning and information processing [@Plaut1996; @Seidenberg1989]. Cognition in this view happens by virtue of cooperative and competitive interactions among many neuron-like processing units in a given cognitive system. These interactions take place as a result of exposure to experiences that support and constrain their activity, specified by what we know about the development of knowledge in a given domain. Introspection about their activity then constitutes introspection about knowledge and its development. As a result, connectionism posits a broader set of assumptions about human learning that have been developed over many years and concerns a variety of domains, not only reading [@Plaut2000]. Nonetheless, reading is an important behavior because it taps a number of knowledge domains each of which has separate but interconnected theorizing [@Plaut2005; @Seidenberg2022].

@Seidenberg1989 (hereafter SM89) articulated the approach to reading development and the process of reading a printed word aloud. In such a view, learning to read takes place by virtue of mappings among a word's printed, spoken, and semantic codes. The theory was implemented in a connectionist model that emphasizes the computations among codes happening in parallel^[The account does not deny the existence of sequential processes taking place during reading and its development, the focus of the theory and related computations is on the parallel processes that do take place. See @Plaut2005 for more on this point, and see @Rastle2010 for an alternative claim.]. The model came to be known as the _triangle model_ due to its emphasis on three bases of knowledge involved in the reading system: orthography, phonology, and semantics. The focus on the parallel nature of computations taking place in reading aloud is related to the focus on single-syllable words and their use in the computational model of the process developed in SM89 as well as subsequent related models that extended the theory (Plaut et al., 1996; hereafter PMSP96). This focus unfortunately limits the scope of the triangle model and its predecessors given that much of the linguistic structure learned in English concerns words longer than one syllable. Some preliminary implementations of models that are capable of processing longer words have been put forward [@Plaut1999; @Sibley2008; @Sibley2010], but such work is young.

The focus on short words is primarily due to two factors. First, any model of word reading must at the very least account for the development of knowledge of the shortest, simplest words in the language given that short words tend to be learned earlier than long ones. The second is a more technical consideration: modeling information processing over words that are longer than one syllable requires computational procedures that account for time-varying aspects of processing that can be avoided when studying shorter ones. These processes are naturally more complex and require more sophisticated engineering than that needed for simpler stimuli. This includes accounting for variability in prosodic information involved in pronunciation over different syllables (especially of vowels and their associated patterns of stress), among other relevant time-varying phenomena involved in text processing.

## Simulating time in reading aloud
Prevailing models of word reading have dealt with temporal aspects of processing in a few ways. Several influential models have used attractor networks that allow for a pattern of activation to settle into a stable state using recurrent dynamics [@Harm2004; @Plaut1996] when simulating learning and performance for monosyllabic words (mapping a spatially-fixed pattern of print to a spatially-fixed pattern of phonology). These models allow for the state of the system to change over time as a result of experience with a single learning event (like a printed word). The temporal dynamics (and their effects under disorder) have been especially important in understanding the results of injury to the cognitive system, where basins of attraction that develop through the recurrent dynamics of the system show behavior similar to brain-injured patients whose learning and performance they simulate (see Plaut, 1991 for an example, and Seidenberg, 2017 for a thorough discussion related to reading behavior).

In a related but less developed line of work, simple recurrent networks (SRNs; see Elman, 1990) have been applied to learning letter and phoneme sequences [@Sibley2008; @Sibley2010]. In their most simple form when applied to word learning, simple recurrent networks allow for the encoding of variable length sequences (like printed or spoken words) into a fixed length pattern. This is made possible through a network that passes the elements of a given sequence through a time-varying network, where a set of weighted connections is used to process elements of the sequence, one element at a time. Despite the proximal (one timestep at a time) way that error updates occur, the weighted connections come to encode information about more distal elements as well. The models in @Sibley2008 perform this task for letters and phonemes separately, where offline processing allows the output pattern for a given item of print to then be associated using a second SRN with a time-varying sequence of phonemes that corresponds to that print item. The networks in @Sibley2008 and @Sibley2010 used different recurrent layers for orthography and phonology, using the encoded vectors of the orthographic network as training patterns for phonological network. While some aspects of the implementation aren't clear from the reporting on the model^[Some aspects of the architecture aren't specified in these reports, and the code for the models is no longer available (determined through personal communication with the corresponding author in November 2021). More needs to be known about the nature of an SRN-based approach to an orthography-phonology reading model in order to make a more full assessment of its viability and theoretical value, especially when a model has novel features that aren't yet fully developed in the literature. To this end, the work of this dissertation is intended to be an extension of the @Sibley2010 model.], the way that it incorporates temporal processing on orthography and phonology is important from a theoretical perspective. Letters are processed sequentially, as are phonemes.

Finally, some models have implemented a component of the architecture dedicated to represent temporal aspects of naming in an implicit, more covert way, typically in the form of stress on predefined syllables [@Perry2010; @Perry2019]. The best example of this comes from "connectionist dual-process models" [@Perry2010], which encode information about temporal dimensions of words (like the primary-secondary stress pattern in the word `r scaps('thankful')`, or the opposite pattern in `r scaps('royale')`) but accomplished in a non-temporal way. In the @Perry2010 model, orthographic patterns are assigned to slots and then stress is applied via dedicated stress nodes based on the orthographic slots that the letters come to occupy in a two-layer feedforward network [see @Zorzi1998 for its original development]. There is a secondary component that handles stress on words that are stored in the lexical route, allowing for words to contain stress even if they aren't generated via the orthography-phonology network. This approach simulates temporal processes through non-temporal means by treating time-varying aspects of words as spatially fixed, representationally. This method is similar to @Seva2009 which showed that a feedforward architecture can map a word's orthographic representation directly to its corresponding pattern of stress without recurrent processes operating in the network, with the @Perry2010 and related CDP models doing so in the context of a larger model framework that did more than apply stress rules.

## Computational models of reading aloud
Recognizing and naming printed words is an important important phenomenon with longstanding attention in cognitive science and related disciplines [@Adams1991; @Snowling2005; @Seidenberg2017; @Seidenberg2020]. This is due to several factors including the range of phenomena related to its function as well as its enormous importance in public health, including education [@Castles2018; @Rayner2001; @Seidenberg2017]. Reading involves information processing over a variety of bases of knowledge and related perceptual mechanisms, including vision [@Reichle1998], phonological processing [@Harm1999; @Wagner1987], semantics [@Harm2004; @Siegelman2020], and language experience more broadly [@Seidenberg2018], among other knowledge domains. This scientific work is often formalized in computational models of cognition (Coltheart, 2005; Plaut, 2005).

Computational modeling in the scientific enterprise of understanding reading, like other psychological sciences, has been an important part of developing robust theories of the mental processes involved. The computational bases of many of these perceptual and cognitive processes are understood in no small part due to the corresponding computational models that implement important aspects of such processes. Computational models are important for a variety of reasons, including introspecting about aspects of development of cognition that happen outside of conscious awareness, testing hypotheses about such processes, and doing so in a way that allows for detailed investigations about the structure of the system that might not be possible in human or animal models. @Seidenberg2022 provides a summary of the dominant theorizing and a review of the relevant history.

Particular attention has been paid to the specific relationship between print and speech in reading development because of the semi-systematic ways in which these codes map onto the other in some languages (especially English). This type of idiosyncratic structure has come to be known as *quasiregular* [@Plaut1996; @Seidenberg1989]. Our understanding of information processing in knowledge domains involving quasiregular structure originates with the dominant theory of learning to read, SM89, and extended PMSP96. Additional discussion of quasiregularity and its impacts on learning and cognition can be found in @Seidenberg2014 and @Bybee2005, with a discussion of its implications on learning in educational contexts in @Seidenberg2020. Computational models derive from seminal work by the PDP Research Group [@Rumelhart1986] on a variety of cognitive domains to which connectionist learning principles and their associated computational architectures were applied, supporting the emergence of connectionist architectures like SM89.

SM89 proposed a learning architecture using a multi-layer artificial neural network that processed representations of words using representations of their features of print, speech, and semantics^[The actual computational model implemented involved only orthography and phonology. See @Harm2004 for an extension that included semantics.]. Input layers represent bases of perception and action involved in reading words aloud. The artificial neural network processes input representations via weighted connections in interlevel ("hidden") layers, which allow for cooperative and competitive states of activation to take place across representational units for a given word.

SM89 was important both because it implemented a learning model that posited the bases of perception and action relevant to reading aloud, and reproduced several critical effects found in human behavior. The most important of these is the interaction between the frequency of a word and the extent to which that word exhibits typical structure (in terms of the pronunciation of a word with respect to its spelling) when naming the word. Frequent words, regardless of how atypical the spelling-sound structure is are processed with equivalent ease. Words that are less common and possess atypical structure are processed more slowly and are more error-prone than their more typical low-frequency counterparts. This effect has been seen in a range of behavioral studies [@Seidenberg1984; @Seidenberg1985; @Taraban1987], and is applicable to learning and performance in behaviors other than reading. Importantly, like human readers SM89 displayed graded effects of atypical spelling-sound structure, which is known as structural *consistency* [@Plaut1996; @Jared1997]. This effect was first reported in @Glushko1979 and has been reproduced many times since [@Taraban1987; @Jared1997]. These effects and their theoretical importance have been very well documented elsewhere - see @Seidenberg2022 for a thorough review. These effects will be discussed at greater length in the context of the computational model reported in this dissertation in Chapter 3.

Competing theories involve another perspective on how we compute the pronunciation of words of varying structural regularity: the dual-route theory (Coltheart, 2005). The core of this theory is that reading a word aloud requires two different mechanisms. One mechanism computes a phonological output by assembling phonemes from their corresponding letters based on a set of symbolic rules designed to do so. The other mechanism accesses the word from a lexical storage location in cases where the rules aren't able to accomplish the task properly^[This second method actually contains two possibilities: one that references semantics and another that doesn't. The implemented model in @Coltheart2001 posits the additional semantic route (the "lexical semantic route") as an unimplemented portion of the model. The characterization of "dual-routes" holds though because of the distinction between the "assembly" method by letter-sound rules and the activation mechanism that happens via the other "lexical" route/ mechanism.]. This theory is implemented in corresponding computational models [@Coltheart2001; @Coltheart1993], including subsequent models that adhere to some of the theory's principles but extend them in novel ways [@Perry2010; @Perry2019]. These are often referred to as "dual-route cascaded" models (DRC for short) because of the two mechanisms and the way in which information flow is specified in the system (described more below). These alternative computational models of reading came in the wake of SM89, and implement the dual-routes as two separate computational mechanisms. They are also not theories of learning, but theories of performance given the assumptions they implement about storage and access procedures involved in producing speech from print (i.e., the knowledge isn't learned in the system). As a result they are much more limited than the broader and more explanatory learning theory offered by SM89, PMSP96, and the connectionist approach.

The basic process of the original dual route computational model [@Coltheart1993; @Coltheart2001] is as follows. When an input string is provided to the model it is processed over some number of cycles before it completes a pronunciation. The total number of cycles reached by the end of the processing sequence (when a pronunciation is reached) is determined by the interaction between the activation cycling through the system and the parameters selected by the modeler. On each cycle activation is pushed through the system. Each module performs a task in conveying input signal to the phoneme module - the last in the architecture. For example, there is a module for visual feature detection, letters, grapheme-phoneme conversion, etc^[In the technical descriptions of the architecture most of these are called "systems", as in the "phoneme system", "grapheme-phoneme system" and so on. I call them "modules" here just for simplicity, where each module is a box in the model schematic, like the one provided in @Coltheart2001.]. There are also a set of modules that operate as lexical storage locations. The schematics in @Coltheart1993 (p. 598) and @Coltheart2001 (p. 214) depict this process^[There is a more recent extension of the computational model in @Pritchard2018. Their model contains aspects of the theory of learning developed by @Share1995, which the earlier more influential model does not contain. However, the basic assumptions of the DRC model remain.]. As cycles progress in the model, activation builds up on the phonological output module. This activation feeds back on the lexicon, which drives further activation on the phoneme output module. Eventually the sequence of phonemes being activated in the output module reach a threshold. The threshold for pronunciation is determined by the parameters specified. This activation occurs across the two routes, feeding forward through the GPC route, and forward and back on the lexical route (including the backwards flow of activation on the lexicon from the phoneme module). This process results in competing activation from both routes, with the stronger route (determined by the model parameters) providing the output pronunciation. The number of cycles becomes the response time of the full system, and the output of phonemes allows the experimenter to record the accuracy of the production.

A similar line of computational models comes from "connectionist dual-process" theories of reading aloud [@Perry2007; @Perry2010; @Zorzi1998]. The basic parts of this model was first reported in @Zorzi1998, and involves the two routes from the DRC model [@Coltheart2001]. However, the grapheme-phoneme route in the CDP models is replaced by a two layer network that associated print and speech using aspects of connectionist architectures except that the network has no hidden layer. Because of the quasiregular nature of the orthography-phonology domain, the two-layer network is able to capture the most reliable regularities between print and speech, but not the less reliable covarying structure between the two (i.e., the exceptions). This "two-layer assembly" ("TLA") network operates in a parallel fashion, like its connectionist counterparts, but functions in a larger architecture which contains a lexical storage system for words identified as exceptions (here, they are the words whose structure can't be captured by the two-layer feedforward network).

The model architecture from @Zorzi1998 was extended in subsequent work [@Perry2007] and was eventually developed in a way that could accommodate words that were more than one syllable [@Perry2010]. The authors described the advances as taking the best aspects of the prior connectionist dual process model and aspects of DRC model from @Coltheart2001. An important change in the @Perry2007 model, in their view, was serializing the processing of graphemes in the TLA portion of the network. This was made possible by introducing a process of aligning letters into slots through a buffering system. They also included an output buffer that is in charge of selecting the final pronunciation of the model. This module is largely based on the analogous component from @Zorzi1998, namely their "phonological decision system", but was augmented because of the changes to the changes to the nonlexical component of the network from the previous version of the model. They also adopted the lexical process implemented in @Coltheart2001, which is a small change from that pathway in @Zorzi1998. The lexical network (a localist system where a node represents a single word, but there are separate systems for orthographic word nodes, phonological word nodes, and semantic word nodes) here implements the idea that many words are activated in the naming process, where a word is passed to the pronunciation stage if it reaches a certain threshold.

The advancement of the dual process models has allowed for the appearance of a reconciliation between different theoretical views in the literature on computational models of word reading given that they combine aspects of connectionist networks and dual-route theorizing (and components of the associated dual-route computational models). A significant aspect of the connectionist dual process models is that they contain a subcomponent that employs a learning mechanism, given that the nonlexical route includes trainable weights. Their model also is able to capture a noteworthy amount of item-level variance in naming accuracy in addition to being able to pronounce many nonwords correctly.

#### Models that can accomodate longer words
The connectionist dual process model was extended in @Perry2010 to read words with two syllables, marking the first computational reading model to be able to do so. This was an important advance given that most words in English are more than one syllable, and most theorizing (computationally implemented or otherwise) has been focused on monosyllabic words [@Jared1990]. 

@Perry2010, similar to its predecessor dual process models, posits a complex architecture based in the dual-route theoretical tradition, again containing a two layer network that operates as the nonlexical pathway. The adjustment made in @Perry2010 to accept longer words was extending the "graphemic buffer" module to allow for a longer word to fit within an orthographic template that would allow it to be associated with a phonological form that had two syllables. There was also a subcomponent of the two-layer network dedicated to stress. Nodes are assigned to each syllable identified, allowing for stress to be applied on each of the (at most) two syllables in a given phonological form in the nonlexical route.

The model reproduces several important behavioral effects, including accounting for a high amount of variance in naming data from the English Lexicon Project [@Balota2007; @Yap2009] and experiments involving multisyllabic words [@Chateau2003]. Nonetheless, the model's function, like those of previous dual process models and the dual-route cascaded model [@Coltheart2001] relies on presetting parameters to allow the model to perform successfully (see @Seidenberg2022 and @Sibley2010 for critical views on this). The original authors argue that the process of setting parameters isn't an issue of concern given that the primary purpose of the parameter setting process is to establish the right balance between lexical and nonlexical routes and that all parameters are interpretable in terms of the psychological constructs they are designed to employ.

#### Connectionist accounts of processing multisyllabic words
In a separate line of work, there have been preliminary computational models of multisyllabic word processing using normative connectionist architectures^[In the sense that the model is constructed using information processing assumptions like those specified in @McClelland1993, @Plaut1996, and @Seidenberg1989, discussed previously.]. This class of models all utilize simple recurrent networks [@Elman1990; @Jordan1986], which allow for a temporal process to operate over a feedforward network. Simple recurrent networks (SRNs) process sequences one element at a time, where the hidden state of the network is copied for use as a context in which the subsequent element in the sequence is processed. This allows dependencies across elements of the input sequence to be learned by the network, though longer distance dependencies become encoded as well despite the proximal nature of the dependency learning taking place. For example, in a simple implementation of this type of network, @Elman1990 was able to show that word boundaries could be learned by a model trained on a continuous sequence of letters (i.e., letters within words) where at each point in time the model is trained to predict only the next letter. Networks of this kind have been applied to other cognitive processes and behavioral tasks [@Botvinick2006; @Elman1990].

@Sibley2008 sought to simulate lexical development at a scale greater than previous connectionist treatments of word learning. Simple recurrent networks were used as the computational mechanism to perform such a task because of the limitations with previous models of word learning (namely those derived from SM89), due to the characteristics already described: they have the capacity to learn long sequences (like word more than one syllable in length). Rather than developing a model of word reading (despite its resemblance to their primary purpose), @Sibley2008 sought to develop orthographic and phonological representations for a large set of words, as well as an architecture that could relate the representations from the two different (print, speech) domains. An additional motivation for the research was to develop a new approach to representation that avoided problems associated with slot-based representations. Such representations, what they call "conjunctive codes", arise because modelers represent orthographic or phonological patterns in ways that position segments (letters or phonemes) in spatially fixed "slots" on input or output layers of connectionist learning systems. This leads to knowledge about a segment that is tied to a particular position (like at the beginning of the word), leading to position-specific knowledge in a way that is undesirable, and uncharacteristic of learning in humans given the way that it limits generalization [@Plaut1996].

Their model was designed either to encode a sequence of letters or phonemes into a lower dimensional, fixed-size vector representation - like other autoencoding models. They refer to the architecture as a "sequence encoder" because of the (novel) sequential way in which segments are processed, applying the architecture either to orthographic sequences and phonological sequences, as two different implementations. The method yielded accuracy measures for nonwords that were correlated positively with human well-formedness ratings of the same words. This advance represented a limited demonstration for connectionist learning models of word processing (not quite reading or naming per se), but demonstrated some promise for future architectures that could process time-distributed representations of print and speech in a single system.

@Sibley2010 built off the architecture in @Sibley2008 by integrating the two different networks into a single model that maps orthographic inputs directly to their phonological forms. Their results were promising, accounting for variance in naming latency for a set of word-level characteristics in a way that resembled analogous analyses on naming data from the English Lexicon Project [@Balota2007; @Yap2009]. Also, like its predecessor, the model was able to train on a more substantial quantity of words, including words with two syllables. However, their model had some noteworthy weaknesses. Like the sequence encoder from @Sibley2008, experimentation with longer words (orthographically) yielded poor results. They attributed this to the fact that the model learns that the end-of-word segment only occurs in certain positions on the output layer. It also relates to the limitations of the simple recurrent network, which learns transitional probabilities one segment at a time.

#### Print-to-speech conversion in engineering disciplines
It is important to note that psychological work on learning to read and other related forms of language processing has developed alongside related research and development in computer science concerning converting printed language to its spoken analogue, and vice versa. Engineering work to this end has involved a number of different approaches implementing a variety of computational and task assumptions about language processing. This research is typically oriented towards solving an engineering problem: how can a learning system accurately produce a pronunciation for a word given its spelling. This is important for a variety of human-centered engineering tasks (e.g., speech recognition,  text-to-speech systems, etc.).

Computational systems range from those that rely on structured grapheme-to-phoneme conversion processes [@Bisani2008; @Toshinwal2017; @Rao2015; @Yao2015] to fully probabilistic artificial neural network models that don't build in assumptions about the nature of the distinct speech sounds that comprise the spoken language [@VandenOord2016]. Many of these systems employ assumptions that share roots with connectionist learning systems, and it has become standard that such systems employ some type of artificial neural network as part of its architecture. The most modern architectures, like that of @VandenOord2016, achieve spoken language performance that both plausibly simulates spoken language of a human, including those of different dialects and language backgrounds at high levels of accuracy. Many of these networks have some commonalities to recurrent networks used in the cognitive literature discussed previously. @Rao2015, for example, contains a sequence of LSTM layers that work together to predict the phoneme sequence on the output layer, but without the type of sequence-to-sequence architecture reported on in this dissertation.

## Scope of the dissertation
Drawing from these latest modeling exercises [@Sibley2008; @Sibley2010], this dissertation reports on a computational model of word reading that focuses on the generation of a phonological code from print, and that can accommodate words not bound by orthographic or phonological length (i.e., of words that are not limited to a single syllable). The cognitively plausible learning model takes in temporally distributed input segments in service of mapping print to speech, incorporating a number of standard assumptions about the fundamental characteristics of learning^[These assumptions were adopted and extended from those articulated in @Plaut1996. See also the description of GRAIN networks in @McClelland1993 for related characteristics of similar information processing systems to this one.]:

* Learning happens over distributed representations of letters and sounds, where such representations convey featural information about their corresponding visual and articulatory structure.
* Representational information comes to be encoded in patterns of activity on units in the system's architecture.
* These patterns of activity encode graded knowledge about the elements of learning (words) and their myriad substructure.
* Learning about visual and phonological features, and their covariation, happens gradually over the course of learning.

Additionally, this work extends previous computational models of word reading by developing a model with the following characteristics:

1. Sequential dependencies within items (i.e., letters, sounds) are encoded by *time-sensitive* processes captured by the model^[For the purposes of discussion, these processes are termed *time-varying* in this dissertation, but other related terms that emphasize sensitivity to temporal structure are also applicable.].
2. Letters and sounds are represented veridically. Each letter and phoneme is represented with a pattern over features that captures something about its unique (distributed) structure relative to all other letters and phonemes.
3. Knowledge about letters and phonemes is not bound by their slots within items during learning.
4. Learning that happens over the time-varying print and speech sequences are integrated within a single learning system, where activation within and across layers of the network is mutually constraining.

The intention of this dissertation is to motivate a novel direction for the development of learning architectures that account for time-varying aspects of perceptual processes of human behavior related to reading such as, but not limited to, audition and vision. This will be accomplished through presenting details of the computational model relevant to the development of such theories alongside computational and behavioral results that convey aspects of its behavior that lends legitimacy to the system.

This is accomplished by introducing novel aspects of a computational architecture that incorporate time into the reading system in explicit, psychologically plausible ways. This includes using recurrence in orthographic and phonological portions of the network made possible through the implementation of computational mechanisms for simulating time [@Hochreiter1997a; @Hochreiter1997b], featural information about stress encoded on phonological segments (namely vowels), and model organization that allows time-varying orthographic information to mix with phonological processing within a single, homogeneous computational architecture. These ingredients to the system are delivered in a fully-specified way that can be reproduced in other efforts in service of the broader goal of understanding the cognitive aspects of reading and its development through simulations like the ones reported on here.

The chapters of this dissertation are organized in the following way. Chapter 1 was focused on the central issues in word reading as they relate to the type of sequential processing that is essential to the theory developed in the dissertation along with a discussion of the different computational models that have been put forward to implement such processes. Chapter 2 describes the new model architecture, highlighting the features that make it distinct from other previous architectures. Chapter 3 describes a range of human behavioral phenomena captured by the model, thus validating the architecture. Chapter 4 provides general discussion, addressing lingering issues with the model described, and framing the dissertation in a broader theoretical landscape of human learning, including education.

A range of relevant technical specifications and performance data can be found in the appendices at the end of this document and the code for this work can be found at the following repository: https://github.com/MCooperBorkenhagen/dissertation.

\newpage
