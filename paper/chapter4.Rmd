---
title: "discussion"
output: html_document
---


# Chapter 5: General Discussion
Reading processes given the perceptual processes that underlie it are time-dependent. Vision operates over patterns of fixation over letters, audition occurs over temporally distributed processes where sounds are received by the auditory system, and oral naming occurs when spoken language is produced over time by virtue of the articulatory system. Historically these processes have been avoided in models of word recognition in naming in service of more primary aspects of the reading system: the nature of the more fundamental computations that participate in producing a spoken form of a word from the printed code.

The model presented here is an attempt to push theories of reading and the computational models that implement them to account for time-dependent processes in a way that builds on top of previous connectionist approaches.



## Future work
This model architecture could be extended easily, in some cases trivially, to account for representational assumptions that would more realistically approximate spoken language.

Note that an architecture of this kind can be easily extended to include both a phonological and orthographic output layer despite the more simple version of the architecture adopted for this work. Such an extension would be important to pursue given the role of writing (action) in reading development; the two activities are almost always unavoidably linked in development.

## Batches
One potential limitation of the learning approach developed here concerns the type of batch learning used. Batches constitute the learning trials over which weight updates occur and therefore likely play a role in the dynamics of how well aspects of structure are learned and the timecourse for such learning. These dynamics may be quite complex, with larger implications for the nature of knowledge development in the long term developmental process of the model and a reader that it is designed to simulate. This is no doubt an area of potentially valuable future research, though a few things can be noted here.

The assembly of minibatches in this approach is designed for reasons more related to the engineering of the computational system provided rather than a direct hypothesis about the nature of human learning in this learning domain. This specification of the architecture allows for the use of words without alignment of any kind - a feature thought to be a virtue of the approach given the potential learning challenges imposed by including empty slots in input and output representations. However, this may not be an unrelated issue to human learning. For example, the minibatch approach taken here can be implemented over batches of only one learning trial, which is termed "stochastic gradient descent" in the literature on artificial neural networks. One possibility is that human learning occurs over such minimal number of learning experiences. If so, the architecture implemented here represents a viable one for experimenting with such a possibility, which is afforded by the ability to learn without slots and alignment.



