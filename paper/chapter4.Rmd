---
title: "discussion"
output: html_document
---


```{r, echo=FALSE}
source('freezephon.R')
source('nullphon.R')
```


# Chapter 5: General Discussion
Reading processes given the perceptual processes that underlie it are time-dependent. Vision operates over patterns of fixation over letters, audition occurs over temporally distributed processes where sounds are received by the auditory system, and oral naming occurs when spoken language is produced over time by virtue of the articulatory system. Historically these processes have been avoided in models of word recognition in naming in service of more primary aspects of the reading system: the nature of the more fundamental computations that participate in producing a spoken form of a word from the printed code.

The model presented here is an attempt to push theories of reading and the computational models that implement them to account for time-dependent processes in a way that builds on top of previous connectionist approaches.

## Phonology is important, but it isn't everything
Phonological knowledge always predates the onset of learning how print relates to speech. It is for this reason that so many theories (though to varying degrees) emphasize the primacy of phonological development in service of learning to read. It is established that a robust knowledge of the sound structure of the language is essential to skilled reading [@Melby-Lervag2012; @Harm2004; @Seidenberg2017; @Share1995; @Wagner1987], and that the act of reading involves a division of labor among phonological, and other cognitive processes [@Harm2004].

The computational model described here relies heavily on phonology over the course of developing knowledge about the relationship between print and speech. This results from the phonological demands of the architecture during training; phonological inputs are present when mapping orthography to phonology. This aspect of learning has a temporal component (the phonological LSTM) and a feedforward component (the dense phonological layer that projects to the phonological outputs that define output phonemes). The results using the offline test mode of the model indicate that producing a phonological output from orthographic input without an explicit temporal signal governing phonology on its input (i.e., with a sequence of input segments representing the number of timesteps over which to produce a phonological output) operates in a very similar way to the training and test method that uses phonological inputs to control the temporal flow of the output signal.

When we read, we most often don't hear the spoken forms of the words that we are reading. However, the training process in this architecture involves this kind of "reading while listening" training paradigm. This form of training corresponds to a relatively common form of learning in educational settings [@Reitsma1988; @Snow1998], though its effectiveness is questionable in children [@Reitsma1988; @Wim1991].

As an alternative, the network developed here has the capacity to learn only from orthography, though high levels of performance rely on learning taking place in the phonological portions of the architecture. As a demonstration, weights for phonological layers of the network were frozen at different points throughout training (either to the phonological LSTM or both the phononological LSTM and the feedforward phonological output layer). The results of this procedure are shown in Figure XX.


```{r, echo=FALSE, warning=FALSE, fig.cap='The effects of stopping weight updates to different portions of the phonologocal network during training is shown. Shaded regions represent the period of training in which no phonological learning is taking place. The left column shows the effects of halting weight updates to all phonological portions of the network, and the right column shows the effects when halting updates to the phonological LSTM layer only (the portion that tunes to statistical properties across phonological segments of the word). Rows represent the time (epochs are noted) during training when updates to the given portion of the phonological network were stopped. The top row shows behavior in a network where no updates were ever provided and the bottom row showing behavior for a network whose phonological weights were never affected.'}

WHENS = c('Always', 'Epoch 18', 'Epoch 36', 'Epoch 54', 'Never')


COLORS = c('always'='tomato3', 'early'='purple', 'middle'='springgreen4', 'late'='blue', 'never'='turquoise',
           'Always'='tomato3', 'Epoch 18'='purple', 'Epoch 36'='springgreen4', 'Epoch 54'='blue', 'Never'='turquoise')

MIN = c(9, 18, 36, 54, NA)
MAX = c(63, 63, 63, 63, NA)

SHADED = data.frame(cbind(WHENS, MIN, MAX)) %>% 
  rename(condition = WHENS,
         xmin = MIN,
         xmax = MAX) %>% 
  mutate(condition = as.factor(condition),
         xmin = as.numeric(xmin),
         xmax = as.numeric(xmax))

EPOCHS = c(9, 18, 27, 36, 45, 54, 63)
PROPS = c(.90, .95, 1.00)

freezephon %>% 
  filter(epoch %in% EPOCHS) %>% 
  #mutate(period = case_when(when_freeze == 'always' ~ 'frozen',
  #                          when_freeze == 'early' & epoch >= 18 ~ 'frozen',
  #                          when_freeze == 'middle' & epoch >= 36 ~ 'frozen',
  #                          when_freeze == 'late' & epoch >= 54 ~ 'frozen',
  #                          TRUE ~ 'learning')) %>% 
  group_by(which_freeze, when_freeze, epoch) %>% 
  summarize(error_m = mean(mse),
            error_sd = sd(mse),
            error_sem = error_sd/sqrt(N),
            accuracy_m = mean(binary_acc),
            accuracy_sd = sd(binary_acc),
            accuracy_sem = accuracy_sd/sqrt(N)) %>% 
  mutate(which_freeze = case_when(which_freeze == 'all' ~ 'Stopped all phonology',
                                  which_freeze == 'lstm' ~ 'Stopped phon LSTM only'),
         condition = case_when(when_freeze == 'always' ~ WHENS[1],
                               when_freeze == 'early' ~ WHENS[2],
                               when_freeze == 'middle' ~ WHENS[3],
                               when_freeze == 'late' ~ WHENS[4],
                               when_freeze == 'never' ~ WHENS[5])) %>% 
  left_join(SHADED) %>% 
  mutate(condition = as.factor(condition),
         condition = fct_relevel(condition, WHENS)) %>% 
  ggplot(aes(epoch, accuracy_m, group = condition)) +
  geom_rect(aes(x = NULL, y = NULL, xmin = xmin, xmax = xmax, ymin = .90, ymax = 1), fill = 'grey90', color = 'transparent', alpha = .09) +
  geom_point() +
  geom_line() + 
  scale_color_manual(values=COLORS) +
  geom_errorbar(aes(ymin = accuracy_m-accuracy_sem, ymax = accuracy_m+accuracy_sem), width = .1) +
  facet_grid(rows = vars(condition), cols = vars(which_freeze)) +
  labs(x = 'Epoch', y = 'Accuracy (proportion)', color = 'Removal of phon learning\n(time of removal)') +
  theme_apa() +
  theme(legend.title.align = .5,
        axis.text.x = element_text(size = 8),
        strip.text.x = element_text(size = 12),
        strip.text.y = element_text(size = 8),
        legend.position = 'none') +
  scale_x_continuous(breaks = EPOCHS) +
  scale_y_continuous(breaks = round(PROPS, digits = 2))# +
  #guides(color = guide_legend(nrow = 2))

# possibly useful later if you want to calculate error over epoch as a functin of freeze time:
CUTOFFS = data.frame(unique(freezephon$when_freeze), MIN, MAX) %>% 
  rename(when_freeze = unique.freezephon.when_freeze.) %>% 
  replace_na(list(MIN = 0, MAX = 0)) %>% 
  mutate(SPAN = MAX - MIN)
  
descriptives = freezephon %>% 
  group_by(which_freeze, when_freeze) %>% 
  summarise(accuracy_m = mean(binary_acc),
            accuracy_sd = sd(binary_acc))

rounD <- function(x) sprintf("%.4f", x)

freezephon %>% 
  left_join(CUTOFFS) %>% 
  mutate(when_freeze_num = case_when(when_freeze == 'never' ~ 63,
                                     when_freeze == 'early' ~ 18,
                                     when_freeze == 'middle' ~ 36,
                                     when_freeze == 'late' ~ 54,
                                     when_freeze == 'always' ~ 9)) %>% 
  mutate(phon_frozen = case_when(SPAN == 0 ~ FALSE,
                                 epoch < when_freeze_num ~ FALSE,
                                 epoch >= when_freeze_num ~ TRUE)) %>% 
  filter(phon_frozen == TRUE) %>% 
  filter(epoch == MIN | epoch == MAX) %>% 
  left_join(descriptives) %>% 
  group_by(which_freeze, when_freeze, epoch) %>% 
  summarise(MIN = first(MIN),
            MAX = first(MAX),
            how_long_frozen = first(SPAN), 
            accuracy_m = first(accuracy_m),
            accuracy_sd = first(accuracy_sd),
            accuracy = mean(binary_acc),
            accuracy_z = (accuracy-accuracy_m)/accuracy_sd) %>% 
  ungroup() %>% 
  group_by(which_freeze, when_freeze) %>% 
  arrange(desc(accuracy)) %>% 
  summarise(how_long_frozen = first(how_long_frozen),
            accuracy_increase = difference(accuracy_z)) %>% 
  ungroup() %>% 
  group_by(which_freeze, when_freeze) %>% 
  summarize(growth_per_epoch = accuracy_increase/how_long_frozen) %>% 
  mutate(which_freeze = case_when(which_freeze == 'all' ~ 'Stopped all phonology',
                                  which_freeze == 'lstm' ~ 'Stopped phon LSTM only'),
         condition = case_when(when_freeze == 'always' ~ WHENS[1],
                               when_freeze == 'early' ~ WHENS[2],
                               when_freeze == 'middle' ~ WHENS[3],
                               when_freeze == 'late' ~ WHENS[4],
                               when_freeze == 'never' ~ WHENS[5])) %>% 
  ggplot(aes(condition, growth_per_epoch)) +
  geom_bar(stat = 'identity', fill = 'grey29', color = 'black') +
  facet_grid(~which_freeze) +
  scale_y_continuous(labels = comma) +
  labs(x = 'Period of training when learning stops',
       y = 'Accuracy growth per epoch after stop (standardized)') +
  theme(legend.position = 'none') +
  theme_apa()
  

```

It is clear that weight layers dedicated to phonology are critical to support high levels of orthography-phonology knowledge. The top row of the figure shows the dramatic effect of having no phonological layers receiving weight updates, with the left column ("stopped all phonology" column) showing the most dramatic effect; this translates into very low levels of performance throughout training, with only modest increases in accuracy at the end of learning as compared to the start. It is clear that both the phonological LSTM and the feedforward phonological (output) layer contribute substantially given that the simulations where only the phonological LSTM affected show a significant advantage over those where all phonological layers are affected. This difference is most clear in the top row of the figure with the learning curve on the righthand pane showing greater increases than that in the left.

So, clearly phonology is a critical aspect of the architecture - as it should be given that reading across levels of expertise relies on phonological knowledge [@VanOrden1987]. This model can then be easily extended in order to investigate issues related to the relative importance of phonology and orthography, and temporal aspects of each, in developing knowledge involved in word naming.

Nonetheless, the implemented model is capable of achieving high levels of knowledge by learning without explicit phonological inputs during training (i.e., learning over orthographic inputs and phonological outputs) if an explicit temporal signal is provided to the phonological network on input. This implementation is very similar to the process in 


## Future work
This model architecture could be extended easily, in some cases trivially, to account for representational assumptions that would more realistically approximate spoken language.

Note that an architecture of this kind can be easily extended to include both a phonological and orthographic output layer despite the more simple version of the architecture adopted for this work. Such an extension would be important to pursue given the role of writing (action) in reading development; the two activities are almost always unavoidably linked in development.

## Batches
One potential limitation of the learning approach developed here concerns the type of batch learning used. Batches constitute the learning trials over which weight updates occur and therefore likely play a role in the dynamics of how well aspects of structure are learned and the timecourse for such learning. These dynamics may be quite complex, with larger implications for the nature of knowledge development in the long term developmental process of the model and a reader that it is designed to simulate. This is no doubt an area of potentially valuable future research, though a few things can be noted here.

The assembly of minibatches in this approach is designed for reasons more related to the engineering of the computational system provided rather than a direct hypothesis about the nature of human learning in this learning domain. This specification of the architecture allows for the use of words without alignment of any kind - a feature thought to be a virtue of the approach given the potential learning challenges imposed by including empty slots in input and output representations. However, this may not be an unrelated issue to human learning. For example, the minibatch approach taken here can be implemented over batches of only one learning trial, which is termed "stochastic gradient descent" in the literature on artificial neural networks. One possibility is that human learning occurs over such minimal number of learning experiences. If so, the architecture implemented here represents a viable one for experimenting with such a possibility, which is afforded by the ability to learn without slots and alignment.


@Jared1990 commented "It should be noted however that in the @Seidenberg1989 model, the computation from orthography to phonology is mediated by an interlevel of _hidden_ units. In a model of this sort, the hidden units pick up higher generalizations about the correspondence between input and output codes. We speculate that the hidden units will tend to pick up the statistical regularities in terms of the orthographic and phonological characteristics of syllables. Thus, the syllable-like units would be an emergent property of the system."
