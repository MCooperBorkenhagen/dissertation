
```{r, echo=FALSE, warning=FALSE}
source('data/data_freezephon.R')
source('data/data_nullphon.R')
```


# Chapter 5: General Discussion
Reading is a temporally governed aspect of cognition given the temporal nature of the perceptual processes that underlie it. Vision operates over patterns of fixation over letters, audition occurs over temporally distributed processes where sounds are received by the auditory system, and oral naming occurs when spoken language is produced over time by virtue of the articulatory system. Historically, connectionist models of word naming have focused on other more primary aspects of the reading system: the nature of the more fundamental computations that participate in producing a spoken form of a word from the printed code. This dissertation extends that work by applying examining the application of connectionist principles of cognition to time-distributed representations of print and speech in an integrated computational model of word naming. 

The model described here processes print and speech in a time-dependent way, which enables the system to give rise to a range of phenomena found in behavioral data on word reading - including those that involve statistical properties of words that emerge across longer words and syllables. The focus of the description here has been to show that (a) the performance of the model is similar to other well-established computational systems, (2) the model approximates several behavioral effects seen in humans, and (3) to explain properties of the model in terms of analogous properties of cognition and development in human readers.

## Summary of the basic properties of the model
Print input is processed over letters in a sequential, letter-by-letter way. This computation enables two representations to be passed to phonology in order to generate phonological output. One vector is a hidden activation of the orthography layer at the last timestep (the last letter) and the other represents a pattern of activation that builds up across the entire sequence of the orthographic input (the cell state), which encodes the long term dependencies across the orthographic input. These vectors are used as the context against which phonological outputs are generated. 

The phonological portion of the network during training takes in phonological input, phonemic segment by phonemic segment, and produces the same phonemic sequence as output, set ahead by one timestep. This segment-by-segment process happens using the hidden patterns passed from orthography as the phonological LSTM's starting state, allowing for the "mixing" of phonological and orthographic input signals, though all layers receive error signals using back propagation through time driven by the model's phonological output layer. The phonological input is always initiated with a start of word segment, and its output always ends with an end of word segment. This enables the model to learn about word boundaries throughout training, and also enables offline production where only the word-initial segment is used to dynamically produce phonological output from computed orthographic hidden patterns (i.e., a hidden state and cell state, as described before).

Note that the architecture conveys something related to the intuition made by @Jared1990 relating the general properties of connectionist architectures of word reading (namely the one implemented by SM89 and its predecessors): "It should be noted however that in the SM89 model, the computation from orthography to phonology is mediated by an interlevel of _hidden_ units. In a model of this sort, the hidden units pick up higher generalizations about the correspondence between input and output codes. We speculate that the hidden units will tend to pick up the statistical regularities in terms of the orthographic and phonological characteristics of syllables. Thus, the syllable-like units would be an emergent property of the system." The model developed here exhibits such properties (e.g., those specifically related to syllables) despite the fact that syllables aren't explicitly represented in learning.

## The learning process
Words are assembled based on their phonological length. Within a given set of words that are equal in length, minibatches are assembled for training. A given length was selected using a weighted sampling process, based on how common on average words of that length are in text corpora. Nonetheless, other implementations could be used, including batch size one. The basic elements of this scheme were developed so as to avoid issues of justifying inputs and outputs in order to avoid slot-based representations, an established problem in computational architectures designed to simulate reading development [@Plaut1996]. More specifically, this is avoided because orthographic inputs are masked with as many timesteps are necessary in order to create a set of equal dimension patterns within words of a given phonological length (masked timesteps are skipped when processing the orthographic input). Training of this type leads to learning curves that resemble other computational approaches, with asymptotic performance achieved on training and test sets within a reasonable time horizon.

## Phonology is important, but it isn't everything
Phonological knowledge always predates the onset of learning how print relates to speech. It is for this reason that so many theories (though to varying degrees) emphasize the primacy of phonological development in service of learning to read. It is established that a robust knowledge of the sound structure of the language is essential to skilled reading [@Melby-Lervag2012; @Harm2004; @Seidenberg2017; @Share1995; @Wagner1987], and reading involves a division of labor among phonological, and other cognitive processes [@Harm2004].

The computational model described here relies heavily on phonology over the course of developing knowledge about the relationship between print and speech. This results from the phonological demands of the architecture during training; phonological inputs are present when mapping orthography to phonology. This aspect of learning has a temporal component (the phonological LSTM) and a feedforward component (the dense phonological layer that projects to the phonological outputs that define output phonemes). The results using the offline test mode of the model indicate that producing a phonological output from orthographic input without an explicit temporal signal governing phonology on its input (i.e., with a sequence of input segments representing the number of timesteps over which to produce a phonological output) operates in a very similar way to the training and test method that uses phonological inputs to control the temporal flow of the output signal.

When we read, we most often don't hear the spoken forms of the words that we are reading. However, the training process in this architecture involves this kind of "reading while listening" training paradigm. This form of training corresponds to a relatively common form of learning in educational settings [@Reitsma1988; @Snow1998], though its effectiveness is questionable in children [@Reitsma1988; @VanBon1991].

As an alternative, the network developed here has the capacity to learn only from orthography, though high levels of performance rely on learning taking place in the phonological portions of the architecture. As a demonstration, weights for phonological layers of the network were frozen at different points throughout training (either to the phonological LSTM or both the phononological LSTM and the feedforward phonological output layer). The results of this procedure are shown in Figure XX.


```{r, echo=FALSE, warning=FALSE, fig.cap='The effects of stopping weight updates to different portions of the phonologocal network during training is shown. Shaded regions represent the period of training in which no phonological learning is taking place. The left column shows the effects of halting weight updates to all phonological portions of the network, and the right column shows the effects when halting updates to the phonological LSTM layer only (the portion that tunes to statistical properties across phonological segments of the word). Rows represent the time (epochs are noted) during training when updates to the given portion of the phonological network were stopped. The top row shows behavior in a network where no updates were ever provided and the bottom row showing behavior for a network whose phonological weights were never affected.'}
N = length(unique(nullphon$word))

WHENS = c('Always', 'Epoch 18', 'Epoch 36', 'Epoch 54', 'Never')


COLORS = c('always'='tomato3', 'early'='purple', 'middle'='springgreen4', 'late'='blue', 'never'='turquoise',
           'Always'='tomato3', 'Epoch 18'='purple', 'Epoch 36'='springgreen4', 'Epoch 54'='blue', 'Never'='turquoise')

MIN = c(9, 18, 36, 54, NA)
MAX = c(63, 63, 63, 63, NA)

SHADED = data.frame(cbind(WHENS, MIN, MAX)) %>% 
  rename(condition = WHENS,
         xmin = MIN,
         xmax = MAX) %>% 
  mutate(condition = as.factor(condition),
         xmin = as.numeric(xmin),
         xmax = as.numeric(xmax))

EPOCHS = c(9, 18, 27, 36, 45, 54, 63)
PROPS = c(.90, .95, 1.00)

freezephon %>% 
  filter(epoch %in% EPOCHS) %>% 
  #mutate(period = case_when(when_freeze == 'always' ~ 'frozen',
  #                          when_freeze == 'early' & epoch >= 18 ~ 'frozen',
  #                          when_freeze == 'middle' & epoch >= 36 ~ 'frozen',
  #                          when_freeze == 'late' & epoch >= 54 ~ 'frozen',
  #                          TRUE ~ 'learning')) %>% 
  group_by(which_freeze, when_freeze, epoch) %>% 
  summarize(error_m = mean(mse),
            error_sd = sd(mse),
            error_sem = error_sd/sqrt(N),
            accuracy_m = mean(binary_acc),
            accuracy_sd = sd(binary_acc),
            accuracy_sem = accuracy_sd/sqrt(N)) %>% 
  mutate(which_freeze = case_when(which_freeze == 'all' ~ 'Stopped all phonology',
                                  which_freeze == 'lstm' ~ 'Stopped phon LSTM only'),
         condition = case_when(when_freeze == 'always' ~ WHENS[1],
                               when_freeze == 'early' ~ WHENS[2],
                               when_freeze == 'middle' ~ WHENS[3],
                               when_freeze == 'late' ~ WHENS[4],
                               when_freeze == 'never' ~ WHENS[5])) %>% 
  left_join(SHADED) %>% 
  mutate(condition = as.factor(condition),
         condition = fct_relevel(condition, WHENS)) %>% 
  ggplot(aes(epoch, accuracy_m, group = condition)) +
  geom_rect(aes(x = NULL, y = NULL, xmin = xmin, xmax = xmax, ymin = .90, ymax = 1), fill = 'grey90', color = 'transparent', alpha = .09) +
  geom_point() +
  geom_line() + 
  scale_color_manual(values=COLORS) +
  geom_errorbar(aes(ymin = accuracy_m-accuracy_sem, ymax = accuracy_m+accuracy_sem), width = .1) +
  facet_grid(rows = vars(condition), cols = vars(which_freeze)) +
  labs(x = 'Epoch', y = 'Accuracy (proportion)', color = 'Removal of phon learning\n(time of removal)') +
  theme_apa() +
  theme(legend.title.align = .5,
        axis.text.x = element_text(size = 8),
        strip.text.x = element_text(size = 12),
        strip.text.y = element_text(size = 8),
        legend.position = 'none') +
  scale_x_continuous(breaks = EPOCHS) +
  scale_y_continuous(breaks = round(PROPS, digits = 2))# +
  #guides(color = guide_legend(nrow = 2))


```

It is clear that layers dedicated to phonology are critical to support high levels of orthography-phonology knowledge. The top row of the figure shows the dramatic effect of having no phonological layers receiving weight updates, with the left column ("stopped all phonology" column) showing the most dramatic effect; this translates into very low levels of performance throughout training, with only modest increases in accuracy by the end of the learning period. It is clear that both the phonological LSTM and the feedforward phonological (output) layer contribute substantially given that the simulations where only the phonological LSTM affected show a significant advantage over those where all phonological layers are affected. This difference is most clear in the top row of the figure with the learning curve on the righthand pane showing greater increases than that in the left.

Clearly phonology is a critical aspect of the architecture - as it should be given that reading across levels of expertise relies on phonological knowledge [@VanOrden1987]. However the orthographic portion of the network also contributes to learning, where relevant subsequent work would concern relative importance of phonology and orthography, and temporal aspects of each, on the way to developing knowledge about long words and their properties.

Additionally, the implemented model is capable of achieving high levels of knowledge by learning without explicit phonological inputs during training (i.e., learning over orthographic inputs and phonological outputs) if an explicit temporal signal is provided to the phonological network on input. To demonstrate, as similar procedure to the previous simulations (data shown in Figure XX) was conducted, instead training with null phonological inputs. For example, the phonological form of the word `r scaps('went')` (with phonemes `r scaps('w-eh1-n-t')`) would be supplied with the null sequence `r scaps('# _ _ _ _')`, and the null sequence `r scaps('# _ _ _ _')` would be used for `r scaps('water')`, `r scaps('w-a0-t-er0')`). These null inputs were introduced at different points in training (the same scheme as those in Figure XX). Note that this implementation is similar to the process involved in production mode, where only the start of word segment is provided in order to generate an offline production, with the difference being that all segments after the start of word segment are provided as inputs, just with all units set to zero. This is equivalent to guiding the model through an explicit temporal sequence, but giving it no specific phoneme on input to reproduce on output during learning.

The results from this implementation make it clear that orthographic input along with error feedback on the phonological output layer are enough to drive training to high levels of accuracy. While a model that trains without any null phonological inputs (rightmost panel) seems to outperform others, it isn't dramatically so. This addresses one limitation of the system by showing that the portion of the network that concerns phonology relies less on the autoencoding function it seems to perform (mapping phoneme inputs to phoneme outputs offset by one timestep) and more on the way that it drives the temporal process associated with producing time-distributed phoneme sequences on the output layer. Nonetheless, one critical aspect of the architecture is maintaining the ability of the phonological layers to learn throughout training, which will also be the case when training on null phonological inputs.

```{r, nullphon, echo=FALSE, warning=FALSE, fig.height=2.5, fig.cap='Learning curves for models trained with null phonological inputs starting at different points in training are shown. Panels (and their corresponding grey regions) correspond to the different points in training where null phonological inputs were introduced. The "Always" condition corresponds to training in which phonological inputs were always null, and the "Never" condition corresponds to training in which phonological inputs were always provided (i.e., they are always true phonemic segments).'}

PROPS = c(.94, .96, .98, 1.00)

nullphon %>% 
  filter(epoch %in% EPOCHS) %>% 
  group_by(when_null, epoch) %>% 
  summarize(error_m = mean(mse),
            error_sd = sd(mse),
            error_sem = error_sd/sqrt(N),
            accuracy_m = mean(binary_acc),
            accuracy_sd = sd(binary_acc),
            accuracy_sem = accuracy_sd/sqrt(N)) %>% 
  mutate(condition = case_when(when_null == 'always' ~ WHENS[1],
                               when_null == 'early' ~ WHENS[2],
                               when_null == 'middle' ~ WHENS[3],
                               when_null == 'late' ~ WHENS[4],
                               when_null == 'never' ~ WHENS[5])) %>% 
  left_join(SHADED) %>% 
  mutate(condition = as.factor(condition),
         condition = fct_relevel(condition, WHENS)) %>% 
  ggplot(aes(epoch, accuracy_m, group = condition)) +
  geom_rect(aes(x = NULL, y = NULL, xmin = xmin, xmax = xmax, ymin = .90, ymax = 1), fill = 'grey90', color = 'transparent', alpha = .09) +
  geom_line(color = 'grey48') + 
  geom_point() +
  geom_errorbar(aes(ymin = accuracy_m-accuracy_sem, ymax = accuracy_m+accuracy_sem), width = .1) +
  facet_grid(cols = vars(condition)) +
  labs(x = 'Epoch', y = 'Accuracy (proportion)', color = 'Removal of phon learning\n(time of removal)') +
  theme_apa() +
  theme(legend.title.align = .5,
        axis.text.x = element_text(size = 6),
        strip.text.x = element_text(size = 12),
        legend.position = 'none') +
  coord_cartesian(ylim = c(.94, 1.00)) +
  scale_x_continuous(breaks = EPOCHS) +
  scale_y_continuous(breaks = round(PROPS, digits = 2))
  
```

## Time-distributed processing, "serial" processing, and phonological "assembly"
One criticism of theoreticians who have espoused a dual-mechanism approach to word naming (see @Rastle2010 for a review) is that connectionist models of reading fail to capture the sequential aspect of phonological production given the limitations of feedforward mechanisms in the dominant connectionist models of reading (e.g., @Seidenberg1989 and @Plaut1996). In the fully parallel (feedforward) and non-recurrent versions of the model, an orthographic input pattern centered on the vowel is mapped spatially onto its corresponding vowel-centered phonological output pattern. This has been described by dual-route theorists as fundamental flaw of the processing mechanisms employed by connectionist models because they fail to capture something about the "serial" nature of processing involved in reading, along with a range of human behavior related to such processing [@Rastle2010, p.6]^[Note though that feedforward processing has been thought to be a simplifying assumption in previous connectionist implementations so that computations could happen in simplified architectures that learn monosyllabic words (e.g., @Plaut1996, Simulation 1).]. The model implemented in this dissertation is an important one in this debate given the range of processing assumptions it employs. In the dual-route parlance, it converts print to speech (or, phonemic sequences as a simplifying assumption) in a _serial_ way (i.e., a sequential/ time-distributed way) over both orthography and phonology. But it does so without phonological "assembly", or the generation of a phonological code from rules that specify relationships between certain letter sequences (graphemes) to certain phonemes. In this sense the proposed model relies on _sequential_ phonological processes that are _not assembled_, but relies on the same sorts of processing assumptions as previous connectionist models. Furthermore, this involves both time-distributed processing (facilitated by the orthographic and phonological LSTM layers) and feedforward processing (the phonological output layer that converts a hidden but orthographically conditioned phonological pattern to segmental phoneme output sequences). This architectural advance is important for facilitating future debate about the different types of processes (sequential and non-sequential) involved in reading. It furthermore addresses this one important criticism of previous, fully parallel computational models.

## Future work
This model architecture could be extended easily, in some cases trivially, to account for representational assumptions that would more realistically approximate spoken language. For example, phonological representations were based on assumptions about the relevant features that operate over phonemes, with the implementation being similar to prior ones (phoneme features were based on those from @Harm2004). However, the sequential nature of the phonological portion of the network could easily be applied to raw acoustic data in service of developing a more veridical basis of phonological knowledge from which to incorporate print knowledge. This extension would be difficult if not impossible using feedforward architectures of reading.

Also, an architecture of this kind can be easily extended to include other input and output layers despite the more simple version of the architecture adopted for this work. Similar to SM89, the implemented model only concerns producing speech from print, but should be thought of as being situated in a more general framework that concerns semantics. Extending the architecture to include semantics would involve making decisions about semantic aspects of mords that become quite complex, especially when considering longer words (e.g., morphological structure). Other extensions are possible as well, including producing print from speech - the inverse problem to the one approached here. Such an extension would be valuable to pursue given the role of writing (action) in reading development; the two activities are almost always unavoidably linked in development. In summary, this architecture is flexible given the way that the components of the network have been assembled in a single system and it can be used as the basis for more sophisticated models of word reading beyond the simple orthography-to-phonology case.

### Windows of visual processing and related recurrent dynamics
The implemented model makes a processing assumption about the window of time (and space) over which orthographic processes occur prior to being integrated with phonology: an entire visual word form is processed and passed along. Importantly, the corpus of words being used here are relatively short (8 letters or less), in an effort to avoid issues related to refixation during word recognition. The dynamics concerning visual information in processing words are known to operate serially [@Reichle2006], and phonology plays a role in visual processes [@Rayner1998], the timecourse of phonological processes in processing long words is not well understood. This computational model may form the basis to investigate these issues given the serial nature in which orthographic information is passed to phonology in the architecture. A promising extension of the relatively simple demonstration in this dissertation would be to investigate the effects of different visual spans in the orthographic layer on generating a phonological code. While complex, the types of computations afforded by the LSTM-based architecture have the capacity to serve as a rich computational basis from which to investigate a range of issues related to the sequential and temporal dynamics associated with word recognition processes.

### Segmental phenomena NEEDS REVIEW
One debate about the viability of connectionist models of word reading concerns the capacity of prevailing models (or lack thereof) to account directly for segmental phenomena found in experimental work with humans [@Kawamoto2015]. The temporal model developed here is capable of addressing such issues given its ability to simulate phenomena related to segments due to the fact that segments are part of the temporal phenomena related to speech and print.  Introspection about different representational grain sizes in the perceptual inputs of the learning system should be possible in a system that is concerned with learning that involves segments, and this is the case for the architecture being proposed. This capacity deviates sharply from other models of word reading given the limitations of purely spatial learning systems, and the ways in which representational schemes for letters and sounds further exacerbate the capacity to introspect about segmental properties of the learning they involve. For example, the LSTM architecture captures more closely that the learner has representations for "sublexical" units possessed by any given input or output, and that the representational capacity can be observed by observing the model respond to that sublexical unit directly. For example, individual letters and letter sequences have pronunciations, which can be directly observed by setting up a testing trial using the network. This isn't possible in feedforward networks due to the abstract ways in which segments are specified and processed in those systems. This virtue of the architecture will be returned to in the results when we look at the dynamics of a training trial and the types of targeted phenomena the network can simulate for segments of language.

# Conclusion
Computational models have been influential in developing a scientific understanding of basic properties that underlie development and performance in a range of reading processes in humans [@Harm2004; @Plaut1996; @Rastle2010; @Seidenberg2017; @Seidenberg1989]. Debates about the computations that matter in development and performance have been plentiful, and have stalled in part due to architectural limitations of the computational systems recruited for theory building of this sort. This is mostly due to challenges with simulating learning over longer words given the additional cognitive and perceptual processes that are relevant to their information processing. These challenges have given rise to few approaches to the problem, with little consensus about well-principled architectures that should be used for understanding the computational bases of learning and performance for words of this type.

This dissertation extends previous connectionist approaches to understanding the development of word reading abilities, including initial attempts to apply connectionist architectures to reading multisyllabic words [@Sibley2008; @Sibley2010]. This is made possible by simulating temporal processes over orthography and phonology and allowing for those processes to interact in their computations. Furthermore, these processing assumptions operate over orthography and phonology by using representations that more closely resemble those that function in human learners. The assumptions that went into this model are based in more general principles -- statistical learning, distributed representations, dynamical systems -- that apply to learning and other phenomena and build on top of cognitive and computational work relevant to understanding human learning. As a result, this work fits within a broader theoretical framework and applies these principles to temporal aspects of word reading in an effort to extent our understanding of the computations involved in reading beyond the simplest words in the language.

  
\newpage
# References