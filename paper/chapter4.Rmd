---
title: "discussion"
output: html_document
---


# Chapter 5: General Discussion
Reading processes given the perceptual processes that underlie it are time-dependent. Vision operates over patterns of fixation over letters, audition occurs over temporally distributed processes where sounds are received by the auditory system, and oral naming occurs when spoken language is produced over time by virtue of the articulatory system. Historically these processes have been avoided in models of word recognition in naming in service of more primary aspects of the reading system: the nature of the more fundamental computations that participate in producing a spoken form of a word from the printed code.

The model presented here is an attempt to push theories of reading and the computational models that implement them to account for time-dependent processes in a way that builds on top of previous connectionist approaches.

## Phonology is important, but it isn't everything
Phonological knowledge always predates the onset of learning to read, in the sense of learning how print relates to speech. It is for this reason that so many theories (though to varying degrees) emphasize the primacy of phonological development in service of learning to read. It is established that a robust knowledge of the sound structure of the language is essential to skilled reading [], and that the act of reading involves a division of labor across orthographic, phonological, and semantic cognitive processes [@Harm2004]. However, the nature and timecourse of phonological learning as it relates to speech and print is still not well understood aside from the general truism that phonological knowledge is a sine qua non of reading development.

The computational model described here overrelies on phonology over the course of developing knowledge about the relationship between print and speech. This results from the temporal demands of the architecture during training; phonological inputs are always present when mapping orthography to phonology. Furthermore, the phonological portion of the models is always learning as the model trains. This creates two different but related flaws in the architecture



## Future work
This model architecture could be extended easily, in some cases trivially, to account for representational assumptions that would more realistically approximate spoken language.

Note that an architecture of this kind can be easily extended to include both a phonological and orthographic output layer despite the more simple version of the architecture adopted for this work. Such an extension would be important to pursue given the role of writing (action) in reading development; the two activities are almost always unavoidably linked in development.

## Batches
One potential limitation of the learning approach developed here concerns the type of batch learning used. Batches constitute the learning trials over which weight updates occur and therefore likely play a role in the dynamics of how well aspects of structure are learned and the timecourse for such learning. These dynamics may be quite complex, with larger implications for the nature of knowledge development in the long term developmental process of the model and a reader that it is designed to simulate. This is no doubt an area of potentially valuable future research, though a few things can be noted here.

The assembly of minibatches in this approach is designed for reasons more related to the engineering of the computational system provided rather than a direct hypothesis about the nature of human learning in this learning domain. This specification of the architecture allows for the use of words without alignment of any kind - a feature thought to be a virtue of the approach given the potential learning challenges imposed by including empty slots in input and output representations. However, this may not be an unrelated issue to human learning. For example, the minibatch approach taken here can be implemented over batches of only one learning trial, which is termed "stochastic gradient descent" in the literature on artificial neural networks. One possibility is that human learning occurs over such minimal number of learning experiences. If so, the architecture implemented here represents a viable one for experimenting with such a possibility, which is afforded by the ability to learn without slots and alignment.


@Jared1990 commented "It should be noted however that in the @Seidenberg1989 model, the computation from orthography to phonology is mediated by an interlevel of _hidden_ units. In a model of this sort, the hidden units pick up higher generalizations about the correspondence between input and output codes. We speculate that the hidden units will tend to pick up the statistical regularities in terms of the orthographic and phonological characteristics of syllables. Thus, the syllable-like units would be an emergent property of the system."
